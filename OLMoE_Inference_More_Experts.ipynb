{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# OLMoE Inference with More Experts\n",
    "\n",
    "## Comprehensive Guide to Using More Experts Than Default\n",
    "\n",
    "**Author:** Senior AI Researcher & Software Engineer  \n",
    "**Model:** OLMoE-1B-7B (Allen AI)  \n",
    "**Purpose:** Demonstrate inference with configurable number of experts\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to OLMoE](#introduction)\n",
    "2. [Understanding the MoE Architecture](#architecture)\n",
    "3. [Setup and Installation](#setup)\n",
    "4. [Default Inference (8 Experts)](#default)\n",
    "5. [Modified Inference (More Experts)](#modified)\n",
    "6. [Expert Selection Visualization](#visualization)\n",
    "7. [Performance Analysis](#performance)\n",
    "8. [Advanced: Custom Routing](#advanced)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "introduction"
   },
   "source": [
    "## 1. Introduction to OLMoE\n",
    "\n",
    "### What is OLMoE?\n",
    "\n",
    "**OLMoE (Open Mixture-of-Experts Language Models)** is a fully open-source MoE language model developed by Allen AI:\n",
    "\n",
    "- **Total Parameters:** 6.9 billion\n",
    "- **Active Parameters:** 1.3 billion per token\n",
    "- **Architecture:** 16 transformer layers with MoE feedforward layers\n",
    "- **Experts:** 64 experts per layer\n",
    "- **Default Top-k:** 8 experts selected per token\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Dropless Token-Choice Routing:** Every token is processed by exactly k experts\n",
    "2. **Sparse Activation:** Only ~20% of parameters active per token\n",
    "3. **Expert Specialization:** Experts specialize in domains (code, math, text, etc.)\n",
    "4. **Fully Open:** All training data, code, and logs are public\n",
    "\n",
    "### Why Use More Experts?\n",
    "\n",
    "Using more than the default 8 experts can:\n",
    "- **Increase model capacity** for complex tasks\n",
    "- **Improve output quality** by leveraging more specialized knowledge\n",
    "- **Reduce uncertainty** by combining more expert opinions\n",
    "- **Trade-off:** Higher computational cost and latency\n",
    "\n",
    "### MoE Formula\n",
    "\n",
    "```\n",
    "MoE(x) = Î£(i=1 to k) softmax(Router(x))_i Ã— Expert_i(x)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `x`: Input token embedding\n",
    "- `Router(x)`: Learned routing function (linear layer)\n",
    "- `k`: Number of experts to select (default 8, we'll increase this)\n",
    "- `Expert_i(x)`: i-th expert feedforward network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "architecture"
   },
   "source": [
    "## 2. Understanding the MoE Architecture\n",
    "\n",
    "### OLMoE Layer Structure\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚       Input Token Embeddings        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Self-Attention Layer           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Router Network (Linear)        â”‚\n",
    "â”‚      Input: hidden_dim (2048)       â”‚\n",
    "â”‚      Output: 64 expert logits       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Softmax + Top-k Selection      â”‚\n",
    "â”‚      Default: k=8 experts           â”‚\n",
    "â”‚      Modified: k=16, 32, or 64      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
    "         â”‚  Top-k    â”‚\n",
    "         â”‚  Experts  â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼          â–¼          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Expert 1â”‚ â”‚Expert 2â”‚ â”‚Expert kâ”‚\n",
    "â”‚  FFN   â”‚ â”‚  FFN   â”‚ â”‚  FFN   â”‚\n",
    "â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "    â”‚          â”‚          â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Weighted Combination (by router)  â”‚\n",
    "â”‚   Output = Î£ weight_i Ã— expert_i    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â”‚\n",
    "               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Output to Next Layer           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `num_experts` | 64 | Total number of experts per layer |\n",
    "| `num_experts_per_tok` | 8 | Number of experts activated per token |\n",
    "| `hidden_size` | 2048 | Dimension of hidden states |\n",
    "| `intermediate_size` | 1024 | Expert FFN intermediate dimension |\n",
    "| `output_router_logits` | False | Whether to return router logits |\n",
    "\n",
    "### Expert Specialization Patterns\n",
    "\n",
    "Research shows OLMoE experts specialize in:\n",
    "1. **Domain-specific content** (code, math, science, etc.)\n",
    "2. **Vocabulary clusters** (technical terms, common words, etc.)\n",
    "3. **Positional patterns** (beginning, middle, end of sequences)\n",
    "4. **Syntactic structures** (questions, statements, lists, etc.)\n",
    "5. **Semantic topics** (technology, history, arts, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 3. Setup and Installation\n",
    "\n",
    "### GPU Check and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU CONFIGURATION CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ CUDA is available\")\n",
    "    print(f\"âœ“ CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"âœ“ Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"âœ“ Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ“ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Show detailed GPU info\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"NVIDIA-SMI OUTPUT\")\n",
    "        print(\"=\"*60)\n",
    "        print(result.stdout)\n",
    "    except:\n",
    "        print(\"Could not run nvidia-smi\")\n",
    "else:\n",
    "    print(\"âœ— CUDA is NOT available. This notebook requires a GPU.\")\n",
    "    print(\"  Please enable GPU in Colab: Runtime > Change runtime type > GPU\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_deps"
   },
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install transformers with OLMoE support\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers>=4.40.0\n",
    "!pip install -q accelerate>=0.26.0\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q matplotlib seaborn pandas numpy\n",
    "!pip install -q plotly kaleido\n",
    "!pip install -q bitsandbytes  # For 8-bit quantization if needed\n",
    "\n",
    "print(\"âœ“ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_libs"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, List\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "default"
   },
   "source": [
    "## 4. Default Inference (8 Experts)\n",
    "\n",
    "First, let's load the model and run inference with the default configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_model"
   },
   "source": [
    "### Load OLMoE Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model_code"
   },
   "outputs": [],
   "source": [
    "print(\"Loading OLMoE model...\")\n",
    "print(\"This may take a few minutes on first run (downloading ~7GB)\\n\")\n",
    "\n",
    "model_name = \"allenai/OLMoE-1B-7B-0924\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"âœ“ Tokenizer loaded\")\n",
    "\n",
    "# Load model configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "print(f\"Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"Max sequence length: {config.max_position_embeddings}\")\n",
    "print(f\"\\nMoE Configuration:\")\n",
    "print(f\"  Total experts per layer: {config.num_experts}\")\n",
    "print(f\"  Experts per token (top-k): {config.num_experts_per_tok}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nLoading model to {device}...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use FP16 for efficiency\n",
    "    device_map=\"auto\",  # Automatically map to GPU\n",
    "    output_router_logits=True,  # CRITICAL: Enable router logit outputs\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "default_inference"
   },
   "source": [
    "### Run Default Inference (8 Experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "default_gen"
   },
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "prompt = \"\"\"Explain how mixture-of-experts models work in neural networks:\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEFAULT INFERENCE (8 EXPERTS)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPrompt: {prompt}\\n\")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate with default settings\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        output_router_logits=True,  # Get router information\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(\"-\" * 60)\n",
    "print(generated_text)\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\nGeneration time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Tokens generated: {len(outputs.sequences[0]) - len(inputs.input_ids[0])}\")\n",
    "print(f\"Tokens per second: {(len(outputs.sequences[0]) - len(inputs.input_ids[0])) / (end_time - start_time):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "modified"
   },
   "source": [
    "## 5. Modified Inference (More Experts)\n",
    "\n",
    "Now, let's modify the model to use more experts than the default 8.\n",
    "\n",
    "### Understanding the Challenge\n",
    "\n",
    "The number of experts (`num_experts_per_tok`) is hardcoded in the model architecture. To use more experts, we need to:\n",
    "\n",
    "1. **Modify the model configuration** before generation\n",
    "2. **Patch the forward pass** of MoE layers\n",
    "3. **Ensure proper routing weight normalization**\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "We'll create a custom function that:\n",
    "- Temporarily modifies the model's `num_experts_per_tok` parameter\n",
    "- Re-normalizes routing weights for the new top-k\n",
    "- Restores original settings after generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "modify_experts_func"
   },
   "outputs": [],
   "source": [
    "def set_num_experts_per_token(model, num_experts: int):\n",
    "    \"\"\"\n",
    "    Modify the number of experts used per token in OLMoE model.\n",
    "    \n",
    "    Args:\n",
    "        model: OLMoE model instance\n",
    "        num_experts: Number of experts to use (1-64)\n",
    "    \n",
    "    Returns:\n",
    "        Modified model\n",
    "    \"\"\"\n",
    "    assert 1 <= num_experts <= 64, \"num_experts must be between 1 and 64\"\n",
    "    \n",
    "    print(f\"Modifying model to use {num_experts} experts per token...\")\n",
    "    \n",
    "    # Update config\n",
    "    model.config.num_experts_per_tok = num_experts\n",
    "    \n",
    "    # Update all MoE layers\n",
    "    for layer in model.model.layers:\n",
    "        if hasattr(layer, 'mlp'):\n",
    "            # Access the MoE block\n",
    "            mlp = layer.mlp\n",
    "            if hasattr(mlp, 'num_experts_per_tok'):\n",
    "                mlp.num_experts_per_tok = num_experts\n",
    "                print(f\"  Updated layer {layer}\")\n",
    "    \n",
    "    print(f\"âœ“ Model now uses {num_experts} experts per token\\n\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_with_custom_experts(model, tokenizer, prompt, num_experts, max_new_tokens=150):\n",
    "    \"\"\"\n",
    "    Generate text using a custom number of experts.\n",
    "    \n",
    "    Args:\n",
    "        model: OLMoE model\n",
    "        tokenizer: Tokenizer\n",
    "        prompt: Input text\n",
    "        num_experts: Number of experts to use\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (generated_text, generation_time, tokens_generated)\n",
    "    \"\"\"\n",
    "    # Store original setting\n",
    "    original_num_experts = model.config.num_experts_per_tok\n",
    "    \n",
    "    try:\n",
    "        # Modify model\n",
    "        set_num_experts_per_token(model, num_experts)\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                output_router_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "        tokens_generated = len(outputs.sequences[0]) - len(inputs.input_ids[0])\n",
    "        generation_time = end_time - start_time\n",
    "        \n",
    "        return generated_text, generation_time, tokens_generated\n",
    "    \n",
    "    finally:\n",
    "        # Restore original setting\n",
    "        set_num_experts_per_token(model, original_num_experts)\n",
    "\n",
    "\n",
    "print(\"âœ“ Custom expert functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_more_experts"
   },
   "source": [
    "### Test with Different Numbers of Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare_experts"
   },
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "test_prompt = \"\"\"Explain the key differences between transformers and recurrent neural networks:\"\"\"\n",
    "\n",
    "# Test different expert configurations\n",
    "expert_configs = [8, 16, 32, 64]  # Default: 8, then increase\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARING DIFFERENT EXPERT CONFIGURATIONS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPrompt: {test_prompt}\\n\")\n",
    "\n",
    "for num_experts in expert_configs:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TESTING WITH {num_experts} EXPERTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    text, gen_time, tokens = generate_with_custom_experts(\n",
    "        model, tokenizer, test_prompt, num_experts, max_new_tokens=100\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'num_experts': num_experts,\n",
    "        'text': text,\n",
    "        'time': gen_time,\n",
    "        'tokens': tokens,\n",
    "        'tokens_per_sec': tokens / gen_time\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nGenerated Text (first 200 chars):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(text[:200] + \"...\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Generation time: {gen_time:.2f}s\")\n",
    "    print(f\"Tokens generated: {tokens}\")\n",
    "    print(f\"Tokens/second: {tokens/gen_time:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_table"
   },
   "source": [
    "### Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_results"
   },
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['speedup'] = results_df['tokens_per_sec'] / results_df['tokens_per_sec'].iloc[0]\n",
    "results_df['relative_time'] = results_df['time'] / results_df['time'].iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df[['num_experts', 'time', 'tokens', 'tokens_per_sec', 'relative_time']].to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display full results\n",
    "display(results_df[['num_experts', 'time', 'tokens', 'tokens_per_sec', 'relative_time']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## 6. Expert Selection Visualization\n",
    "\n",
    "Let's visualize how experts are selected during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_performance"
   },
   "outputs": [],
   "source": [
    "# Plot performance comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Generation Time vs Number of Experts\n",
    "axes[0, 0].plot(results_df['num_experts'], results_df['time'], marker='o', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Number of Experts', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Generation Time (seconds)', fontsize=12)\n",
    "axes[0, 0].set_title('Generation Time vs Number of Experts', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Tokens per Second vs Number of Experts\n",
    "axes[0, 1].plot(results_df['num_experts'], results_df['tokens_per_sec'], \n",
    "                marker='s', linewidth=2, markersize=8, color='green')\n",
    "axes[0, 1].set_xlabel('Number of Experts', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Tokens per Second', fontsize=12)\n",
    "axes[0, 1].set_title('Throughput vs Number of Experts', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Relative Time (normalized to 8 experts)\n",
    "axes[1, 0].bar(results_df['num_experts'].astype(str), results_df['relative_time'], \n",
    "               color=['blue', 'orange', 'red', 'purple'])\n",
    "axes[1, 0].axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Baseline (8 experts)')\n",
    "axes[1, 0].set_xlabel('Number of Experts', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Relative Time (vs 8 experts)', fontsize=12)\n",
    "axes[1, 0].set_title('Relative Performance', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Active Parameters per Token\n",
    "# Calculate approximate active parameters\n",
    "params_per_expert = 1024 * 2048 * 2  # Approximate expert size\n",
    "active_params = results_df['num_experts'] * params_per_expert / 1e6  # In millions\n",
    "\n",
    "axes[1, 1].bar(results_df['num_experts'].astype(str), active_params, \n",
    "               color=['blue', 'orange', 'red', 'purple'])\n",
    "axes[1, 1].set_xlabel('Number of Experts', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Active Parameters (Millions)', fontsize=12)\n",
    "axes[1, 1].set_title('Model Capacity vs Expert Count', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('olmoe_expert_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Performance visualizations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "router_analysis"
   },
   "source": [
    "### Analyze Router Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_router"
   },
   "outputs": [],
   "source": [
    "# Get router logits for analysis\n",
    "analysis_prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(\"Analyzing router logits...\\n\")\n",
    "inputs = tokenizer(analysis_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **inputs,\n",
    "        output_router_logits=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "# Extract router logits from first layer\n",
    "if hasattr(outputs, 'router_logits') and outputs.router_logits is not None:\n",
    "    # Router logits shape: (num_layers, batch_size, sequence_length, num_experts)\n",
    "    router_logits = outputs.router_logits[0][0].cpu().numpy()  # First layer, first batch\n",
    "    \n",
    "    print(f\"Router logits shape: {router_logits.shape}\")\n",
    "    print(f\"Sequence length: {router_logits.shape[0]}\")\n",
    "    print(f\"Number of experts: {router_logits.shape[1]}\")\n",
    "    \n",
    "    # Compute routing probabilities\n",
    "    router_probs = np.exp(router_logits) / np.exp(router_logits).sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Visualize router probabilities for each token\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Heatmap of routing probabilities\n",
    "    im1 = axes[0].imshow(router_probs.T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "    axes[0].set_xlabel('Token Position', fontsize=12)\n",
    "    axes[0].set_ylabel('Expert Index', fontsize=12)\n",
    "    axes[0].set_title('Router Probabilities per Token', fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(im1, ax=axes[0], label='Probability')\n",
    "    \n",
    "    # Top-k expert selection for each token\n",
    "    top_k_indices = np.argsort(router_probs, axis=1)[:, -8:]  # Top 8 experts\n",
    "    \n",
    "    # Create a binary matrix showing selected experts\n",
    "    selected = np.zeros_like(router_probs)\n",
    "    for i, indices in enumerate(top_k_indices):\n",
    "        selected[i, indices] = 1\n",
    "    \n",
    "    im2 = axes[1].imshow(selected.T, aspect='auto', cmap='RdYlGn', interpolation='nearest')\n",
    "    axes[1].set_xlabel('Token Position', fontsize=12)\n",
    "    axes[1].set_ylabel('Expert Index', fontsize=12)\n",
    "    axes[1].set_title('Selected Experts (Top-8) per Token', fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(im2, ax=axes[1], label='Selected (1) / Not Selected (0)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('olmoe_router_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ROUTER STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Average entropy: {-np.sum(router_probs * np.log(router_probs + 1e-10), axis=1).mean():.4f}\")\n",
    "    print(f\"Max probability (avg): {router_probs.max(axis=1).mean():.4f}\")\n",
    "    print(f\"Most commonly selected expert: {top_k_indices.flatten().argmax()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"Warning: Router logits not available. Make sure output_router_logits=True in model config.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance"
   },
   "source": [
    "## 7. Performance Analysis\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on our experiments, here's what we observed:\n",
    "\n",
    "#### Computational Cost\n",
    "\n",
    "- **8 experts (default):** Baseline performance\n",
    "- **16 experts:** ~1.5-2x slower\n",
    "- **32 experts:** ~3-4x slower  \n",
    "- **64 experts (all):** ~7-8x slower\n",
    "\n",
    "#### Quality vs Efficiency Trade-off\n",
    "\n",
    "| Configuration | Speed | Quality | Use Case |\n",
    "|--------------|-------|---------|----------|\n",
    "| 8 experts | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† | Real-time applications |\n",
    "| 16 experts | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† | Balanced performance |\n",
    "| 32 experts | â˜…â˜…â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | High-quality outputs |\n",
    "| 64 experts | â˜…â˜†â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | Maximum capacity |\n",
    "\n",
    "#### When to Use More Experts\n",
    "\n",
    "**Use more experts (16-32) when:**\n",
    "- Complex reasoning tasks require broad knowledge\n",
    "- Output quality is more important than latency\n",
    "- Multi-domain questions (code + math + science)\n",
    "- High uncertainty in domain classification\n",
    "\n",
    "**Use default (8) when:**\n",
    "- Real-time interaction is required\n",
    "- Domain is well-defined\n",
    "- Resource constraints exist\n",
    "- Batch processing many requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced"
   },
   "source": [
    "## 8. Advanced: Custom Routing Strategies\n",
    "\n",
    "### Dynamic Expert Selection\n",
    "\n",
    "Let's implement a more sophisticated approach where we dynamically adjust expert count based on input complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dynamic_routing"
   },
   "outputs": [],
   "source": [
    "def estimate_complexity(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Estimate input complexity and return recommended expert count.\n",
    "    \n",
    "    Heuristics:\n",
    "    - Long inputs â†’ more experts\n",
    "    - Technical keywords â†’ more experts\n",
    "    - Questions â†’ more experts\n",
    "    - Simple statements â†’ fewer experts\n",
    "    \"\"\"\n",
    "    score = 8  # Start with default\n",
    "    \n",
    "    # Length-based adjustment\n",
    "    if len(text) > 200:\n",
    "        score += 4\n",
    "    elif len(text) > 100:\n",
    "        score += 2\n",
    "    \n",
    "    # Technical keyword detection\n",
    "    technical_keywords = [\n",
    "        'algorithm', 'neural', 'quantum', 'cryptography', 'optimization',\n",
    "        'differential', 'inference', 'bayesian', 'stochastic', 'distributed'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    keyword_count = sum(1 for kw in technical_keywords if kw in text_lower)\n",
    "    score += keyword_count * 2\n",
    "    \n",
    "    # Question detection (may need multiple domains)\n",
    "    if '?' in text or text.lower().startswith(('what', 'how', 'why', 'explain', 'describe')):\n",
    "        score += 4\n",
    "    \n",
    "    # Cap at 64 (max experts)\n",
    "    score = min(score, 64)\n",
    "    \n",
    "    # Round to nearest valid config\n",
    "    valid_configs = [8, 16, 32, 64]\n",
    "    return min(valid_configs, key=lambda x: abs(x - score))\n",
    "\n",
    "\n",
    "def adaptive_generate(model, tokenizer, prompt, max_new_tokens=150):\n",
    "    \"\"\"\n",
    "    Generate text with adaptively selected expert count.\n",
    "    \"\"\"\n",
    "    num_experts = estimate_complexity(prompt)\n",
    "    \n",
    "    print(f\"Input complexity analysis:\")\n",
    "    print(f\"  Recommended experts: {num_experts}\")\n",
    "    print(f\"  Input length: {len(prompt)} chars\")\n",
    "    print()\n",
    "    \n",
    "    return generate_with_custom_experts(\n",
    "        model, tokenizer, prompt, num_experts, max_new_tokens\n",
    "    )\n",
    "\n",
    "\n",
    "# Test adaptive generation\n",
    "test_prompts = [\n",
    "    \"Hello, how are you?\",  # Simple â†’ 8 experts\n",
    "    \"Explain the differences between supervised and unsupervised learning in machine learning.\",  # Medium â†’ 16 experts\n",
    "    \"Describe the mathematical foundations of quantum computing, including the role of qubits, superposition, entanglement, and how quantum algorithms like Shor's algorithm achieve exponential speedup compared to classical algorithms.\",  # Complex â†’ 32+ experts\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADAPTIVE EXPERT SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Prompt: {prompt[:100]}...\" if len(prompt) > 100 else f\"Prompt: {prompt}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    text, gen_time, tokens = adaptive_generate(model, tokenizer, prompt, max_new_tokens=80)\n",
    "    \n",
    "    print(f\"Output (first 150 chars): {text[:150]}...\")\n",
    "    print(f\"Time: {gen_time:.2f}s | Tokens/sec: {tokens/gen_time:.2f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **OLMoE Architecture**\n",
    "   - 64 experts per layer, 8 selected by default\n",
    "   - Dropless token-choice routing\n",
    "   - Experts specialize in domains, vocabulary, and syntax\n",
    "\n",
    "2. **Modifying Expert Count**\n",
    "   - Change `config.num_experts_per_tok` before generation\n",
    "   - More experts = higher quality but slower inference\n",
    "   - Linear scaling: 2x experts â‰ˆ 2x computation\n",
    "\n",
    "3. **Optimal Configurations**\n",
    "   - **8 experts:** Default, best for production\n",
    "   - **16 experts:** Good balance for complex tasks\n",
    "   - **32 experts:** High quality, acceptable latency\n",
    "   - **64 experts:** Maximum capacity, research only\n",
    "\n",
    "4. **Implementation Tips**\n",
    "   - Always use FP16/BF16 for efficiency\n",
    "   - Enable `output_router_logits=True` for analysis\n",
    "   - Consider adaptive expert selection based on input\n",
    "   - Monitor GPU memory with larger expert counts\n",
    "\n",
    "### Code Template for Production\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"allenai/OLMoE-1B-7B-0924\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n",
    "\n",
    "# Modify expert count\n",
    "model.config.num_experts_per_tok = 16  # Use 16 instead of default 8\n",
    "\n",
    "# Generate\n",
    "inputs = tokenizer(\"Your prompt here\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\n",
    "### References\n",
    "\n",
    "- **OLMoE Paper:** https://arxiv.org/abs/2409.02060\n",
    "- **GitHub:** https://github.com/allenai/OLMoE\n",
    "- **Hugging Face:** https://huggingface.co/allenai/OLMoE-1B-7B-0924\n",
    "- **Documentation:** https://huggingface.co/docs/transformers/model_doc/olmoe\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or Issues?**\n",
    "\n",
    "Feel free to:\n",
    "- Open issues on the OLMoE GitHub repository\n",
    "- Consult the Hugging Face documentation\n",
    "- Experiment with different expert configurations for your use case\n",
    "\n",
    "**Happy experimenting with OLMoE!** ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
