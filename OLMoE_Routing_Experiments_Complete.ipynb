{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLMoE Routing Experiments - Complete End-to-End Notebook\n",
    "\n",
    "**Complete workflow from installation to results analysis**\n",
    "\n",
    "This notebook runs on:\n",
    "- ‚úÖ **Google Colab** (Recommended - GPU enabled)\n",
    "- ‚úÖ Local Jupyter (GPU or CPU)\n",
    "- ‚úÖ Kaggle, Paperspace, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start (Google Colab)\n",
    "\n",
    "### Recommended: Run in Google Drive\n",
    "1. Upload this notebook to Google Drive\n",
    "2. Open with Google Colab\n",
    "3. Enable GPU: `Runtime ‚Üí Change runtime type ‚Üí GPU`\n",
    "4. Run all cells - the notebook will:\n",
    "   - Mount your Google Drive\n",
    "   - Clone/update the GitHub repository to Drive\n",
    "   - Install dependencies\n",
    "   - Run experiments with **internal routing logs**\n",
    "   - Save results to Drive\n",
    "\n",
    "**Benefits:**\n",
    "- Repository persists in Drive (only clone once!)\n",
    "- Results saved to Drive automatically\n",
    "- **Internal router_logits logged for every sample**\n",
    "- Faster subsequent runs (repo already available)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã What This Notebook Does\n",
    "\n",
    "1. **Environment Detection**: Auto-detects Colab/Kaggle/local\n",
    "2. **Repository Setup**: Clones repo to Google Drive (or uses existing)\n",
    "3. **GPU Configuration**: Configures GPU/CPU automatically\n",
    "4. **Installation**: Installs all required packages\n",
    "5. **Custom Routing**: Implements custom expert selection with internal logging\n",
    "6. **Internal Logs Demo**: Shows exactly what gets logged\n",
    "7. **Experiments**: Tests different routing strategies and expert counts\n",
    "8. **Two-Phase Analysis**: Analyzes baseline, then tests modified routing\n",
    "9. **Complete Visibility**: Access router_logits for every decision\n",
    "10. **Comprehensive Reports**: Generates visualizations and detailed reports\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ File Locations (Google Drive)\n",
    "\n",
    "When running in Colab, files are stored in:\n",
    "- **Repository**: `/content/drive/MyDrive/MOE-with-feature-selection/`\n",
    "- **Results**: `/content/drive/MyDrive/olmoe_experiments/`\n",
    "- **Internal Logs**: `olmoe_experiments/*/logs/*_internal_routing.json`\n",
    "\n",
    "Your results AND internal logs persist across Colab sessions!\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [GPU Configuration](#2-gpu-configuration)\n",
    "3. [Installation](#3-installation)\n",
    "4. [Custom Expert Selection & Model Patching](#4-custom-expert-selection--model-patching)\n",
    "   - **4.5. Internal Logs Overview & Demo** (NEW!)\n",
    "5. [Framework Setup](#5-framework-setup)\n",
    "6. [Running Tests](#6-running-tests)\n",
    "7. [Quick Experiment](#7-quick-experiment)\n",
    "   - **View Internal Routing Logs** (NEW!)\n",
    "   - **Analyze Routing Patterns** (NEW!)\n",
    "8. [**Two-Phase Routing Experiment**](#8-two-phase-routing-experiment-new)\n",
    "   - **Baseline Internal Routing Behavior** (NEW!)\n",
    "9. [Full Experiments](#9-full-experiments)\n",
    "10. [Results Analysis](#10-results-analysis)\n",
    "11. [Visualization](#11-visualization)\n",
    "12. [**Internal Logs Deep Dive**](#12-internal-logs-deep-dive) (NEW!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Detect environment and configure accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "print(f\"Running in Kaggle: {IN_KAGGLE}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Set working directory based on environment\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive first\n",
    "    from google.colab import drive\n",
    "    print(\"\\nüìÅ Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set working directory in Google Drive\n",
    "    WORK_DIR = '/content/drive/MyDrive/olmoe_experiments'\n",
    "    # Repository will be cloned to Google Drive (persists across sessions)\n",
    "    REPO_DIR = '/content/drive/MyDrive/MOE-with-feature-selection'\n",
    "else:\n",
    "    # Local environment\n",
    "    WORK_DIR = './olmoe_experiments'\n",
    "    REPO_DIR = None\n",
    "\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Store repo directory for later use\n",
    "if IN_COLAB:\n",
    "    print(f\"‚úÖ Repository location: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Configuration\n",
    "\n",
    "Check GPU availability and configure for optimal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GPU CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "Number of GPUs: 1\n",
      "\n",
      "GPU 0: NVIDIA A100-SXM4-40GB\n",
      "  Memory: 42.47 GB\n",
      "\n",
      "‚úÖ GPU is ready!\n",
      "\n",
      "Device for experiments: cuda\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check CUDA availability\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\nCUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Set device\n",
    "    device = 'cuda'\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n‚úÖ GPU is ready!\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"\\n‚ö†Ô∏è  GPU not available. Using CPU (will be slower).\")\n",
    "    if IN_COLAB:\n",
    "        print(\"\\nüí° TIP: Enable GPU in Colab:\")\n",
    "        print(\"   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nDevice for experiments: {device}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Installation\n",
    "\n",
    "Install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages installed!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install dependencies\n",
    "pip install -q torch transformers datasets pandas numpy matplotlib seaborn tqdm rich \n",
    "echo \"‚úÖ All packages installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Versions:\n",
      "  torch: 2.8.0+cu126\n",
      "  transformers: 4.57.1\n",
      "  datasets: 4.0.0\n",
      "  pandas: 2.2.2\n",
      "  numpy: 2.0.2\n",
      "\n",
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Verify installations\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  torch: {torch.__version__}\")\n",
    "print(f\"  transformers: {transformers.__version__}\")\n",
    "print(f\"  datasets: {datasets.__version__}\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(\"\\n‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Expert Selection & Model Patching\n",
    "\n",
    "**NEW: Support for custom forward pass with internal logging**\n",
    "\n",
    "This section implements:\n",
    "1. Custom expert selection (uniform weights)\n",
    "2. Model patching to return router_logits\n",
    "3. Internal logging of routing decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom expert selection functions defined!\n",
      "\n",
      "Functions available:\n",
      "  - custom_select_experts()\n",
      "  - create_custom_forward()\n",
      "  - patch_model_with_custom_routing()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "def custom_select_experts(\n",
    "    router_logits: torch.Tensor,\n",
    "    top_k: int,\n",
    "    num_experts: int\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Custom expert selection with uniform weights.\n",
    "    \n",
    "    This is equivalent to UniformRouting but integrated directly into the model.\n",
    "    \n",
    "    Args:\n",
    "        router_logits: [tokens, num_experts] - Raw routing scores\n",
    "        top_k: Number of experts to select\n",
    "        num_experts: Total number of experts\n",
    "    \n",
    "    Returns:\n",
    "        routing_weights: [tokens, top_k] - Uniform weights (1/top_k)\n",
    "        selected_experts: [tokens, top_k] - Selected expert indices\n",
    "    \"\"\"\n",
    "    # Convert logits to probabilities\n",
    "    routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "    \n",
    "    # Select top-k experts based on probabilities\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "    \n",
    "    # KEY: Give each selected expert EQUAL probability (uniform routing)\n",
    "    routing_weights = torch.ones_like(selected_experts, dtype=torch.float)\n",
    "    routing_weights /= top_k\n",
    "    \n",
    "    return routing_weights.to(router_logits.dtype), selected_experts\n",
    "\n",
    "\n",
    "def create_custom_forward(original_forward, top_k, num_experts):\n",
    "    \"\"\"\n",
    "    Create a custom forward pass that:\n",
    "    1. Uses custom_select_experts for routing\n",
    "    2. Returns router_logits for analysis\n",
    "    3. Enables internal logging of routing decisions\n",
    "    \"\"\"\n",
    "    def new_forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "        hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "        \n",
    "        # Get router logits: (batch * sequence_length, n_experts)\n",
    "        router_logits = self.gate(hidden_states)\n",
    "\n",
    "        # Use custom expert selection\n",
    "        routing_weights, selected_experts = custom_select_experts(\n",
    "            router_logits,\n",
    "            top_k=top_k,\n",
    "            num_experts=num_experts\n",
    "        )\n",
    "\n",
    "        # Initialize output\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, hidden_dim), \n",
    "            dtype=hidden_states.dtype, \n",
    "            device=hidden_states.device\n",
    "        )\n",
    "\n",
    "        # Create expert mask for efficient indexing\n",
    "        # expert_mask: [num_experts, top_k, tokens]\n",
    "        expert_mask = torch.nn.functional.one_hot(\n",
    "            selected_experts, \n",
    "            num_classes=num_experts\n",
    "        ).permute(2, 1, 0)\n",
    "\n",
    "        # Process each expert\n",
    "        for expert_idx in range(num_experts):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            \n",
    "            # Get tokens assigned to this expert\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            if top_x.numel() == 0:\n",
    "                continue  # No tokens for this expert\n",
    "\n",
    "            # Compute expert output with routing weights\n",
    "            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "\n",
    "            # Accumulate results\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "        \n",
    "        # Reshape output\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        \n",
    "        # Return both output and router_logits for analysis\n",
    "        return final_hidden_states, router_logits\n",
    "    \n",
    "    return new_forward\n",
    "\n",
    "\n",
    "def patch_model_with_custom_routing(model, top_k=None):\n",
    "    \"\"\"\n",
    "    Patch OLMoE model to use custom routing with internal logging.\n",
    "    \n",
    "    Args:\n",
    "        model: OLMoE model instance\n",
    "        top_k: Number of experts to use (None = use model default)\n",
    "    \"\"\"\n",
    "    if top_k is None:\n",
    "        top_k = getattr(model.config, 'num_experts_per_tok', 8)\n",
    "    \n",
    "    num_experts = getattr(model.config, 'num_local_experts', 64)\n",
    "    \n",
    "    print(f\"Patching model with custom routing:\")\n",
    "    print(f\"  top_k: {top_k}\")\n",
    "    print(f\"  num_experts: {num_experts}\")\n",
    "    \n",
    "    patched_layers = 0\n",
    "    \n",
    "    # Patch all MoE layers\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        if hasattr(layer, 'mlp') and hasattr(layer.mlp, 'experts'):\n",
    "            # Save original forward\n",
    "            original_forward = layer.mlp.forward\n",
    "            \n",
    "            # Create and apply custom forward\n",
    "            layer.mlp.forward = create_custom_forward(\n",
    "                original_forward, \n",
    "                top_k, \n",
    "                num_experts\n",
    "            ).__get__(layer.mlp, layer.mlp.__class__)\n",
    "            \n",
    "            patched_layers += 1\n",
    "    \n",
    "    print(f\"‚úÖ Patched {patched_layers} MoE layers\")\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"‚úÖ Custom expert selection functions defined!\")\n",
    "print(\"\\nFunctions available:\")\n",
    "print(\"  - custom_select_experts()\")\n",
    "print(\"  - create_custom_forward()\")\n",
    "print(\"  - patch_model_with_custom_routing()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Internal Logs Overview & Demo\n",
    "\n",
    "**What are Internal Logs?**\n",
    "\n",
    "Internal logs capture the model's routing decisions at every layer:\n",
    "- **router_logits**: Raw routing scores for each token\n",
    "- **selected_experts**: Which experts were chosen\n",
    "- **expert_weights**: How much weight each expert received\n",
    "- **Layer-by-layer data**: Routing decisions for all 16 transformer layers\n",
    "\n",
    "**Why are they useful?**\n",
    "- Understand which experts specialize in what\n",
    "- Detect routing patterns and biases\n",
    "- Debug routing strategies\n",
    "- Validate that modifications work correctly\n",
    "\n",
    "**File structure:**\n",
    "- `logs/{config}_{dataset}.json` - Summary metrics\n",
    "- `logs/{config}_{dataset}_internal_routing.json` - Detailed routing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INTERNAL ROUTING LOGS DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulate router logits for 1 token across 64 experts\n",
    "torch.manual_seed(42)\n",
    "router_logits = torch.randn(1, 64)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Router Logits (raw scores):\")\n",
    "print(f\"   Shape: {router_logits.shape}\")\n",
    "print(f\"   Sample values: {[f'{v:.4f}' for v in router_logits[0, :5].tolist()]}\")\n",
    "\n",
    "# Convert to probabilities\n",
    "probs = F.softmax(router_logits, dim=-1)\n",
    "print(\"\\n2Ô∏è‚É£ Softmax Probabilities:\")\n",
    "print(f\"   Top 5 probs: {[f'{v:.4f}' for v in torch.topk(probs, 5)[0][0].tolist()]}\")\n",
    "print(f\"   Sum: {probs.sum().item():.6f} (should be 1.0)\")\n",
    "\n",
    "# Select top-8 experts\n",
    "top_k = 8\n",
    "weights, indices = torch.topk(probs, top_k)\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ Top-{top_k} Expert Selection:\")\n",
    "print(f\"   Selected experts: {indices[0].tolist()}\")\n",
    "print(f\"   Their weights: {[f'{w:.4f}' for w in weights[0].tolist()]}\")\n",
    "print(f\"   Weight sum: {weights.sum().item():.6f}\")\n",
    "\n",
    "# Uniform routing (what custom patching does)\n",
    "uniform_weights = torch.ones_like(weights) / top_k\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£ Uniform Routing (Custom Patching):\")\n",
    "print(f\"   Equal weights: {[f'{w:.4f}' for w in uniform_weights[0].tolist()]}\")\n",
    "print(f\"   Each expert gets: {1/top_k:.4f}\")\n",
    "\n",
    "print(\"\\nüí° These are the values saved in internal_routing.json files!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Framework Setup\n",
    "\n",
    "**Clone/Update repository and load the routing experiments framework**\n",
    "\n",
    "This will:\n",
    "1. Check if repository exists in Google Drive\n",
    "2. Clone if needed, or pull latest changes if it exists\n",
    "3. Add repository to Python path\n",
    "4. Import all framework modules\n",
    "\n",
    "**Note:** Repository is stored in Google Drive and persists across sessions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FRAMEWORK SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: Clone repository if in Colab\n",
    "if IN_COLAB:\n",
    "    # Check if repo already exists in Google Drive\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        print(f\"\\nüìÇ Step 1: Repository already exists in Google Drive\")\n",
    "        print(f\"   Location: {REPO_DIR}\")\n",
    "        print(f\"   ‚úÖ Using existing repository\")\n",
    "        \n",
    "        # Optionally pull latest changes\n",
    "        print(f\"\\n   Pulling latest changes...\")\n",
    "        !cd {REPO_DIR} && git pull\n",
    "    else:\n",
    "        print(\"\\nüì• Step 1: Cloning GitHub repository...\")\n",
    "        print(f\"   Cloning to: {REPO_DIR}\")\n",
    "        !git clone https://github.com/aliabbasjaffri/MOE-with-feature-selection.git {REPO_DIR}\n",
    "        \n",
    "        # Verify clone was successful\n",
    "        if os.path.exists(REPO_DIR):\n",
    "            print(f\"   ‚úÖ Repository cloned successfully!\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed to clone repository!\")\n",
    "            raise Exception(\"Repository clone failed\")\n",
    "    \n",
    "    # List main files\n",
    "    print(f\"\\n   üìÇ Repository contents:\")\n",
    "    !ls -la {REPO_DIR}/*.py | head -5\n",
    "    \n",
    "    framework_dir = REPO_DIR\n",
    "else:\n",
    "    # Local environment - assume we're already in the repo\n",
    "    framework_dir = os.path.abspath('.')\n",
    "    print(f\"\\nüìÇ Step 1: Using local directory: {framework_dir}\")\n",
    "\n",
    "# Step 2: Add to Python path\n",
    "print(f\"\\nüì¶ Step 2: Adding to Python path...\")\n",
    "if framework_dir not in sys.path:\n",
    "    sys.path.insert(0, framework_dir)\n",
    "    print(f\"   ‚úÖ Added: {framework_dir}\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Already in path: {framework_dir}\")\n",
    "\n",
    "print(f\"\\n   Python path (first 3 entries):\")\n",
    "for i, p in enumerate(sys.path[:3], 1):\n",
    "    print(f\"     {i}. {p}\")\n",
    "\n",
    "# Step 3: Verify framework file exists\n",
    "print(f\"\\nüîç Step 3: Verifying framework file...\")\n",
    "framework_file = os.path.join(framework_dir, 'olmoe_routing_experiments.py')\n",
    "if os.path.exists(framework_file):\n",
    "    file_size = os.path.getsize(framework_file)\n",
    "    print(f\"   ‚úÖ Found: olmoe_routing_experiments.py ({file_size:,} bytes)\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Not found: {framework_file}\")\n",
    "    print(f\"\\n   Available Python files:\")\n",
    "    !ls -la {framework_dir}/*.py\n",
    "    raise Exception(\"Framework file not found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ FRAMEWORK SETUP COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"IMPORTING FRAMEWORK MODULES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Clear any cached imports\n",
    "if 'olmoe_routing_experiments' in sys.modules:\n",
    "    print(\"üîÑ Clearing cached module...\")\n",
    "    del sys.modules['olmoe_routing_experiments']\n",
    "\n",
    "# Import the framework\n",
    "print(\"\\nüì¶ Importing olmoe_routing_experiments...\")\n",
    "\n",
    "try:\n",
    "    import olmoe_routing_experiments\n",
    "    print(\"‚úÖ Module imported successfully!\")\n",
    "    \n",
    "    # Import specific classes\n",
    "    from olmoe_routing_experiments import (\n",
    "        RoutingConfig,\n",
    "        ExperimentResults,\n",
    "        RoutingStrategy,\n",
    "        RegularRouting,\n",
    "        NormalizedRouting,\n",
    "        UniformRouting,\n",
    "        AdaptiveRouting,\n",
    "        RoutingExperimentRunner,\n",
    "        ModelPatchingUtils\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìö Available Components:\")\n",
    "    print(\"  ‚Ä¢ RoutingConfig, ExperimentResults\")\n",
    "    print(\"  ‚Ä¢ RoutingStrategy (base class)\")\n",
    "    print(\"  ‚Ä¢ RegularRouting, NormalizedRouting\")\n",
    "    print(\"  ‚Ä¢ UniformRouting, AdaptiveRouting\")\n",
    "    print(\"  ‚Ä¢ RoutingExperimentRunner\")\n",
    "    print(\"  ‚Ä¢ ModelPatchingUtils\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ ALL IMPORTS SUCCESSFUL\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"\\n‚ùå Import failed: {e}\")\n",
    "    print(\"\\nüîß Debugging info:\")\n",
    "    print(f\"   sys.path[0]: {sys.path[0]}\")\n",
    "    print(f\"   Current dir: {os.getcwd()}\")\n",
    "    \n",
    "    # List Python files in path\n",
    "    import glob\n",
    "    py_files = glob.glob(os.path.join(sys.path[0], '*.py'))\n",
    "    print(f\"\\n   Python files in {sys.path[0]}:\")\n",
    "    for f in py_files[:10]:\n",
    "        print(f\"     - {os.path.basename(f)}\")\n",
    "    \n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Framework with Custom Patching Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedRoutingExperimentRunner(RoutingExperimentRunner):\n",
    "    \"\"\"\n",
    "    Extended runner with support for custom model patching.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, use_custom_routing=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.use_custom_routing = use_custom_routing\n",
    "        \n",
    "        if use_custom_routing:\n",
    "            print(\"\\nüîß CUSTOM ROUTING MODE ENABLED\")\n",
    "            print(\"   Model will be patched with custom_select_experts\")\n",
    "    \n",
    "    def _set_expert_count(self, num_experts):\n",
    "        \"\"\"Override to support custom routing.\"\"\"\n",
    "        super()._set_expert_count(num_experts)\n",
    "        \n",
    "        if self.use_custom_routing:\n",
    "            # Re-patch model with new expert count\n",
    "            patch_model_with_custom_routing(self.model, top_k=num_experts)\n",
    "\n",
    "print(\"‚úÖ Extended framework with custom patching support ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running Tests\n",
    "\n",
    "Validate the framework is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RUNNING VALIDATION TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: Routing Strategies\n",
    "print(\"\\n[1/5] Testing routing strategies...\")\n",
    "torch.manual_seed(42)\n",
    "logits = torch.randn(1, 1, 64)\n",
    "\n",
    "regular = RegularRouting(num_experts=8)\n",
    "normalized = NormalizedRouting(num_experts=8)\n",
    "uniform = UniformRouting(num_experts=8)\n",
    "\n",
    "reg_indices, reg_weights = regular.route(logits)\n",
    "norm_indices, norm_weights = normalized.route(logits)\n",
    "uni_indices, uni_weights = uniform.route(logits)\n",
    "\n",
    "# Verify uniform has equal weights\n",
    "assert torch.allclose(uni_weights, torch.ones_like(uni_weights) / 8, atol=1e-6)\n",
    "print(\"   ‚úÖ Routing strategies work correctly\")\n",
    "\n",
    "# Test 2: Custom Expert Selection\n",
    "print(\"\\n[2/5] Testing custom expert selection...\")\n",
    "router_logits = torch.randn(10, 64)  # 10 tokens, 64 experts\n",
    "weights, indices = custom_select_experts(router_logits, top_k=8, num_experts=64)\n",
    "\n",
    "assert weights.shape == (10, 8)\n",
    "assert indices.shape == (10, 8)\n",
    "assert torch.allclose(weights, torch.ones_like(weights) / 8, atol=1e-6)\n",
    "print(\"   ‚úÖ Custom expert selection works correctly\")\n",
    "\n",
    "# Test 3: Different expert counts produce different routing\n",
    "print(\"\\n[3/5] Testing expert count variation...\")\n",
    "weights_4, indices_4 = custom_select_experts(router_logits, top_k=4, num_experts=64)\n",
    "weights_16, indices_16 = custom_select_experts(router_logits, top_k=16, num_experts=64)\n",
    "\n",
    "assert weights_4.shape[-1] == 4\n",
    "assert weights_16.shape[-1] == 16\n",
    "print(\"   ‚úÖ Different expert counts work correctly\")\n",
    "\n",
    "# Test 4: Verify uniform weights\n",
    "print(\"\\n[4/5] Testing uniform weight distribution...\")\n",
    "for k in [4, 8, 16]:\n",
    "    w, _ = custom_select_experts(router_logits, top_k=k, num_experts=64)\n",
    "    expected = torch.ones_like(w) / k\n",
    "    assert torch.allclose(w, expected, atol=1e-6)\n",
    "print(\"   ‚úÖ Uniform weights verified for all k values\")\n",
    "\n",
    "# Test 5: Statistics tracking\n",
    "print(\"\\n[5/5] Testing statistics tracking...\")\n",
    "strategy = UniformRouting(num_experts=8)\n",
    "for _ in range(5):\n",
    "    test_logits = torch.randn(2, 5, 64)\n",
    "    strategy.route(test_logits)\n",
    "\n",
    "stats = strategy.get_summary_stats()\n",
    "assert 'avg_entropy' in stats\n",
    "assert 'avg_concentration' in stats\n",
    "assert 'unique_experts' in stats\n",
    "print(\"   ‚úÖ Statistics tracking works correctly\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL TESTS PASSED!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Experiment\n",
    "\n",
    "Run a minimal experiment to verify everything works (~5 minutes)\n",
    "\n",
    "**This experiment will:**\n",
    "- Test 2 expert counts (8, 16)\n",
    "- Test 2 strategies (regular, custom uniform)\n",
    "- Evaluate on 50 samples\n",
    "- Generate visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"QUICK EXPERIMENT (Standard Routing)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"  Expert counts: [8, 16]\")\n",
    "print(\"  Strategies: [regular, uniform]\")\n",
    "print(\"  Dataset: WikiText-2\")\n",
    "print(\"  Samples: 50\")\n",
    "print(\"  Estimated time: ~5 minutes\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Create runner (standard mode)\n",
    "runner_standard = RoutingExperimentRunner(\n",
    "    model_name=\"allenai/OLMoE-1B-7B-0924\",\n",
    "    device=device,\n",
    "    output_dir=\"./quick_experiment_standard\"\n",
    ")\n",
    "\n",
    "# Run experiments\n",
    "results_df_standard = runner_standard.run_all_experiments(\n",
    "    expert_counts=[8, 16],\n",
    "    strategies=['regular', 'uniform'],\n",
    "    datasets=['wikitext'],\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Standard routing experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nüìä QUICK EXPERIMENT RESULTS (Standard Routing)\\n\")\n",
    "print(results_df_standard[[\n",
    "    'config', 'perplexity', 'token_accuracy', \n",
    "    'tokens_per_second', 'avg_entropy'\n",
    "]].to_string(index=False))\n",
    "\n",
    "# Best configuration\n",
    "best_idx = results_df_standard['perplexity'].idxmin()\n",
    "best = results_df_standard.loc[best_idx]\n",
    "\n",
    "print(\"\\nüèÜ BEST CONFIGURATION:\")\n",
    "print(f\"   Config: {best['config']}\")\n",
    "print(f\"   Perplexity: {best['perplexity']:.2f}\")\n",
    "print(f\"   Accuracy: {best['token_accuracy']:.4f}\")\n",
    "print(f\"   Speed: {best['tokens_per_second']:.1f} tok/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Experiment with Custom Routing (Internal Logging)\n",
    "\n",
    "**This uses the custom forward pass with internal router_logits logging**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Internal Routing Logs\n",
    "\n",
    "See what's actually logged for each configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VIEWING INTERNAL ROUTING LOGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find internal routing log files\n",
    "logs_dir = Path(\"./quick_experiment_standard/logs\")\n",
    "internal_log_files = list(logs_dir.glob(\"*_internal_routing.json\"))\n",
    "\n",
    "if internal_log_files:\n",
    "    print(f\"\\n‚úÖ Found {len(internal_log_files)} internal routing log files:\")\n",
    "    for f in internal_log_files:\n",
    "        print(f\"   ‚Ä¢ {f.name}\")\n",
    "    \n",
    "    # Load and display first one\n",
    "    log_file = internal_log_files[0]\n",
    "    print(f\"\\nüìÇ Loading: {log_file.name}\")\n",
    "    \n",
    "    with open(log_file, 'r') as f:\n",
    "        internal_logs = json.load(f)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Config: {internal_logs['config']}\")\n",
    "    print(f\"   Strategy: {internal_logs['strategy']}\")\n",
    "    print(f\"   Num Experts: {internal_logs['num_experts']}\")\n",
    "    print(f\"   Dataset: {internal_logs['dataset']}\")\n",
    "    print(f\"   Total Samples: {internal_logs['summary']['total_samples']}\")\n",
    "    print(f\"   Total Tokens: {internal_logs['summary']['total_tokens']}\")\n",
    "    \n",
    "    # Show first sample\n",
    "    if internal_logs['samples']:\n",
    "        first_sample = internal_logs['samples'][0]\n",
    "        print(f\"\\nüîç First Sample Details:\")\n",
    "        print(f\"   Sample ID: {first_sample['sample_id']}\")\n",
    "        print(f\"   Num Tokens: {first_sample['num_tokens']}\")\n",
    "        print(f\"   Num Layers: {len(first_sample['layers'])}\")\n",
    "        \n",
    "        # Show first layer\n",
    "        first_layer = first_sample['layers'][0]\n",
    "        print(f\"\\n   Layer 0 Routing:\")\n",
    "        print(f\"      Router logits shape: {first_layer['router_logits_shape']}\")\n",
    "        print(f\"      Selected experts (token 0): {first_layer['selected_experts'][0][0]}\")\n",
    "        print(f\"      Expert weights (token 0): {[f'{w:.4f}' for w in first_layer['expert_weights'][0][0]]}\")\n",
    "        \n",
    "        print(f\"\\nüí° Full routing data available in: {log_file}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No internal routing logs found.\")\n",
    "    print(\"   Set save_internal_logs=True in evaluate_configuration()\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Routing Patterns from Internal Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if internal_log_files:\n",
    "    print(\"=\"*70)\n",
    "    print(\"ANALYZING ROUTING PATTERNS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Aggregate expert usage across all samples\n",
    "    expert_usage = np.zeros(64)\n",
    "    \n",
    "    for sample in internal_logs['samples'][:10]:  # First 10 samples\n",
    "        for layer_data in sample['layers']:\n",
    "            selected_experts = layer_data['selected_experts']\n",
    "            for token_experts in selected_experts:\n",
    "                for token_expert_list in token_experts:\n",
    "                    for expert_id in token_expert_list:\n",
    "                        expert_usage[expert_id] += 1\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Expert usage histogram\n",
    "    ax1 = axes[0]\n",
    "    ax1.bar(range(64), expert_usage, alpha=0.7, color='steelblue')\n",
    "    ax1.set_xlabel('Expert ID')\n",
    "    ax1.set_ylabel('Activation Count')\n",
    "    ax1.set_title(f'Expert Usage Pattern\\\\n({internal_logs[\"config\"]})')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Usage distribution\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(expert_usage, bins=20, alpha=0.7, color='coral', edgecolor='black')\n",
    "    ax2.set_xlabel('Activation Count')\n",
    "    ax2.set_ylabel('Number of Experts')\n",
    "    ax2.set_title('Distribution of Expert Utilization')\n",
    "    ax2.axvline(expert_usage.mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {expert_usage.mean():.1f}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('internal_logs_routing_pattern.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nüìä Expert Usage Statistics:\")\n",
    "    print(f\"   Most used expert: {expert_usage.argmax()} ({expert_usage.max():.0f} activations)\")\n",
    "    print(f\"   Least used expert: {expert_usage.argmin()} ({expert_usage.min():.0f} activations)\")\n",
    "    print(f\"   Average usage: {expert_usage.mean():.1f}\")\n",
    "    print(f\"   Std deviation: {expert_usage.std():.1f}\")\n",
    "    print(f\"   Unused experts: {(expert_usage == 0).sum()}\")\n",
    "    \n",
    "    print(\"\\nüí° Analysis shows which experts are preferred by the routing mechanism!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"QUICK EXPERIMENT (Custom Routing with Internal Logging)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"  Expert counts: [8, 16]\")\n",
    "print(\"  Routing: Custom uniform (patched forward pass)\")\n",
    "print(\"  Dataset: WikiText-2\")\n",
    "print(\"  Samples: 50\")\n",
    "print(\"  Features: Internal router_logits logging\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Create runner with custom routing enabled\n",
    "runner_custom = ExtendedRoutingExperimentRunner(\n",
    "    model_name=\"allenai/OLMoE-1B-7B-0924\",\n",
    "    device=device,\n",
    "    output_dir=\"./quick_experiment_custom\",\n",
    "    use_custom_routing=True\n",
    ")\n",
    "\n",
    "# Patch the model\n",
    "patch_model_with_custom_routing(runner_custom.model, top_k=8)\n",
    "\n",
    "# Run experiments (only uniform strategy makes sense with custom routing)\n",
    "results_df_custom = runner_custom.run_all_experiments(\n",
    "    expert_counts=[8, 16],\n",
    "    strategies=['uniform'],  # Custom routing is uniform\n",
    "    datasets=['wikitext'],\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Custom routing experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nüìä QUICK EXPERIMENT RESULTS (Custom Routing)\\n\")\n",
    "print(results_df_custom[[\n",
    "    'config', 'perplexity', 'token_accuracy', \n",
    "    'tokens_per_second', 'avg_entropy'\n",
    "]].to_string(index=False))\n",
    "\n",
    "# Compare with standard\n",
    "print(\"\\nüìà COMPARISON: Custom vs Standard Uniform Routing\\n\")\n",
    "\n",
    "comparison_data = []\n",
    "for expert_count in [8, 16]:\n",
    "    std_row = results_df_standard[\n",
    "        (results_df_standard['num_experts'] == expert_count) & \n",
    "        (results_df_standard['strategy'] == 'uniform')\n",
    "    ].iloc[0]\n",
    "    \n",
    "    custom_row = results_df_custom[\n",
    "        results_df_custom['num_experts'] == expert_count\n",
    "    ].iloc[0]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Expert Count': expert_count,\n",
    "        'Standard PPL': f\"{std_row['perplexity']:.2f}\",\n",
    "        'Custom PPL': f\"{custom_row['perplexity']:.2f}\",\n",
    "        'Difference': f\"{custom_row['perplexity'] - std_row['perplexity']:.2f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nüí° Note: Small differences are expected due to implementation details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Two-Phase Routing Experiment (NEW!)\n",
    "\n",
    "**üî¨ Advanced: Analyze baseline routing, then test modifications**\n",
    "\n",
    "This experiment uses a comprehensive 3-phase approach:\n",
    "1. **Phase 1**: Analyze model's natural routing (baseline)\n",
    "2. **Phase 2**: Apply modified routing strategies\n",
    "3. **Phase 3**: Compare and generate detailed reports\n",
    "\n",
    "This tells you:\n",
    "- How does the model naturally route tokens to experts?\n",
    "- Which experts are underutilized?\n",
    "- Does custom routing improve or hurt quality?\n",
    "- What's the impact of forcing uniform weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TWO-PHASE ROUTING EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"  Expert counts: [8, 16]\")\n",
    "print(\"  Datasets: [wikitext]\")\n",
    "print(\"  Samples: 50\")\n",
    "print(\"  Modifications: [uniform, normalized]\")\n",
    "print(\"\\nPhases:\")\n",
    "print(\"  1. Baseline: Analyze model's natural routing\")\n",
    "print(\"  2. Modified: Test custom routing strategies\")\n",
    "print(\"  3. Analysis: Compare and generate report\")\n",
    "print(\"\\nEstimated time: ~10 minutes\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create runner\n",
    "runner_two_phase = RoutingExperimentRunner(\n",
    "    model_name=\"allenai/OLMoE-1B-7B-0924\",\n",
    "    device=device,\n",
    "    output_dir=\"./two_phase_experiment\"\n",
    ")\n",
    "\n",
    "# Run two-phase experiment\n",
    "results_df, routing_insights = runner_two_phase.run_two_phase_experiment(\n",
    "    expert_counts=[8, 16],\n",
    "    datasets=['wikitext'],\n",
    "    max_samples=50,\n",
    "    routing_modifications=['uniform', 'normalized']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Two-phase experiment complete!\")\n",
    "print(f\"   Results saved to: {runner_two_phase.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Baseline Insights\n",
    "\n",
    "View what the model naturally does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: BASELINE ROUTING INSIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display insights\n",
    "insights_data = []\n",
    "for key, insight in routing_insights.items():\n",
    "    insights_data.append({\n",
    "        'Experts': insight['num_experts'],\n",
    "        'Dataset': insight['dataset'],\n",
    "        'Perplexity': f\"{insight['baseline_perplexity']:.2f}\",\n",
    "        'Accuracy': f\"{insight['baseline_accuracy']:.4f}\",\n",
    "        'Experts Used': f\"{insight['unique_experts_used']}/64\",\n",
    "        'Utilization': f\"{insight['expert_utilization']:.1%}\",\n",
    "        'Entropy': f\"{insight['avg_entropy']:.3f}\"\n",
    "    })\n",
    "\n",
    "insights_df = pd.DataFrame(insights_data)\n",
    "print(\"\\n\")\n",
    "print(insights_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nüìä Key Observations:\")\n",
    "avg_util = np.mean([i['expert_utilization'] for i in routing_insights.values()])\n",
    "avg_entropy = np.mean([i['avg_entropy'] for i in routing_insights.values()])\n",
    "\n",
    "print(f\"  ‚Ä¢ Average expert utilization: {avg_util:.1%}\")\n",
    "print(f\"  ‚Ä¢ Average routing entropy: {avg_entropy:.3f}\")\n",
    "\n",
    "if avg_util < 0.5:\n",
    "    print(f\"  ‚ö†Ô∏è  Many experts are underutilized!\")\n",
    "    print(f\"      ‚Üí Custom routing may help balance usage\")\n",
    "\n",
    "if avg_entropy < 1.0:\n",
    "    print(f\"  ‚ö†Ô∏è  Routing is highly concentrated!\")\n",
    "    print(f\"      ‚Üí Model heavily favors certain experts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Internal Routing Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BASELINE: INTERNAL ROUTING BEHAVIOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load baseline internal logs\n",
    "baseline_logs_dir = Path(\"./two_phase_experiment/logs\")\n",
    "baseline_files = list(baseline_logs_dir.glob(\"*baseline*_internal_routing.json\"))\n",
    "\n",
    "if baseline_files:\n",
    "    # Load first baseline log\n",
    "    with open(baseline_files[0], 'r') as f:\n",
    "        baseline_internal = json.load(f)\n",
    "    \n",
    "    print(f\"\\nüìÇ Analyzing: {baseline_files[0].name}\\n\")\n",
    "    \n",
    "    # Analyze weight concentration\n",
    "    all_weights = []\n",
    "    all_entropies = []\n",
    "    \n",
    "    for sample in baseline_internal['samples'][:20]:  # First 20 samples\n",
    "        for layer_data in sample['layers']:\n",
    "            weights = layer_data['expert_weights']\n",
    "            for token_weights in weights:\n",
    "                for weight_vec in token_weights:\n",
    "                    all_weights.append(weight_vec)\n",
    "                    # Calculate entropy\n",
    "                    w = np.array(weight_vec)\n",
    "                    w = w + 1e-10\n",
    "                    entropy = -np.sum(w * np.log(w))\n",
    "                    all_entropies.append(entropy)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    all_weights = np.array(all_weights)\n",
    "    all_entropies = np.array(all_entropies)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    fig.suptitle('Baseline Routing Analysis', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Weight distribution\n",
    "    ax1 = axes[0]\n",
    "    ax1.hist(all_weights.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.set_xlabel('Expert Weight')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Weight Distribution')\n",
    "    ax1.axvline(all_weights.mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {all_weights.mean():.4f}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Entropy distribution\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(all_entropies, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    ax2.set_xlabel('Routing Entropy')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Entropy Distribution')\n",
    "    ax2.axvline(all_entropies.mean(), color='red', linestyle='--',\n",
    "                linewidth=2, label=f'Mean: {all_entropies.mean():.3f}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Max weight per token\n",
    "    max_weights = all_weights.max(axis=1)\n",
    "    ax3 = axes[2]\n",
    "    ax3.hist(max_weights, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    ax3.set_xlabel('Max Weight (per token)')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Weight Concentration')\n",
    "    ax3.axvline(max_weights.mean(), color='red', linestyle='--',\n",
    "                linewidth=2, label=f'Mean: {max_weights.mean():.3f}')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('baseline_routing_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Baseline Routing Statistics:\")\n",
    "    print(f\"   Average weight: {all_weights.mean():.4f}\")\n",
    "    print(f\"   Average max weight: {max_weights.mean():.3f}\")\n",
    "    print(f\"   Average entropy: {all_entropies.mean():.3f}\")\n",
    "    print(f\"   Weight concentration: {(max_weights > 0.5).sum() / len(max_weights):.1%} tokens have max weight > 0.5\")\n",
    "    \n",
    "    if max_weights.mean() > 0.5:\n",
    "        print(f\"\\nüí° High weight concentration detected!\")\n",
    "        print(f\"   ‚Üí Model heavily favors certain experts per token\")\n",
    "        print(f\"   ‚Üí Uniform routing may help distribute load\")\n",
    "    \n",
    "    if all_entropies.mean() < 1.0:\n",
    "        print(f\"\\nüí° Low entropy detected!\")\n",
    "        print(f\"   ‚Üí Routing is highly deterministic\")\n",
    "        print(f\"   ‚Üí Few experts dominate\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No baseline internal logs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 & 3: Modified Routing Comparison\n",
    "\n",
    "Compare baseline vs modified routing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2 & 3: ROUTING COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For each configuration, show baseline vs modified\n",
    "for num_experts in sorted(results_df['num_experts'].unique()):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Configuration: {num_experts} Experts\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    config_df = results_df[results_df['num_experts'] == num_experts]\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison = []\n",
    "    baseline_row = config_df[config_df['strategy'] == 'baseline'].iloc[0]\n",
    "    \n",
    "    for _, row in config_df.iterrows():\n",
    "        delta_ppl = row['perplexity'] - baseline_row['perplexity']\n",
    "        delta_acc = row['token_accuracy'] - baseline_row['token_accuracy']\n",
    "        \n",
    "        comparison.append({\n",
    "            'Strategy': row['strategy'],\n",
    "            'Perplexity': f\"{row['perplexity']:.2f}\",\n",
    "            'Œî PPL': f\"{delta_ppl:+.2f}\",\n",
    "            'Accuracy': f\"{row['token_accuracy']:.4f}\",\n",
    "            'Œî Acc': f\"{delta_acc:+.4f}\",\n",
    "            'Speed': f\"{row['tokens_per_second']:.1f}\",\n",
    "            'Entropy': f\"{row['avg_entropy']:.3f}\"\n",
    "        })\n",
    "    \n",
    "    comp_df = pd.DataFrame(comparison)\n",
    "    print(\"\\n\" + comp_df.to_string(index=False))\n",
    "    \n",
    "    # Highlight findings\n",
    "    best_modified = config_df[config_df['strategy'] != 'baseline']['perplexity'].min()\n",
    "    if best_modified < baseline_row['perplexity']:\n",
    "        improvement = baseline_row['perplexity'] - best_modified\n",
    "        best_strategy = config_df[config_df['perplexity'] == best_modified]['strategy'].iloc[0]\n",
    "        print(f\"\\n‚úÖ Modified routing improved! Best: {best_strategy} (‚àí{improvement:.2f} PPL)\")\n",
    "    else:\n",
    "        print(f\"\\n‚ÑπÔ∏è  Baseline routing performs best for this configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Two-Phase Results\n",
    "\n",
    "View the generated comparison visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated visualization\n",
    "from IPython.display import Image, display\n",
    "\n",
    "viz_path = \"./two_phase_experiment/visualizations/two_phase_comparison.png\"\n",
    "print(\"üìä Two-Phase Experiment Visualizations:\\n\")\n",
    "display(Image(filename=viz_path))\n",
    "\n",
    "print(\"\\n‚úÖ Visualization includes:\")\n",
    "print(\"  1. Perplexity comparison (baseline vs modified)\")\n",
    "print(\"  2. Accuracy comparison\")\n",
    "print(\"  3. Delta perplexity (improvement/degradation)\")\n",
    "print(\"  4. Expert utilization (baseline)\")\n",
    "print(\"  5. Routing entropy comparison\")\n",
    "print(\"  6. Speed-quality trade-off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Report\n",
    "\n",
    "View the detailed markdown report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated report\n",
    "from IPython.display import Markdown\n",
    "\n",
    "report_path = \"./two_phase_experiment/two_phase_report.md\"\n",
    "\n",
    "with open(report_path, 'r') as f:\n",
    "    report_content = f.read()\n",
    "\n",
    "print(\"üìÑ Full Report:\\n\")\n",
    "display(Markdown(report_content))\n",
    "\n",
    "print(\"\\n\\nüìÅ Files generated:\")\n",
    "print(f\"  ‚Ä¢ {report_path}\")\n",
    "print(f\"  ‚Ä¢ ./two_phase_experiment/two_phase_results.csv\")\n",
    "print(f\"  ‚Ä¢ ./two_phase_experiment/two_phase_results.json\")\n",
    "print(f\"  ‚Ä¢ ./two_phase_experiment/visualizations/two_phase_comparison.png\")\n",
    "print(f\"  ‚Ä¢ ./two_phase_experiment/visualizations/two_phase_comparison.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Full Experiments\n",
    "\n",
    "Run comprehensive experiments with all configurations (~30-60 minutes)\n",
    "\n",
    "**‚ö†Ô∏è WARNING: This will take significant time and GPU resources!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run full experiments\n",
    "RUN_FULL_EXPERIMENTS = True  # Set to True to run\n",
    "\n",
    "if RUN_FULL_EXPERIMENTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FULL EXPERIMENTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  Expert counts: [4, 8, 16, 32, 64]\")\n",
    "    print(\"  Strategies: [regular, normalized, uniform]\")\n",
    "    print(\"  Datasets: [wikitext, lambada]\")\n",
    "    print(\"  Samples: 500 per dataset\")\n",
    "    print(\"  Total experiments: 30\")\n",
    "    print(\"  Estimated time: ~60 minutes\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    # Create runner\n",
    "    runner_full = RoutingExperimentRunner(\n",
    "        model_name=\"allenai/OLMoE-1B-7B-0924\",\n",
    "        device=device,\n",
    "        output_dir=\"./full_experiments\"\n",
    "    )\n",
    "    \n",
    "    # Run experiments\n",
    "    results_df_full = runner_full.run_all_experiments(\n",
    "        expert_counts=[4, 8, 16, 32, 64],\n",
    "        strategies=['regular', 'normalized', 'uniform'],\n",
    "        datasets=['wikitext', 'lambada'],\n",
    "        max_samples=500\n",
    "    )\n",
    "    \n",
    "    # Generate visualizations\n",
    "    runner_full.visualize_results(results_df_full)\n",
    "    \n",
    "    # Generate report\n",
    "    runner_full.generate_report(results_df_full)\n",
    "    \n",
    "    print(\"\\n‚úÖ Full experiments complete!\")\n",
    "    print(f\"   Results: {runner_full.output_dir}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Full experiments skipped (set RUN_FULL_EXPERIMENTS = True to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Analysis\n",
    "\n",
    "Analyze experiment results with various methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze_results import ResultAnalyzer\n",
    "\n",
    "# Use quick experiment results for analysis\n",
    "analyzer = ResultAnalyzer(\"./quick_experiment_standard\")\n",
    "\n",
    "# Print summary\n",
    "analyzer.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Internal Logs Deep Dive\n",
    "\n",
    "**Complete guide to accessing and using internal routing logs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPLETE INTERNAL LOGS TUTORIAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tutorial = \"\"\"\n",
    "üìö INTERNAL ROUTING LOGS - COMPLETE GUIDE\n",
    "==========================================\n",
    "\n",
    "1Ô∏è‚É£ WHAT'S SAVED:\n",
    "   ‚Ä¢ logs/{config}_{dataset}.json\n",
    "     - Summary metrics (perplexity, accuracy, speed)\n",
    "     - Aggregate statistics (entropy, concentration)\n",
    "   \n",
    "   ‚Ä¢ logs/{config}_{dataset}_internal_routing.json\n",
    "     - Detailed routing decisions for each sample\n",
    "     - router_logits for each layer\n",
    "     - selected_experts for each token\n",
    "     - expert_weights for each token\n",
    "\n",
    "2Ô∏è‚É£ FILE STRUCTURE:\n",
    "   {\n",
    "     \"config\": \"16experts_uniform\",\n",
    "     \"samples\": [\n",
    "       {\n",
    "         \"sample_id\": 0,\n",
    "         \"num_tokens\": 128,\n",
    "         \"layers\": [\n",
    "           {\n",
    "             \"layer\": 0,\n",
    "             \"router_logits_shape\": [1, 128, 64],\n",
    "             \"selected_experts\": [[experts for token 0], ...],\n",
    "             \"expert_weights\": [[weights for token 0], ...]\n",
    "           },\n",
    "           ... (16 layers total)\n",
    "         ]\n",
    "       },\n",
    "       ... (multiple samples)\n",
    "     ]\n",
    "   }\n",
    "\n",
    "3Ô∏è‚É£ HOW TO LOAD:\n",
    "   import json\n",
    "   with open('logs/16experts_uniform_wikitext_internal_routing.json', 'r') as f:\n",
    "       logs = json.load(f)\n",
    "   \n",
    "   # Access first sample, first layer\n",
    "   layer_0 = logs['samples'][0]['layers'][0]\n",
    "   selected = layer_0['selected_experts']\n",
    "   weights = layer_0['expert_weights']\n",
    "\n",
    "4Ô∏è‚É£ WHAT TO ANALYZE:\n",
    "   ‚úì Which experts are most frequently selected?\n",
    "   ‚úì How does routing differ across layers?\n",
    "   ‚úì What's the weight distribution pattern?\n",
    "   ‚úì Are any experts never used?\n",
    "   ‚úì Does entropy change across layers?\n",
    "   ‚úì How does custom routing differ from baseline?\n",
    "\n",
    "5Ô∏è‚É£ VISUALIZATION IDEAS:\n",
    "   ‚Ä¢ Expert activation heatmap (layer vs expert)\n",
    "   ‚Ä¢ Weight distribution histograms\n",
    "   ‚Ä¢ Entropy evolution across layers\n",
    "   ‚Ä¢ Per-token routing patterns\n",
    "   ‚Ä¢ Expert specialization analysis\n",
    "\n",
    "6Ô∏è‚É£ DEBUGGING USE CASES:\n",
    "   ‚Ä¢ Verify modifications work (compare baseline vs custom)\n",
    "   ‚Ä¢ Detect routing biases\n",
    "   ‚Ä¢ Identify underutilized experts\n",
    "   ‚Ä¢ Validate uniform weight distribution\n",
    "   ‚Ä¢ Check for numerical issues\n",
    "\"\"\"\n",
    "\n",
    "print(tutorial)\n",
    "print(\"\\n‚úÖ Internal logs provide complete visibility into routing decisions!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"‚úÖ INTERNAL LOGS CHECKLIST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checklist = [\n",
    "    (\"Internal routing logs enabled?\", \n",
    "     \"Set save_internal_logs=True in evaluate_configuration()\"),\n",
    "    (\"Logs directory exists?\",\n",
    "     \"Check ./experiment_*/logs/ for *_internal_routing.json files\"),\n",
    "    (\"Loaded and viewed logs?\",\n",
    "     \"Use json.load() to inspect routing decisions\"),\n",
    "    (\"Analyzed expert usage?\",\n",
    "     \"Aggregate selected_experts across samples\"),\n",
    "    (\"Visualized routing patterns?\",\n",
    "     \"Create heatmaps and histograms\"),\n",
    "    (\"Compared baseline vs custom?\",\n",
    "     \"Load both log files and compare weight distributions\")\n",
    "]\n",
    "\n",
    "for i, (question, tip) in enumerate(checklist, 1):\n",
    "    print(f\"\\n{i}. {question}\")\n",
    "    print(f\"   üí° {tip}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ YOU NOW HAVE COMPLETE INTERNAL ROUTING VISIBILITY!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare strategies\n",
    "comparison = analyzer.compare_strategies()\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal configuration\n",
    "optimal = analyzer.find_optimal_config(\n",
    "    quality_weight=0.7,  # 70% weight on quality\n",
    "    speed_weight=0.3     # 30% weight on speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific strategy\n",
    "analyzer.analyze_strategy('uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization\n",
    "\n",
    "Create custom visualizations from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "# Load results\n",
    "df = results_df_standard\n",
    "\n",
    "# Create custom visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('OLMoE Routing Experiments Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Perplexity by strategy\n",
    "ax1 = axes[0, 0]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax1.plot(\n",
    "        strategy_df['num_experts'], \n",
    "        strategy_df['perplexity'], \n",
    "        marker='o', \n",
    "        label=strategy,\n",
    "        linewidth=2\n",
    "    )\n",
    "ax1.set_xlabel('Number of Experts')\n",
    "ax1.set_ylabel('Perplexity (‚Üì better)')\n",
    "ax1.set_title('Perplexity vs Expert Count')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Token accuracy by strategy\n",
    "ax2 = axes[0, 1]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax2.plot(\n",
    "        strategy_df['num_experts'], \n",
    "        strategy_df['token_accuracy'], \n",
    "        marker='s', \n",
    "        label=strategy,\n",
    "        linewidth=2\n",
    "    )\n",
    "ax2.set_xlabel('Number of Experts')\n",
    "ax2.set_ylabel('Token Accuracy (‚Üë better)')\n",
    "ax2.set_title('Token Accuracy vs Expert Count')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Speed-quality trade-off\n",
    "ax3 = axes[1, 0]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax3.scatter(\n",
    "        strategy_df['perplexity'],\n",
    "        strategy_df['tokens_per_second'],\n",
    "        label=strategy,\n",
    "        s=100,\n",
    "        alpha=0.7\n",
    "    )\n",
    "ax3.set_xlabel('Perplexity (‚Üì better)')\n",
    "ax3.set_ylabel('Tokens/Second (‚Üë better)')\n",
    "ax3.set_title('Speed vs Quality Trade-off')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Routing entropy by strategy\n",
    "ax4 = axes[1, 1]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax4.plot(\n",
    "        strategy_df['num_experts'], \n",
    "        strategy_df['avg_entropy'], \n",
    "        marker='^', \n",
    "        label=strategy,\n",
    "        linewidth=2\n",
    "    )\n",
    "ax4.set_xlabel('Number of Experts')\n",
    "ax4.set_ylabel('Average Entropy')\n",
    "ax4.set_title('Routing Entropy vs Expert Count')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('custom_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Standard vs Custom Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Standard vs Custom Routing Comparison', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Get uniform routing data from both\n",
    "std_uniform = results_df_standard[results_df_standard['strategy'] == 'uniform']\n",
    "custom_uniform = results_df_custom\n",
    "\n",
    "# 1. Perplexity comparison\n",
    "ax1 = axes[0]\n",
    "x = range(len(std_uniform))\n",
    "width = 0.35\n",
    "ax1.bar([i - width/2 for i in x], std_uniform['perplexity'], width, label='Standard', alpha=0.8)\n",
    "ax1.bar([i + width/2 for i in x], custom_uniform['perplexity'], width, label='Custom (Patched)', alpha=0.8)\n",
    "ax1.set_xlabel('Configuration')\n",
    "ax1.set_ylabel('Perplexity')\n",
    "ax1.set_title('Perplexity: Standard vs Custom')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f\"{row['num_experts']} exp\" for _, row in std_uniform.iterrows()])\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Entropy comparison\n",
    "ax2 = axes[1]\n",
    "ax2.bar([i - width/2 for i in x], std_uniform['avg_entropy'], width, label='Standard', alpha=0.8)\n",
    "ax2.bar([i + width/2 for i in x], custom_uniform['avg_entropy'], width, label='Custom (Patched)', alpha=0.8)\n",
    "ax2.set_xlabel('Configuration')\n",
    "ax2.set_ylabel('Average Entropy')\n",
    "ax2.set_title('Routing Entropy: Standard vs Custom')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f\"{row['num_experts']} exp\" for _, row in std_uniform.iterrows()])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('standard_vs_custom.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Comparison visualization created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "‚úÖ **Environment Setup**: Configured GPU/CPU for optimal performance\n",
    "‚úÖ **Installation**: Installed all required packages\n",
    "‚úÖ **Custom Routing**: Implemented custom expert selection with internal logging\n",
    "‚úÖ **Model Patching**: Added support for patching MoE forward pass\n",
    "‚úÖ **Testing**: Validated all components work correctly\n",
    "‚úÖ **Quick Experiments**: Ran both standard and custom routing experiments\n",
    "‚úÖ **Analysis**: Analyzed results with multiple methods\n",
    "‚úÖ **Visualization**: Created comprehensive visualizations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "View the results above to understand:\n",
    "1. How different expert counts affect quality (perplexity)\n",
    "2. How routing strategies compare\n",
    "3. Speed vs quality trade-offs\n",
    "4. Routing entropy patterns\n",
    "5. Differences between standard and custom routing\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Run Full Experiments**: Set `RUN_FULL_EXPERIMENTS = True` for comprehensive analysis\n",
    "2. **Custom Strategies**: Implement your own routing strategies\n",
    "3. **More Datasets**: Test on additional datasets (PIQA, etc.)\n",
    "4. **Deep Analysis**: Use the ResultAnalyzer for detailed comparisons\n",
    "5. **Save Results**: Download results to Google Drive for further analysis\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `quick_experiment_standard/` - Standard routing results\n",
    "- `quick_experiment_custom/` - Custom routing results\n",
    "- `custom_analysis.png` - Custom visualizations\n",
    "- `standard_vs_custom.png` - Comparison plot\n",
    "\n",
    "### Documentation\n",
    "\n",
    "For more details, see:\n",
    "- `QUICKSTART.md` - Quick start guide\n",
    "- `ROUTING_EXPERIMENTS_README.md` - Complete documentation\n",
    "- `ARCHITECTURE.md` - System architecture\n",
    "- `IMPLEMENTATION_SUMMARY.md` - Technical details\n",
    "\n",
    "---\n",
    "\n",
    "**Happy experimenting! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
