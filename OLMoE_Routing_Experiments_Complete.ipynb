{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLMoE Routing Experiments - Complete End-to-End Notebook\n",
    "\n",
    "**Complete workflow from installation to results analysis**\n",
    "\n",
    "This notebook runs on:\n",
    "- ‚úÖ Google Colab (GPU recommended)\n",
    "- ‚úÖ Local Jupyter (GPU or CPU)\n",
    "- ‚úÖ Kaggle, Paperspace, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [GPU Configuration](#2-gpu-configuration)\n",
    "3. [Installation](#3-installation)\n",
    "4. [Custom Expert Selection & Model Patching](#4-custom-expert-selection--model-patching)\n",
    "5. [Framework Setup](#5-framework-setup)\n",
    "6. [Running Tests](#6-running-tests)\n",
    "7. [Quick Experiment](#7-quick-experiment)\n",
    "8. [Full Experiments](#8-full-experiments)\n",
    "9. [Results Analysis](#9-results-analysis)\n",
    "10. [Visualization](#10-visualization)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Detect environment and configure accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Google Colab: True\n",
      "Running in Kaggle: False\n",
      "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "\n",
      "Working directory: /Users/aliab/Desktop/GitHub/MOE-with-feature-selection/olmoe_experiments\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "print(f\"Running in Kaggle: {IN_KAGGLE}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "WORK_DIR = '/Users/aliab/Desktop/GitHub/MOE-with-feature-selection/olmoe_experiments'\n",
    "\n",
    "# Set working directory\n",
    "# if IN_COLAB:\n",
    "#     # Mount Google Drive (optional - for saving results)\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/drive')\n",
    "#     WORK_DIR = '/content/olmoe_experiments'\n",
    "# else:\n",
    "#     WORK_DIR = './olmoe_experiments'\n",
    "\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Configuration\n",
    "\n",
    "Check GPU availability and configure for optimal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GPU CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "Number of GPUs: 1\n",
      "\n",
      "GPU 0: NVIDIA A100-SXM4-40GB\n",
      "  Memory: 42.47 GB\n",
      "\n",
      "‚úÖ GPU is ready!\n",
      "\n",
      "Device for experiments: cuda\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check CUDA availability\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\nCUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Set device\n",
    "    device = 'cuda'\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n‚úÖ GPU is ready!\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"\\n‚ö†Ô∏è  GPU not available. Using CPU (will be slower).\")\n",
    "    if IN_COLAB:\n",
    "        print(\"\\nüí° TIP: Enable GPU in Colab:\")\n",
    "        print(\"   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nDevice for experiments: {device}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Installation\n",
    "\n",
    "Install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages installed!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install dependencies\n",
    "pip install -q torch transformers datasets pandas numpy matplotlib seaborn tqdm rich \n",
    "echo \"‚úÖ All packages installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Versions:\n",
      "  torch: 2.8.0+cu126\n",
      "  transformers: 4.57.1\n",
      "  datasets: 4.0.0\n",
      "  pandas: 2.2.2\n",
      "  numpy: 2.0.2\n",
      "\n",
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Verify installations\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  torch: {torch.__version__}\")\n",
    "print(f\"  transformers: {transformers.__version__}\")\n",
    "print(f\"  datasets: {datasets.__version__}\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(\"\\n‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Expert Selection & Model Patching\n",
    "\n",
    "**NEW: Support for custom forward pass with internal logging**\n",
    "\n",
    "This section implements:\n",
    "1. Custom expert selection (uniform weights)\n",
    "2. Model patching to return router_logits\n",
    "3. Internal logging of routing decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom expert selection functions defined!\n",
      "\n",
      "Functions available:\n",
      "  - custom_select_experts()\n",
      "  - create_custom_forward()\n",
      "  - patch_model_with_custom_routing()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "def custom_select_experts(\n",
    "    router_logits: torch.Tensor,\n",
    "    top_k: int,\n",
    "    num_experts: int\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Custom expert selection with uniform weights.\n",
    "    \n",
    "    This is equivalent to UniformRouting but integrated directly into the model.\n",
    "    \n",
    "    Args:\n",
    "        router_logits: [tokens, num_experts] - Raw routing scores\n",
    "        top_k: Number of experts to select\n",
    "        num_experts: Total number of experts\n",
    "    \n",
    "    Returns:\n",
    "        routing_weights: [tokens, top_k] - Uniform weights (1/top_k)\n",
    "        selected_experts: [tokens, top_k] - Selected expert indices\n",
    "    \"\"\"\n",
    "    # Convert logits to probabilities\n",
    "    routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "    \n",
    "    # Select top-k experts based on probabilities\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "    \n",
    "    # KEY: Give each selected expert EQUAL probability (uniform routing)\n",
    "    routing_weights = torch.ones_like(selected_experts, dtype=torch.float)\n",
    "    routing_weights /= top_k\n",
    "    \n",
    "    return routing_weights.to(router_logits.dtype), selected_experts\n",
    "\n",
    "\n",
    "def create_custom_forward(original_forward, top_k, num_experts):\n",
    "    \"\"\"\n",
    "    Create a custom forward pass that:\n",
    "    1. Uses custom_select_experts for routing\n",
    "    2. Returns router_logits for analysis\n",
    "    3. Enables internal logging of routing decisions\n",
    "    \"\"\"\n",
    "    def new_forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "        hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "        \n",
    "        # Get router logits: (batch * sequence_length, n_experts)\n",
    "        router_logits = self.gate(hidden_states)\n",
    "\n",
    "        # Use custom expert selection\n",
    "        routing_weights, selected_experts = custom_select_experts(\n",
    "            router_logits,\n",
    "            top_k=top_k,\n",
    "            num_experts=num_experts\n",
    "        )\n",
    "\n",
    "        # Initialize output\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, hidden_dim), \n",
    "            dtype=hidden_states.dtype, \n",
    "            device=hidden_states.device\n",
    "        )\n",
    "\n",
    "        # Create expert mask for efficient indexing\n",
    "        # expert_mask: [num_experts, top_k, tokens]\n",
    "        expert_mask = torch.nn.functional.one_hot(\n",
    "            selected_experts, \n",
    "            num_classes=num_experts\n",
    "        ).permute(2, 1, 0)\n",
    "\n",
    "        # Process each expert\n",
    "        for expert_idx in range(num_experts):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            \n",
    "            # Get tokens assigned to this expert\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            if top_x.numel() == 0:\n",
    "                continue  # No tokens for this expert\n",
    "\n",
    "            # Compute expert output with routing weights\n",
    "            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "\n",
    "            # Accumulate results\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "        \n",
    "        # Reshape output\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        \n",
    "        # Return both output and router_logits for analysis\n",
    "        return final_hidden_states, router_logits\n",
    "    \n",
    "    return new_forward\n",
    "\n",
    "\n",
    "def patch_model_with_custom_routing(model, top_k=None):\n",
    "    \"\"\"\n",
    "    Patch OLMoE model to use custom routing with internal logging.\n",
    "    \n",
    "    Args:\n",
    "        model: OLMoE model instance\n",
    "        top_k: Number of experts to use (None = use model default)\n",
    "    \"\"\"\n",
    "    if top_k is None:\n",
    "        top_k = getattr(model.config, 'num_experts_per_tok', 8)\n",
    "    \n",
    "    num_experts = getattr(model.config, 'num_local_experts', 64)\n",
    "    \n",
    "    print(f\"Patching model with custom routing:\")\n",
    "    print(f\"  top_k: {top_k}\")\n",
    "    print(f\"  num_experts: {num_experts}\")\n",
    "    \n",
    "    patched_layers = 0\n",
    "    \n",
    "    # Patch all MoE layers\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        if hasattr(layer, 'mlp') and hasattr(layer.mlp, 'experts'):\n",
    "            # Save original forward\n",
    "            original_forward = layer.mlp.forward\n",
    "            \n",
    "            # Create and apply custom forward\n",
    "            layer.mlp.forward = create_custom_forward(\n",
    "                original_forward, \n",
    "                top_k, \n",
    "                num_experts\n",
    "            ).__get__(layer.mlp, layer.mlp.__class__)\n",
    "            \n",
    "            patched_layers += 1\n",
    "    \n",
    "    print(f\"‚úÖ Patched {patched_layers} MoE layers\")\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"‚úÖ Custom expert selection functions defined!\")\n",
    "print(\"\\nFunctions available:\")\n",
    "print(\"  - custom_select_experts()\")\n",
    "print(\"  - create_custom_forward()\")\n",
    "print(\"  - patch_model_with_custom_routing()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Framework Setup\n",
    "\n",
    "Load the routing experiments framework with custom patching support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FRAMEWORK SETUP\n",
      "======================================================================\n",
      "‚úÖ Already in Python path: /Users/aliab/Desktop/GitHub/MOE-with-feature-selection\n",
      "\n",
      "Python path (first 3 entries):\n",
      "  1. /Users/aliab/Desktop/GitHub/MOE-with-feature-selection\n",
      "  2. /content\n",
      "  3. /env/python\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Framework Setup - Simple direct path addition\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FRAMEWORK SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Direct path to the framework directory\n",
    "framework_dir = '/Users/aliab/Desktop/GitHub/MOE-with-feature-selection'\n",
    "\n",
    "# Add to Python path\n",
    "if framework_dir not in sys.path:\n",
    "    sys.path.insert(0, framework_dir)\n",
    "    print(f\"‚úÖ Added to Python path: {framework_dir}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Already in Python path: {framework_dir}\")\n",
    "\n",
    "print(f\"\\nPython path (first 3 entries):\")\n",
    "for i, p in enumerate(sys.path[:3], 1):\n",
    "    print(f\"  {i}. {p}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "!cd '/Users/aliab/Desktop/GitHub/MOE-with-feature-selection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Importing framework modules...\n",
      "Looking in: /Users/aliab/Desktop/GitHub/MOE-with-feature-selection\n",
      "\n",
      "‚ùå Module not found: No module named 'olmoe_routing_experiments'\n",
      "\n",
      "üîß DEBUGGING:\n",
      "\n",
      "1. File exists? False\n",
      "   Path: /Users/aliab/Desktop/GitHub/MOE-with-feature-selection/olmoe_routing_experiments.py\n",
      "4. Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "\n",
      "üí° ALTERNATIVE FIX:\n",
      "   Run this in a new cell:\n",
      "   !cp /Users/aliab/Desktop/GitHub/MOE-with-feature-selection/olmoe_routing_experiments.py .\n",
      "   import olmoe_routing_experiments\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'olmoe_routing_experiments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2124080578.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Method 1: Direct import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0molmoe_routing_experiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Step 1: Module imported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'olmoe_routing_experiments'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import with cache clearing\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "print(\"üì¶ Importing framework modules...\")\n",
    "print(f\"Looking in: {sys.path[0]}\\n\")\n",
    "\n",
    "# Clear any cached imports\n",
    "if 'olmoe_routing_experiments' in sys.modules:\n",
    "    print(\"üîÑ Clearing cached module...\")\n",
    "    del sys.modules['olmoe_routing_experiments']\n",
    "\n",
    "# Try importing with detailed error handling\n",
    "try:\n",
    "    # Method 1: Direct import\n",
    "    import olmoe_routing_experiments\n",
    "    print(\"‚úÖ Step 1: Module imported\")\n",
    "    \n",
    "    # Import specific classes\n",
    "    from olmoe_routing_experiments import (\n",
    "        RoutingConfig,\n",
    "        ExperimentResults,\n",
    "        RoutingStrategy,\n",
    "        RegularRouting,\n",
    "        NormalizedRouting,\n",
    "        UniformRouting,\n",
    "        AdaptiveRouting,\n",
    "        RoutingExperimentRunner,\n",
    "        ModelPatchingUtils\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Step 2: All classes imported successfully!\")\n",
    "    print(\"\\nüìö Available Components:\")\n",
    "    print(\"  ‚Ä¢ RoutingExperimentRunner\")\n",
    "    print(\"  ‚Ä¢ ModelPatchingUtils\")\n",
    "    print(\"  ‚Ä¢ RegularRouting, NormalizedRouting\")\n",
    "    print(\"  ‚Ä¢ UniformRouting, AdaptiveRouting\")\n",
    "    \n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"‚ùå Module not found: {e}\")\n",
    "    print(\"\\nüîß DEBUGGING:\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    import os\n",
    "    file_path = os.path.join(sys.path[0], 'olmoe_routing_experiments.py')\n",
    "    print(f\"\\n1. File exists? {os.path.exists(file_path)}\")\n",
    "    print(f\"   Path: {file_path}\")\n",
    "    \n",
    "    # Check file permissions\n",
    "    if os.path.exists(file_path):\n",
    "        import stat\n",
    "        st = os.stat(file_path)\n",
    "        print(f\"2. File readable? {bool(st.st_mode & stat.S_IRUSR)}\")\n",
    "        print(f\"3. File size: {st.st_size} bytes\")\n",
    "    \n",
    "    # Check Python version\n",
    "    print(f\"4. Python version: {sys.version}\")\n",
    "    \n",
    "    print(\"\\nüí° ALTERNATIVE FIX:\")\n",
    "    print(\"   Run this in a new cell:\")\n",
    "    print(\"   !cp /Users/aliab/Desktop/GitHub/MOE-with-feature-selection/olmoe_routing_experiments.py .\")\n",
    "    print(\"   import olmoe_routing_experiments\")\n",
    "    \n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "cp: cannot stat '/Users/aliab/Desktop/GitHub/MOE-with-feature-selection/olmoe_routing_experiments.py': No such file or directory\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'olmoe_routing_experiments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-934647058.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls -lt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp /Users/aliab/Desktop/GitHub/MOE-with-feature-selection/olmoe_routing_experiments.py .'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0molmoe_routing_experiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'olmoe_routing_experiments'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!ls -lt\n",
    "!cp /Users/aliab/Desktop/GitHub/MOE-with-feature-selection/olmoe_routing_experiments.py .\n",
    "import olmoe_routing_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Framework with Custom Patching Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedRoutingExperimentRunner(RoutingExperimentRunner):\n",
    "    \"\"\"\n",
    "    Extended runner with support for custom model patching.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, use_custom_routing=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.use_custom_routing = use_custom_routing\n",
    "        \n",
    "        if use_custom_routing:\n",
    "            print(\"\\nüîß CUSTOM ROUTING MODE ENABLED\")\n",
    "            print(\"   Model will be patched with custom_select_experts\")\n",
    "    \n",
    "    def _set_expert_count(self, num_experts):\n",
    "        \"\"\"Override to support custom routing.\"\"\"\n",
    "        super()._set_expert_count(num_experts)\n",
    "        \n",
    "        if self.use_custom_routing:\n",
    "            # Re-patch model with new expert count\n",
    "            patch_model_with_custom_routing(self.model, top_k=num_experts)\n",
    "\n",
    "print(\"‚úÖ Extended framework with custom patching support ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running Tests\n",
    "\n",
    "Validate the framework is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RUNNING VALIDATION TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: Routing Strategies\n",
    "print(\"\\n[1/5] Testing routing strategies...\")\n",
    "torch.manual_seed(42)\n",
    "logits = torch.randn(1, 1, 64)\n",
    "\n",
    "regular = RegularRouting(num_experts=8)\n",
    "normalized = NormalizedRouting(num_experts=8)\n",
    "uniform = UniformRouting(num_experts=8)\n",
    "\n",
    "reg_indices, reg_weights = regular.route(logits)\n",
    "norm_indices, norm_weights = normalized.route(logits)\n",
    "uni_indices, uni_weights = uniform.route(logits)\n",
    "\n",
    "# Verify uniform has equal weights\n",
    "assert torch.allclose(uni_weights, torch.ones_like(uni_weights) / 8, atol=1e-6)\n",
    "print(\"   ‚úÖ Routing strategies work correctly\")\n",
    "\n",
    "# Test 2: Custom Expert Selection\n",
    "print(\"\\n[2/5] Testing custom expert selection...\")\n",
    "router_logits = torch.randn(10, 64)  # 10 tokens, 64 experts\n",
    "weights, indices = custom_select_experts(router_logits, top_k=8, num_experts=64)\n",
    "\n",
    "assert weights.shape == (10, 8)\n",
    "assert indices.shape == (10, 8)\n",
    "assert torch.allclose(weights, torch.ones_like(weights) / 8, atol=1e-6)\n",
    "print(\"   ‚úÖ Custom expert selection works correctly\")\n",
    "\n",
    "# Test 3: Different expert counts produce different routing\n",
    "print(\"\\n[3/5] Testing expert count variation...\")\n",
    "weights_4, indices_4 = custom_select_experts(router_logits, top_k=4, num_experts=64)\n",
    "weights_16, indices_16 = custom_select_experts(router_logits, top_k=16, num_experts=64)\n",
    "\n",
    "assert weights_4.shape[-1] == 4\n",
    "assert weights_16.shape[-1] == 16\n",
    "print(\"   ‚úÖ Different expert counts work correctly\")\n",
    "\n",
    "# Test 4: Verify uniform weights\n",
    "print(\"\\n[4/5] Testing uniform weight distribution...\")\n",
    "for k in [4, 8, 16]:\n",
    "    w, _ = custom_select_experts(router_logits, top_k=k, num_experts=64)\n",
    "    expected = torch.ones_like(w) / k\n",
    "    assert torch.allclose(w, expected, atol=1e-6)\n",
    "print(\"   ‚úÖ Uniform weights verified for all k values\")\n",
    "\n",
    "# Test 5: Statistics tracking\n",
    "print(\"\\n[5/5] Testing statistics tracking...\")\n",
    "strategy = UniformRouting(num_experts=8)\n",
    "for _ in range(5):\n",
    "    test_logits = torch.randn(2, 5, 64)\n",
    "    strategy.route(test_logits)\n",
    "\n",
    "stats = strategy.get_summary_stats()\n",
    "assert 'avg_entropy' in stats\n",
    "assert 'avg_concentration' in stats\n",
    "assert 'unique_experts' in stats\n",
    "print(\"   ‚úÖ Statistics tracking works correctly\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL TESTS PASSED!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Experiment\n",
    "\n",
    "Run a minimal experiment to verify everything works (~5 minutes)\n",
    "\n",
    "**This experiment will:**\n",
    "- Test 2 expert counts (8, 16)\n",
    "- Test 2 strategies (regular, custom uniform)\n",
    "- Evaluate on 50 samples\n",
    "- Generate visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"QUICK EXPERIMENT (Standard Routing)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"  Expert counts: [8, 16]\")\n",
    "print(\"  Strategies: [regular, uniform]\")\n",
    "print(\"  Dataset: WikiText-2\")\n",
    "print(\"  Samples: 50\")\n",
    "print(\"  Estimated time: ~5 minutes\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Create runner (standard mode)\n",
    "runner_standard = RoutingExperimentRunner(\n",
    "    model_name=\"allenai/OLMoE-1B-7B-0924\",\n",
    "    device=device,\n",
    "    output_dir=\"./quick_experiment_standard\"\n",
    ")\n",
    "\n",
    "# Run experiments\n",
    "results_df_standard = runner_standard.run_all_experiments(\n",
    "    expert_counts=[8, 16],\n",
    "    strategies=['regular', 'uniform'],\n",
    "    datasets=['wikitext'],\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Standard routing experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nüìä QUICK EXPERIMENT RESULTS (Standard Routing)\\n\")\n",
    "print(results_df_standard[[\n",
    "    'config', 'perplexity', 'token_accuracy', \n",
    "    'tokens_per_second', 'avg_entropy'\n",
    "]].to_string(index=False))\n",
    "\n",
    "# Best configuration\n",
    "best_idx = results_df_standard['perplexity'].idxmin()\n",
    "best = results_df_standard.loc[best_idx]\n",
    "\n",
    "print(\"\\nüèÜ BEST CONFIGURATION:\")\n",
    "print(f\"   Config: {best['config']}\")\n",
    "print(f\"   Perplexity: {best['perplexity']:.2f}\")\n",
    "print(f\"   Accuracy: {best['token_accuracy']:.4f}\")\n",
    "print(f\"   Speed: {best['tokens_per_second']:.1f} tok/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Experiment with Custom Routing (Internal Logging)\n",
    "\n",
    "**This uses the custom forward pass with internal router_logits logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"QUICK EXPERIMENT (Custom Routing with Internal Logging)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"  Expert counts: [8, 16]\")\n",
    "print(\"  Routing: Custom uniform (patched forward pass)\")\n",
    "print(\"  Dataset: WikiText-2\")\n",
    "print(\"  Samples: 50\")\n",
    "print(\"  Features: Internal router_logits logging\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Create runner with custom routing enabled\n",
    "runner_custom = ExtendedRoutingExperimentRunner(\n",
    "    model_name=\"allenai/OLMoE-1B-7B-0924\",\n",
    "    device=device,\n",
    "    output_dir=\"./quick_experiment_custom\",\n",
    "    use_custom_routing=True\n",
    ")\n",
    "\n",
    "# Patch the model\n",
    "patch_model_with_custom_routing(runner_custom.model, top_k=8)\n",
    "\n",
    "# Run experiments (only uniform strategy makes sense with custom routing)\n",
    "results_df_custom = runner_custom.run_all_experiments(\n",
    "    expert_counts=[8, 16],\n",
    "    strategies=['uniform'],  # Custom routing is uniform\n",
    "    datasets=['wikitext'],\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Custom routing experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nüìä QUICK EXPERIMENT RESULTS (Custom Routing)\\n\")\n",
    "print(results_df_custom[[\n",
    "    'config', 'perplexity', 'token_accuracy', \n",
    "    'tokens_per_second', 'avg_entropy'\n",
    "]].to_string(index=False))\n",
    "\n",
    "# Compare with standard\n",
    "print(\"\\nüìà COMPARISON: Custom vs Standard Uniform Routing\\n\")\n",
    "\n",
    "comparison_data = []\n",
    "for expert_count in [8, 16]:\n",
    "    std_row = results_df_standard[\n",
    "        (results_df_standard['num_experts'] == expert_count) & \n",
    "        (results_df_standard['strategy'] == 'uniform')\n",
    "    ].iloc[0]\n",
    "    \n",
    "    custom_row = results_df_custom[\n",
    "        results_df_custom['num_experts'] == expert_count\n",
    "    ].iloc[0]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Expert Count': expert_count,\n",
    "        'Standard PPL': f\"{std_row['perplexity']:.2f}\",\n",
    "        'Custom PPL': f\"{custom_row['perplexity']:.2f}\",\n",
    "        'Difference': f\"{custom_row['perplexity'] - std_row['perplexity']:.2f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nüí° Note: Small differences are expected due to implementation details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Experiments\n",
    "\n",
    "Run comprehensive experiments with all configurations (~30-60 minutes)\n",
    "\n",
    "**‚ö†Ô∏è WARNING: This will take significant time and GPU resources!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run full experiments\n",
    "RUN_FULL_EXPERIMENTS = False  # Set to True to run\n",
    "\n",
    "if RUN_FULL_EXPERIMENTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FULL EXPERIMENTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  Expert counts: [4, 8, 16, 32, 64]\")\n",
    "    print(\"  Strategies: [regular, normalized, uniform]\")\n",
    "    print(\"  Datasets: [wikitext, lambada]\")\n",
    "    print(\"  Samples: 500 per dataset\")\n",
    "    print(\"  Total experiments: 30\")\n",
    "    print(\"  Estimated time: ~60 minutes\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    # Create runner\n",
    "    runner_full = RoutingExperimentRunner(\n",
    "        model_name=\"allenai/OLMoE-1B-7B-0924\",\n",
    "        device=device,\n",
    "        output_dir=\"./full_experiments\"\n",
    "    )\n",
    "    \n",
    "    # Run experiments\n",
    "    results_df_full = runner_full.run_all_experiments(\n",
    "        expert_counts=[4, 8, 16, 32, 64],\n",
    "        strategies=['regular', 'normalized', 'uniform'],\n",
    "        datasets=['wikitext', 'lambada'],\n",
    "        max_samples=500\n",
    "    )\n",
    "    \n",
    "    # Generate visualizations\n",
    "    runner_full.visualize_results(results_df_full)\n",
    "    \n",
    "    # Generate report\n",
    "    runner_full.generate_report(results_df_full)\n",
    "    \n",
    "    print(\"\\n‚úÖ Full experiments complete!\")\n",
    "    print(f\"   Results: {runner_full.output_dir}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Full experiments skipped (set RUN_FULL_EXPERIMENTS = True to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Analysis\n",
    "\n",
    "Analyze experiment results with various methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze_results import ResultAnalyzer\n",
    "\n",
    "# Use quick experiment results for analysis\n",
    "analyzer = ResultAnalyzer(\"./quick_experiment_standard\")\n",
    "\n",
    "# Print summary\n",
    "analyzer.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare strategies\n",
    "comparison = analyzer.compare_strategies()\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal configuration\n",
    "optimal = analyzer.find_optimal_config(\n",
    "    quality_weight=0.7,  # 70% weight on quality\n",
    "    speed_weight=0.3     # 30% weight on speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific strategy\n",
    "analyzer.analyze_strategy('uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization\n",
    "\n",
    "Create custom visualizations from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "# Load results\n",
    "df = results_df_standard\n",
    "\n",
    "# Create custom visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('OLMoE Routing Experiments Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Perplexity by strategy\n",
    "ax1 = axes[0, 0]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax1.plot(\n",
    "        strategy_df['num_experts'], \n",
    "        strategy_df['perplexity'], \n",
    "        marker='o', \n",
    "        label=strategy,\n",
    "        linewidth=2\n",
    "    )\n",
    "ax1.set_xlabel('Number of Experts')\n",
    "ax1.set_ylabel('Perplexity (‚Üì better)')\n",
    "ax1.set_title('Perplexity vs Expert Count')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Token accuracy by strategy\n",
    "ax2 = axes[0, 1]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax2.plot(\n",
    "        strategy_df['num_experts'], \n",
    "        strategy_df['token_accuracy'], \n",
    "        marker='s', \n",
    "        label=strategy,\n",
    "        linewidth=2\n",
    "    )\n",
    "ax2.set_xlabel('Number of Experts')\n",
    "ax2.set_ylabel('Token Accuracy (‚Üë better)')\n",
    "ax2.set_title('Token Accuracy vs Expert Count')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Speed-quality trade-off\n",
    "ax3 = axes[1, 0]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax3.scatter(\n",
    "        strategy_df['perplexity'],\n",
    "        strategy_df['tokens_per_second'],\n",
    "        label=strategy,\n",
    "        s=100,\n",
    "        alpha=0.7\n",
    "    )\n",
    "ax3.set_xlabel('Perplexity (‚Üì better)')\n",
    "ax3.set_ylabel('Tokens/Second (‚Üë better)')\n",
    "ax3.set_title('Speed vs Quality Trade-off')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Routing entropy by strategy\n",
    "ax4 = axes[1, 1]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax4.plot(\n",
    "        strategy_df['num_experts'], \n",
    "        strategy_df['avg_entropy'], \n",
    "        marker='^', \n",
    "        label=strategy,\n",
    "        linewidth=2\n",
    "    )\n",
    "ax4.set_xlabel('Number of Experts')\n",
    "ax4.set_ylabel('Average Entropy')\n",
    "ax4.set_title('Routing Entropy vs Expert Count')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('custom_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Standard vs Custom Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Standard vs Custom Routing Comparison', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Get uniform routing data from both\n",
    "std_uniform = results_df_standard[results_df_standard['strategy'] == 'uniform']\n",
    "custom_uniform = results_df_custom\n",
    "\n",
    "# 1. Perplexity comparison\n",
    "ax1 = axes[0]\n",
    "x = range(len(std_uniform))\n",
    "width = 0.35\n",
    "ax1.bar([i - width/2 for i in x], std_uniform['perplexity'], width, label='Standard', alpha=0.8)\n",
    "ax1.bar([i + width/2 for i in x], custom_uniform['perplexity'], width, label='Custom (Patched)', alpha=0.8)\n",
    "ax1.set_xlabel('Configuration')\n",
    "ax1.set_ylabel('Perplexity')\n",
    "ax1.set_title('Perplexity: Standard vs Custom')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f\"{row['num_experts']} exp\" for _, row in std_uniform.iterrows()])\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Entropy comparison\n",
    "ax2 = axes[1]\n",
    "ax2.bar([i - width/2 for i in x], std_uniform['avg_entropy'], width, label='Standard', alpha=0.8)\n",
    "ax2.bar([i + width/2 for i in x], custom_uniform['avg_entropy'], width, label='Custom (Patched)', alpha=0.8)\n",
    "ax2.set_xlabel('Configuration')\n",
    "ax2.set_ylabel('Average Entropy')\n",
    "ax2.set_title('Routing Entropy: Standard vs Custom')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f\"{row['num_experts']} exp\" for _, row in std_uniform.iterrows()])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('standard_vs_custom.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Comparison visualization created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "‚úÖ **Environment Setup**: Configured GPU/CPU for optimal performance\n",
    "‚úÖ **Installation**: Installed all required packages\n",
    "‚úÖ **Custom Routing**: Implemented custom expert selection with internal logging\n",
    "‚úÖ **Model Patching**: Added support for patching MoE forward pass\n",
    "‚úÖ **Testing**: Validated all components work correctly\n",
    "‚úÖ **Quick Experiments**: Ran both standard and custom routing experiments\n",
    "‚úÖ **Analysis**: Analyzed results with multiple methods\n",
    "‚úÖ **Visualization**: Created comprehensive visualizations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "View the results above to understand:\n",
    "1. How different expert counts affect quality (perplexity)\n",
    "2. How routing strategies compare\n",
    "3. Speed vs quality trade-offs\n",
    "4. Routing entropy patterns\n",
    "5. Differences between standard and custom routing\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Run Full Experiments**: Set `RUN_FULL_EXPERIMENTS = True` for comprehensive analysis\n",
    "2. **Custom Strategies**: Implement your own routing strategies\n",
    "3. **More Datasets**: Test on additional datasets (PIQA, etc.)\n",
    "4. **Deep Analysis**: Use the ResultAnalyzer for detailed comparisons\n",
    "5. **Save Results**: Download results to Google Drive for further analysis\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `quick_experiment_standard/` - Standard routing results\n",
    "- `quick_experiment_custom/` - Custom routing results\n",
    "- `custom_analysis.png` - Custom visualizations\n",
    "- `standard_vs_custom.png` - Comparison plot\n",
    "\n",
    "### Documentation\n",
    "\n",
    "For more details, see:\n",
    "- `QUICKSTART.md` - Quick start guide\n",
    "- `ROUTING_EXPERIMENTS_README.md` - Complete documentation\n",
    "- `ARCHITECTURE.md` - System architecture\n",
    "- `IMPLEMENTATION_SUMMARY.md` - Technical details\n",
    "\n",
    "---\n",
    "\n",
    "**Happy experimenting! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
