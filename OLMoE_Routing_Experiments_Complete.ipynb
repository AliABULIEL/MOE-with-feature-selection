{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# OLMoE Routing Experiments - Complete End-to-End Notebook\n\n**Complete workflow from installation to results analysis**\n\nThis notebook runs on:\n- ‚úÖ **Google Colab** (Recommended - GPU enabled)\n- ‚úÖ Local Jupyter (GPU or CPU)\n- ‚úÖ Kaggle, Paperspace, etc.\n\n---\n\n## üöÄ Quick Start (Google Colab)\n\n### Recommended: Run in Google Drive\n1. Upload this notebook to Google Drive\n2. Open with Google Colab\n3. Enable GPU: `Runtime ‚Üí Change runtime type ‚Üí GPU`\n4. Run all cells - the notebook will:\n   - Mount your Google Drive\n   - Clone/update the GitHub repository to Drive\n   - Install dependencies\n   - Run experiments\n   - Save results to Drive\n\n**Benefits:**\n- Repository persists in Drive (only clone once!)\n- Results saved to Drive automatically\n- Faster subsequent runs (repo already available)\n\n---\n\n## üìã What This Notebook Does\n\n1. **Environment Detection**: Auto-detects Colab/Kaggle/local\n2. **Repository Setup**: Clones repo to Google Drive (or uses existing)\n3. **GPU Configuration**: Configures GPU/CPU automatically\n4. **Installation**: Installs all required packages\n5. **Custom Routing**: Implements custom expert selection with internal logging\n6. **Experiments**: Tests different routing strategies and expert counts\n7. **Two-Phase Analysis**: Analyzes baseline, then tests modified routing\n8. **Comprehensive Reports**: Generates visualizations and detailed reports\n\n---\n\n## üìÅ File Locations (Google Drive)\n\nWhen running in Colab, files are stored in:\n- **Repository**: `/content/drive/MyDrive/MOE-with-feature-selection/`\n- **Results**: `/content/drive/MyDrive/olmoe_experiments/`\n\nYour results persist across Colab sessions!\n\n---\n\n## Table of Contents\n\n1. [Environment Setup](#1-environment-setup)\n2. [GPU Configuration](#2-gpu-configuration)\n3. [Installation](#3-installation)\n4. [Custom Expert Selection & Model Patching](#4-custom-expert-selection--model-patching)\n5. [Framework Setup](#5-framework-setup)\n6. [Running Tests](#6-running-tests)\n7. [Quick Experiment](#7-quick-experiment)\n8. [**Two-Phase Routing Experiment** (NEW!)](#8-two-phase-routing-experiment-new)\n9. [Full Experiments](#9-full-experiments)\n10. [Results Analysis](#10-results-analysis)\n11. [Visualization](#11-visualization)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Detect environment and configure accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\n\n# Detect environment\nIN_COLAB = 'google.colab' in sys.modules\nIN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n\nprint(f\"Running in Google Colab: {IN_COLAB}\")\nprint(f\"Running in Kaggle: {IN_KAGGLE}\")\nprint(f\"Python version: {sys.version}\")\n\n# Set working directory based on environment\nif IN_COLAB:\n    # Mount Google Drive first\n    from google.colab import drive\n    print(\"\\nüìÅ Mounting Google Drive...\")\n    drive.mount('/content/drive')\n    \n    # Set working directory in Google Drive\n    WORK_DIR = '/content/drive/MyDrive/olmoe_experiments'\n    # Repository will be cloned to Google Drive (persists across sessions)\n    REPO_DIR = '/content/drive/MyDrive/MOE-with-feature-selection'\nelse:\n    # Local environment\n    WORK_DIR = './olmoe_experiments'\n    REPO_DIR = None\n\nos.makedirs(WORK_DIR, exist_ok=True)\nos.chdir(WORK_DIR)\nprint(f\"\\n‚úÖ Working directory: {os.getcwd()}\")\n\n# Store repo directory for later use\nif IN_COLAB:\n    print(f\"‚úÖ Repository location: {REPO_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Configuration\n",
    "\n",
    "Check GPU availability and configure for optimal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GPU CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "Number of GPUs: 1\n",
      "\n",
      "GPU 0: NVIDIA A100-SXM4-40GB\n",
      "  Memory: 42.47 GB\n",
      "\n",
      "‚úÖ GPU is ready!\n",
      "\n",
      "Device for experiments: cuda\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check CUDA availability\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\nCUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Set device\n",
    "    device = 'cuda'\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n‚úÖ GPU is ready!\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"\\n‚ö†Ô∏è  GPU not available. Using CPU (will be slower).\")\n",
    "    if IN_COLAB:\n",
    "        print(\"\\nüí° TIP: Enable GPU in Colab:\")\n",
    "        print(\"   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nDevice for experiments: {device}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Installation\n",
    "\n",
    "Install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages installed!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install dependencies\n",
    "pip install -q torch transformers datasets pandas numpy matplotlib seaborn tqdm rich \n",
    "echo \"‚úÖ All packages installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Versions:\n",
      "  torch: 2.8.0+cu126\n",
      "  transformers: 4.57.1\n",
      "  datasets: 4.0.0\n",
      "  pandas: 2.2.2\n",
      "  numpy: 2.0.2\n",
      "\n",
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Verify installations\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  torch: {torch.__version__}\")\n",
    "print(f\"  transformers: {transformers.__version__}\")\n",
    "print(f\"  datasets: {datasets.__version__}\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(\"\\n‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Expert Selection & Model Patching\n",
    "\n",
    "**NEW: Support for custom forward pass with internal logging**\n",
    "\n",
    "This section implements:\n",
    "1. Custom expert selection (uniform weights)\n",
    "2. Model patching to return router_logits\n",
    "3. Internal logging of routing decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom expert selection functions defined!\n",
      "\n",
      "Functions available:\n",
      "  - custom_select_experts()\n",
      "  - create_custom_forward()\n",
      "  - patch_model_with_custom_routing()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "def custom_select_experts(\n",
    "    router_logits: torch.Tensor,\n",
    "    top_k: int,\n",
    "    num_experts: int\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Custom expert selection with uniform weights.\n",
    "    \n",
    "    This is equivalent to UniformRouting but integrated directly into the model.\n",
    "    \n",
    "    Args:\n",
    "        router_logits: [tokens, num_experts] - Raw routing scores\n",
    "        top_k: Number of experts to select\n",
    "        num_experts: Total number of experts\n",
    "    \n",
    "    Returns:\n",
    "        routing_weights: [tokens, top_k] - Uniform weights (1/top_k)\n",
    "        selected_experts: [tokens, top_k] - Selected expert indices\n",
    "    \"\"\"\n",
    "    # Convert logits to probabilities\n",
    "    routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "    \n",
    "    # Select top-k experts based on probabilities\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "    \n",
    "    # KEY: Give each selected expert EQUAL probability (uniform routing)\n",
    "    routing_weights = torch.ones_like(selected_experts, dtype=torch.float)\n",
    "    routing_weights /= top_k\n",
    "    \n",
    "    return routing_weights.to(router_logits.dtype), selected_experts\n",
    "\n",
    "\n",
    "def create_custom_forward(original_forward, top_k, num_experts):\n",
    "    \"\"\"\n",
    "    Create a custom forward pass that:\n",
    "    1. Uses custom_select_experts for routing\n",
    "    2. Returns router_logits for analysis\n",
    "    3. Enables internal logging of routing decisions\n",
    "    \"\"\"\n",
    "    def new_forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "        hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "        \n",
    "        # Get router logits: (batch * sequence_length, n_experts)\n",
    "        router_logits = self.gate(hidden_states)\n",
    "\n",
    "        # Use custom expert selection\n",
    "        routing_weights, selected_experts = custom_select_experts(\n",
    "            router_logits,\n",
    "            top_k=top_k,\n",
    "            num_experts=num_experts\n",
    "        )\n",
    "\n",
    "        # Initialize output\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, hidden_dim), \n",
    "            dtype=hidden_states.dtype, \n",
    "            device=hidden_states.device\n",
    "        )\n",
    "\n",
    "        # Create expert mask for efficient indexing\n",
    "        # expert_mask: [num_experts, top_k, tokens]\n",
    "        expert_mask = torch.nn.functional.one_hot(\n",
    "            selected_experts, \n",
    "            num_classes=num_experts\n",
    "        ).permute(2, 1, 0)\n",
    "\n",
    "        # Process each expert\n",
    "        for expert_idx in range(num_experts):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            \n",
    "            # Get tokens assigned to this expert\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            if top_x.numel() == 0:\n",
    "                continue  # No tokens for this expert\n",
    "\n",
    "            # Compute expert output with routing weights\n",
    "            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "\n",
    "            # Accumulate results\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "        \n",
    "        # Reshape output\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        \n",
    "        # Return both output and router_logits for analysis\n",
    "        return final_hidden_states, router_logits\n",
    "    \n",
    "    return new_forward\n",
    "\n",
    "\n",
    "def patch_model_with_custom_routing(model, top_k=None):\n",
    "    \"\"\"\n",
    "    Patch OLMoE model to use custom routing with internal logging.\n",
    "    \n",
    "    Args:\n",
    "        model: OLMoE model instance\n",
    "        top_k: Number of experts to use (None = use model default)\n",
    "    \"\"\"\n",
    "    if top_k is None:\n",
    "        top_k = getattr(model.config, 'num_experts_per_tok', 8)\n",
    "    \n",
    "    num_experts = getattr(model.config, 'num_local_experts', 64)\n",
    "    \n",
    "    print(f\"Patching model with custom routing:\")\n",
    "    print(f\"  top_k: {top_k}\")\n",
    "    print(f\"  num_experts: {num_experts}\")\n",
    "    \n",
    "    patched_layers = 0\n",
    "    \n",
    "    # Patch all MoE layers\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        if hasattr(layer, 'mlp') and hasattr(layer.mlp, 'experts'):\n",
    "            # Save original forward\n",
    "            original_forward = layer.mlp.forward\n",
    "            \n",
    "            # Create and apply custom forward\n",
    "            layer.mlp.forward = create_custom_forward(\n",
    "                original_forward, \n",
    "                top_k, \n",
    "                num_experts\n",
    "            ).__get__(layer.mlp, layer.mlp.__class__)\n",
    "            \n",
    "            patched_layers += 1\n",
    "    \n",
    "    print(f\"‚úÖ Patched {patched_layers} MoE layers\")\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"‚úÖ Custom expert selection functions defined!\")\n",
    "print(\"\\nFunctions available:\")\n",
    "print(\"  - custom_select_experts()\")\n",
    "print(\"  - create_custom_forward()\")\n",
    "print(\"  - patch_model_with_custom_routing()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Framework Setup\n\n**Clone/Update repository and load the routing experiments framework**\n\nThis will:\n1. Check if repository exists in Google Drive\n2. Clone if needed, or pull latest changes if it exists\n3. Add repository to Python path\n4. Import all framework modules\n\n**Note:** Repository is stored in Google Drive and persists across sessions!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\n\nprint(\"=\" * 70)\nprint(\"FRAMEWORK SETUP\")\nprint(\"=\" * 70)\n\n# Step 1: Clone repository if in Colab\nif IN_COLAB:\n    # Check if repo already exists in Google Drive\n    if os.path.exists(REPO_DIR):\n        print(f\"\\nüìÇ Step 1: Repository already exists in Google Drive\")\n        print(f\"   Location: {REPO_DIR}\")\n        print(f\"   ‚úÖ Using existing repository\")\n        \n        # Optionally pull latest changes\n        print(f\"\\n   Pulling latest changes...\")\n        !cd {REPO_DIR} && git pull\n    else:\n        print(\"\\nüì• Step 1: Cloning GitHub repository...\")\n        print(f\"   Cloning to: {REPO_DIR}\")\n        !git clone https://github.com/aliabbasjaffri/MOE-with-feature-selection.git {REPO_DIR}\n        \n        # Verify clone was successful\n        if os.path.exists(REPO_DIR):\n            print(f\"   ‚úÖ Repository cloned successfully!\")\n        else:\n            print(f\"   ‚ùå Failed to clone repository!\")\n            raise Exception(\"Repository clone failed\")\n    \n    # List main files\n    print(f\"\\n   üìÇ Repository contents:\")\n    !ls -la {REPO_DIR}/*.py | head -5\n    \n    framework_dir = REPO_DIR\nelse:\n    # Local environment - assume we're already in the repo\n    framework_dir = os.path.abspath('.')\n    print(f\"\\nüìÇ Step 1: Using local directory: {framework_dir}\")\n\n# Step 2: Add to Python path\nprint(f\"\\nüì¶ Step 2: Adding to Python path...\")\nif framework_dir not in sys.path:\n    sys.path.insert(0, framework_dir)\n    print(f\"   ‚úÖ Added: {framework_dir}\")\nelse:\n    print(f\"   ‚úÖ Already in path: {framework_dir}\")\n\nprint(f\"\\n   Python path (first 3 entries):\")\nfor i, p in enumerate(sys.path[:3], 1):\n    print(f\"     {i}. {p}\")\n\n# Step 3: Verify framework file exists\nprint(f\"\\nüîç Step 3: Verifying framework file...\")\nframework_file = os.path.join(framework_dir, 'olmoe_routing_experiments.py')\nif os.path.exists(framework_file):\n    file_size = os.path.getsize(framework_file)\n    print(f\"   ‚úÖ Found: olmoe_routing_experiments.py ({file_size:,} bytes)\")\nelse:\n    print(f\"   ‚ùå Not found: {framework_file}\")\n    print(f\"\\n   Available Python files:\")\n    !ls -la {framework_dir}/*.py\n    raise Exception(\"Framework file not found!\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"‚úÖ FRAMEWORK SETUP COMPLETE\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"IMPORTING FRAMEWORK MODULES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Clear any cached imports\n",
    "if 'olmoe_routing_experiments' in sys.modules:\n",
    "    print(\"üîÑ Clearing cached module...\")\n",
    "    del sys.modules['olmoe_routing_experiments']\n",
    "\n",
    "# Import the framework\n",
    "print(\"\\nüì¶ Importing olmoe_routing_experiments...\")\n",
    "\n",
    "try:\n",
    "    import olmoe_routing_experiments\n",
    "    print(\"‚úÖ Module imported successfully!\")\n",
    "    \n",
    "    # Import specific classes\n",
    "    from olmoe_routing_experiments import (\n",
    "        RoutingConfig,\n",
    "        ExperimentResults,\n",
    "        RoutingStrategy,\n",
    "        RegularRouting,\n",
    "        NormalizedRouting,\n",
    "        UniformRouting,\n",
    "        AdaptiveRouting,\n",
    "        RoutingExperimentRunner,\n",
    "        ModelPatchingUtils\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìö Available Components:\")\n",
    "    print(\"  ‚Ä¢ RoutingConfig, ExperimentResults\")\n",
    "    print(\"  ‚Ä¢ RoutingStrategy (base class)\")\n",
    "    print(\"  ‚Ä¢ RegularRouting, NormalizedRouting\")\n",
    "    print(\"  ‚Ä¢ UniformRouting, AdaptiveRouting\")\n",
    "    print(\"  ‚Ä¢ RoutingExperimentRunner\")\n",
    "    print(\"  ‚Ä¢ ModelPatchingUtils\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ ALL IMPORTS SUCCESSFUL\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"\\n‚ùå Import failed: {e}\")\n",
    "    print(\"\\nüîß Debugging info:\")\n",
    "    print(f\"   sys.path[0]: {sys.path[0]}\")\n",
    "    print(f\"   Current dir: {os.getcwd()}\")\n",
    "    \n",
    "    # List Python files in path\n",
    "    import glob\n",
    "    py_files = glob.glob(os.path.join(sys.path[0], '*.py'))\n",
    "    print(f\"\\n   Python files in {sys.path[0]}:\")\n",
    "    for f in py_files[:10]:\n",
    "        print(f\"     - {os.path.basename(f)}\")\n",
    "    \n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Framework with Custom Patching Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedRoutingExperimentRunner(RoutingExperimentRunner):\n",
    "    \"\"\"\n",
    "    Extended runner with support for custom model patching.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, use_custom_routing=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.use_custom_routing = use_custom_routing\n",
    "        \n",
    "        if use_custom_routing:\n",
    "            print(\"\\nüîß CUSTOM ROUTING MODE ENABLED\")\n",
    "            print(\"   Model will be patched with custom_select_experts\")\n",
    "    \n",
    "    def _set_expert_count(self, num_experts):\n",
    "        \"\"\"Override to support custom routing.\"\"\"\n",
    "        super()._set_expert_count(num_experts)\n",
    "        \n",
    "        if self.use_custom_routing:\n",
    "            # Re-patch model with new expert count\n",
    "            patch_model_with_custom_routing(self.model, top_k=num_experts)\n",
    "\n",
    "print(\"‚úÖ Extended framework with custom patching support ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running Tests\n",
    "\n",
    "Validate the framework is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RUNNING VALIDATION TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: Routing Strategies\n",
    "print(\"\\n[1/5] Testing routing strategies...\")\n",
    "torch.manual_seed(42)\n",
    "logits = torch.randn(1, 1, 64)\n",
    "\n",
    "regular = RegularRouting(num_experts=8)\n",
    "normalized = NormalizedRouting(num_experts=8)\n",
    "uniform = UniformRouting(num_experts=8)\n",
    "\n",
    "reg_indices, reg_weights = regular.route(logits)\n",
    "norm_indices, norm_weights = normalized.route(logits)\n",
    "uni_indices, uni_weights = uniform.route(logits)\n",
    "\n",
    "# Verify uniform has equal weights\n",
    "assert torch.allclose(uni_weights, torch.ones_like(uni_weights) / 8, atol=1e-6)\n",
    "print(\"   ‚úÖ Routing strategies work correctly\")\n",
    "\n",
    "# Test 2: Custom Expert Selection\n",
    "print(\"\\n[2/5] Testing custom expert selection...\")\n",
    "router_logits = torch.randn(10, 64)  # 10 tokens, 64 experts\n",
    "weights, indices = custom_select_experts(router_logits, top_k=8, num_experts=64)\n",
    "\n",
    "assert weights.shape == (10, 8)\n",
    "assert indices.shape == (10, 8)\n",
    "assert torch.allclose(weights, torch.ones_like(weights) / 8, atol=1e-6)\n",
    "print(\"   ‚úÖ Custom expert selection works correctly\")\n",
    "\n",
    "# Test 3: Different expert counts produce different routing\n",
    "print(\"\\n[3/5] Testing expert count variation...\")\n",
    "weights_4, indices_4 = custom_select_experts(router_logits, top_k=4, num_experts=64)\n",
    "weights_16, indices_16 = custom_select_experts(router_logits, top_k=16, num_experts=64)\n",
    "\n",
    "assert weights_4.shape[-1] == 4\n",
    "assert weights_16.shape[-1] == 16\n",
    "print(\"   ‚úÖ Different expert counts work correctly\")\n",
    "\n",
    "# Test 4: Verify uniform weights\n",
    "print(\"\\n[4/5] Testing uniform weight distribution...\")\n",
    "for k in [4, 8, 16]:\n",
    "    w, _ = custom_select_experts(router_logits, top_k=k, num_experts=64)\n",
    "    expected = torch.ones_like(w) / k\n",
    "    assert torch.allclose(w, expected, atol=1e-6)\n",
    "print(\"   ‚úÖ Uniform weights verified for all k values\")\n",
    "\n",
    "# Test 5: Statistics tracking\n",
    "print(\"\\n[5/5] Testing statistics tracking...\")\n",
    "strategy = UniformRouting(num_experts=8)\n",
    "for _ in range(5):\n",
    "    test_logits = torch.randn(2, 5, 64)\n",
    "    strategy.route(test_logits)\n",
    "\n",
    "stats = strategy.get_summary_stats()\n",
    "assert 'avg_entropy' in stats\n",
    "assert 'avg_concentration' in stats\n",
    "assert 'unique_experts' in stats\n",
    "print(\"   ‚úÖ Statistics tracking works correctly\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL TESTS PASSED!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Experiment\n",
    "\n",
    "Run a minimal experiment to verify everything works (~5 minutes)\n",
    "\n",
    "**This experiment will:**\n",
    "- Test 2 expert counts (8, 16)\n",
    "- Test 2 strategies (regular, custom uniform)\n",
    "- Evaluate on 50 samples\n",
    "- Generate visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"QUICK EXPERIMENT (Standard Routing)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"  Expert counts: [8, 16]\")\n",
    "print(\"  Strategies: [regular, uniform]\")\n",
    "print(\"  Dataset: WikiText-2\")\n",
    "print(\"  Samples: 50\")\n",
    "print(\"  Estimated time: ~5 minutes\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Create runner (standard mode)\n",
    "runner_standard = RoutingExperimentRunner(\n",
    "    model_name=\"allenai/OLMoE-1B-7B-0924\",\n",
    "    device=device,\n",
    "    output_dir=\"./quick_experiment_standard\"\n",
    ")\n",
    "\n",
    "# Run experiments\n",
    "results_df_standard = runner_standard.run_all_experiments(\n",
    "    expert_counts=[8, 16],\n",
    "    strategies=['regular', 'uniform'],\n",
    "    datasets=['wikitext'],\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Standard routing experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nüìä QUICK EXPERIMENT RESULTS (Standard Routing)\\n\")\n",
    "print(results_df_standard[[\n",
    "    'config', 'perplexity', 'token_accuracy', \n",
    "    'tokens_per_second', 'avg_entropy'\n",
    "]].to_string(index=False))\n",
    "\n",
    "# Best configuration\n",
    "best_idx = results_df_standard['perplexity'].idxmin()\n",
    "best = results_df_standard.loc[best_idx]\n",
    "\n",
    "print(\"\\nüèÜ BEST CONFIGURATION:\")\n",
    "print(f\"   Config: {best['config']}\")\n",
    "print(f\"   Perplexity: {best['perplexity']:.2f}\")\n",
    "print(f\"   Accuracy: {best['token_accuracy']:.4f}\")\n",
    "print(f\"   Speed: {best['tokens_per_second']:.1f} tok/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Experiment with Custom Routing (Internal Logging)\n",
    "\n",
    "**This uses the custom forward pass with internal router_logits logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"QUICK EXPERIMENT (Custom Routing with Internal Logging)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"  Expert counts: [8, 16]\")\n",
    "print(\"  Routing: Custom uniform (patched forward pass)\")\n",
    "print(\"  Dataset: WikiText-2\")\n",
    "print(\"  Samples: 50\")\n",
    "print(\"  Features: Internal router_logits logging\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Create runner with custom routing enabled\n",
    "runner_custom = ExtendedRoutingExperimentRunner(\n",
    "    model_name=\"allenai/OLMoE-1B-7B-0924\",\n",
    "    device=device,\n",
    "    output_dir=\"./quick_experiment_custom\",\n",
    "    use_custom_routing=True\n",
    ")\n",
    "\n",
    "# Patch the model\n",
    "patch_model_with_custom_routing(runner_custom.model, top_k=8)\n",
    "\n",
    "# Run experiments (only uniform strategy makes sense with custom routing)\n",
    "results_df_custom = runner_custom.run_all_experiments(\n",
    "    expert_counts=[8, 16],\n",
    "    strategies=['uniform'],  # Custom routing is uniform\n",
    "    datasets=['wikitext'],\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Custom routing experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nüìä QUICK EXPERIMENT RESULTS (Custom Routing)\\n\")\n",
    "print(results_df_custom[[\n",
    "    'config', 'perplexity', 'token_accuracy', \n",
    "    'tokens_per_second', 'avg_entropy'\n",
    "]].to_string(index=False))\n",
    "\n",
    "# Compare with standard\n",
    "print(\"\\nüìà COMPARISON: Custom vs Standard Uniform Routing\\n\")\n",
    "\n",
    "comparison_data = []\n",
    "for expert_count in [8, 16]:\n",
    "    std_row = results_df_standard[\n",
    "        (results_df_standard['num_experts'] == expert_count) & \n",
    "        (results_df_standard['strategy'] == 'uniform')\n",
    "    ].iloc[0]\n",
    "    \n",
    "    custom_row = results_df_custom[\n",
    "        results_df_custom['num_experts'] == expert_count\n",
    "    ].iloc[0]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Expert Count': expert_count,\n",
    "        'Standard PPL': f\"{std_row['perplexity']:.2f}\",\n",
    "        'Custom PPL': f\"{custom_row['perplexity']:.2f}\",\n",
    "        'Difference': f\"{custom_row['perplexity'] - std_row['perplexity']:.2f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nüí° Note: Small differences are expected due to implementation details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Two-Phase Routing Experiment (NEW!)\n\n**üî¨ Advanced: Analyze baseline routing, then test modifications**\n\nThis experiment uses a comprehensive 3-phase approach:\n1. **Phase 1**: Analyze model's natural routing (baseline)\n2. **Phase 2**: Apply modified routing strategies\n3. **Phase 3**: Compare and generate detailed reports\n\nThis tells you:\n- How does the model naturally route tokens to experts?\n- Which experts are underutilized?\n- Does custom routing improve or hurt quality?\n- What's the impact of forcing uniform weights?"
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 70)\nprint(\"TWO-PHASE ROUTING EXPERIMENT\")\nprint(\"=\" * 70)\nprint(\"\\nConfiguration:\")\nprint(\"  Expert counts: [8, 16]\")\nprint(\"  Datasets: [wikitext]\")\nprint(\"  Samples: 50\")\nprint(\"  Modifications: [uniform, normalized]\")\nprint(\"\\nPhases:\")\nprint(\"  1. Baseline: Analyze model's natural routing\")\nprint(\"  2. Modified: Test custom routing strategies\")\nprint(\"  3. Analysis: Compare and generate report\")\nprint(\"\\nEstimated time: ~10 minutes\")\nprint(\"=\" * 70)\n\n# Create runner\nrunner_two_phase = RoutingExperimentRunner(\n    model_name=\"allenai/OLMoE-1B-7B-0924\",\n    device=device,\n    output_dir=\"./two_phase_experiment\"\n)\n\n# Run two-phase experiment\nresults_df, routing_insights = runner_two_phase.run_two_phase_experiment(\n    expert_counts=[8, 16],\n    datasets=['wikitext'],\n    max_samples=50,\n    routing_modifications=['uniform', 'normalized']\n)\n\nprint(\"\\n‚úÖ Two-phase experiment complete!\")\nprint(f\"   Results saved to: {runner_two_phase.output_dir}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Phase 1: Baseline Insights\n\nView what the model naturally does",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 70)\nprint(\"PHASE 1: BASELINE ROUTING INSIGHTS\")\nprint(\"=\" * 70)\n\n# Display insights\ninsights_data = []\nfor key, insight in routing_insights.items():\n    insights_data.append({\n        'Experts': insight['num_experts'],\n        'Dataset': insight['dataset'],\n        'Perplexity': f\"{insight['baseline_perplexity']:.2f}\",\n        'Accuracy': f\"{insight['baseline_accuracy']:.4f}\",\n        'Experts Used': f\"{insight['unique_experts_used']}/64\",\n        'Utilization': f\"{insight['expert_utilization']:.1%}\",\n        'Entropy': f\"{insight['avg_entropy']:.3f}\"\n    })\n\ninsights_df = pd.DataFrame(insights_data)\nprint(\"\\n\")\nprint(insights_df.to_string(index=False))\n\nprint(\"\\n\\nüìä Key Observations:\")\navg_util = np.mean([i['expert_utilization'] for i in routing_insights.values()])\navg_entropy = np.mean([i['avg_entropy'] for i in routing_insights.values()])\n\nprint(f\"  ‚Ä¢ Average expert utilization: {avg_util:.1%}\")\nprint(f\"  ‚Ä¢ Average routing entropy: {avg_entropy:.3f}\")\n\nif avg_util < 0.5:\n    print(f\"  ‚ö†Ô∏è  Many experts are underutilized!\")\n    print(f\"      ‚Üí Custom routing may help balance usage\")\n\nif avg_entropy < 1.0:\n    print(f\"  ‚ö†Ô∏è  Routing is highly concentrated!\")\n    print(f\"      ‚Üí Model heavily favors certain experts\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Phase 2 & 3: Modified Routing Comparison\n\nCompare baseline vs modified routing strategies",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 70)\nprint(\"PHASE 2 & 3: ROUTING COMPARISON\")\nprint(\"=\" * 70)\n\n# For each configuration, show baseline vs modified\nfor num_experts in sorted(results_df['num_experts'].unique()):\n    print(f\"\\n{'='*70}\")\n    print(f\"Configuration: {num_experts} Experts\")\n    print(f\"{'='*70}\")\n    \n    config_df = results_df[results_df['num_experts'] == num_experts]\n    \n    # Create comparison table\n    comparison = []\n    baseline_row = config_df[config_df['strategy'] == 'baseline'].iloc[0]\n    \n    for _, row in config_df.iterrows():\n        delta_ppl = row['perplexity'] - baseline_row['perplexity']\n        delta_acc = row['token_accuracy'] - baseline_row['token_accuracy']\n        \n        comparison.append({\n            'Strategy': row['strategy'],\n            'Perplexity': f\"{row['perplexity']:.2f}\",\n            'Œî PPL': f\"{delta_ppl:+.2f}\",\n            'Accuracy': f\"{row['token_accuracy']:.4f}\",\n            'Œî Acc': f\"{delta_acc:+.4f}\",\n            'Speed': f\"{row['tokens_per_second']:.1f}\",\n            'Entropy': f\"{row['avg_entropy']:.3f}\"\n        })\n    \n    comp_df = pd.DataFrame(comparison)\n    print(\"\\n\" + comp_df.to_string(index=False))\n    \n    # Highlight findings\n    best_modified = config_df[config_df['strategy'] != 'baseline']['perplexity'].min()\n    if best_modified < baseline_row['perplexity']:\n        improvement = baseline_row['perplexity'] - best_modified\n        best_strategy = config_df[config_df['perplexity'] == best_modified]['strategy'].iloc[0]\n        print(f\"\\n‚úÖ Modified routing improved! Best: {best_strategy} (‚àí{improvement:.2f} PPL)\")\n    else:\n        print(f\"\\n‚ÑπÔ∏è  Baseline routing performs best for this configuration\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Visualize Two-Phase Results\n\nView the generated comparison visualizations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the generated visualization\nfrom IPython.display import Image, display\n\nviz_path = \"./two_phase_experiment/visualizations/two_phase_comparison.png\"\nprint(\"üìä Two-Phase Experiment Visualizations:\\n\")\ndisplay(Image(filename=viz_path))\n\nprint(\"\\n‚úÖ Visualization includes:\")\nprint(\"  1. Perplexity comparison (baseline vs modified)\")\nprint(\"  2. Accuracy comparison\")\nprint(\"  3. Delta perplexity (improvement/degradation)\")\nprint(\"  4. Expert utilization (baseline)\")\nprint(\"  5. Routing entropy comparison\")\nprint(\"  6. Speed-quality trade-off\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Generated Report\n\nView the detailed markdown report",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display the generated report\nfrom IPython.display import Markdown\n\nreport_path = \"./two_phase_experiment/two_phase_report.md\"\n\nwith open(report_path, 'r') as f:\n    report_content = f.read()\n\nprint(\"üìÑ Full Report:\\n\")\ndisplay(Markdown(report_content))\n\nprint(\"\\n\\nüìÅ Files generated:\")\nprint(f\"  ‚Ä¢ {report_path}\")\nprint(f\"  ‚Ä¢ ./two_phase_experiment/two_phase_results.csv\")\nprint(f\"  ‚Ä¢ ./two_phase_experiment/two_phase_results.json\")\nprint(f\"  ‚Ä¢ ./two_phase_experiment/visualizations/two_phase_comparison.png\")\nprint(f\"  ‚Ä¢ ./two_phase_experiment/visualizations/two_phase_comparison.pdf\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## 9. Full Experiments\n\nRun comprehensive experiments with all configurations (~30-60 minutes)\n\n**‚ö†Ô∏è WARNING: This will take significant time and GPU resources!**",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run full experiments\n",
    "RUN_FULL_EXPERIMENTS = False  # Set to True to run\n",
    "\n",
    "if RUN_FULL_EXPERIMENTS:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FULL EXPERIMENTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  Expert counts: [4, 8, 16, 32, 64]\")\n",
    "    print(\"  Strategies: [regular, normalized, uniform]\")\n",
    "    print(\"  Datasets: [wikitext, lambada]\")\n",
    "    print(\"  Samples: 500 per dataset\")\n",
    "    print(\"  Total experiments: 30\")\n",
    "    print(\"  Estimated time: ~60 minutes\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    # Create runner\n",
    "    runner_full = RoutingExperimentRunner(\n",
    "        model_name=\"allenai/OLMoE-1B-7B-0924\",\n",
    "        device=device,\n",
    "        output_dir=\"./full_experiments\"\n",
    "    )\n",
    "    \n",
    "    # Run experiments\n",
    "    results_df_full = runner_full.run_all_experiments(\n",
    "        expert_counts=[4, 8, 16, 32, 64],\n",
    "        strategies=['regular', 'normalized', 'uniform'],\n",
    "        datasets=['wikitext', 'lambada'],\n",
    "        max_samples=500\n",
    "    )\n",
    "    \n",
    "    # Generate visualizations\n",
    "    runner_full.visualize_results(results_df_full)\n",
    "    \n",
    "    # Generate report\n",
    "    runner_full.generate_report(results_df_full)\n",
    "    \n",
    "    print(\"\\n‚úÖ Full experiments complete!\")\n",
    "    print(f\"   Results: {runner_full.output_dir}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Full experiments skipped (set RUN_FULL_EXPERIMENTS = True to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Results Analysis\n\nAnalyze experiment results with various methods"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze_results import ResultAnalyzer\n",
    "\n",
    "# Use quick experiment results for analysis\n",
    "analyzer = ResultAnalyzer(\"./quick_experiment_standard\")\n",
    "\n",
    "# Print summary\n",
    "analyzer.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare strategies\n",
    "comparison = analyzer.compare_strategies()\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal configuration\n",
    "optimal = analyzer.find_optimal_config(\n",
    "    quality_weight=0.7,  # 70% weight on quality\n",
    "    speed_weight=0.3     # 30% weight on speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific strategy\n",
    "analyzer.analyze_strategy('uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Visualization\n\nCreate custom visualizations from results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "# Load results\n",
    "df = results_df_standard\n",
    "\n",
    "# Create custom visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('OLMoE Routing Experiments Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Perplexity by strategy\n",
    "ax1 = axes[0, 0]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax1.plot(\n",
    "        strategy_df['num_experts'], \n",
    "        strategy_df['perplexity'], \n",
    "        marker='o', \n",
    "        label=strategy,\n",
    "        linewidth=2\n",
    "    )\n",
    "ax1.set_xlabel('Number of Experts')\n",
    "ax1.set_ylabel('Perplexity (‚Üì better)')\n",
    "ax1.set_title('Perplexity vs Expert Count')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Token accuracy by strategy\n",
    "ax2 = axes[0, 1]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax2.plot(\n",
    "        strategy_df['num_experts'], \n",
    "        strategy_df['token_accuracy'], \n",
    "        marker='s', \n",
    "        label=strategy,\n",
    "        linewidth=2\n",
    "    )\n",
    "ax2.set_xlabel('Number of Experts')\n",
    "ax2.set_ylabel('Token Accuracy (‚Üë better)')\n",
    "ax2.set_title('Token Accuracy vs Expert Count')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Speed-quality trade-off\n",
    "ax3 = axes[1, 0]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax3.scatter(\n",
    "        strategy_df['perplexity'],\n",
    "        strategy_df['tokens_per_second'],\n",
    "        label=strategy,\n",
    "        s=100,\n",
    "        alpha=0.7\n",
    "    )\n",
    "ax3.set_xlabel('Perplexity (‚Üì better)')\n",
    "ax3.set_ylabel('Tokens/Second (‚Üë better)')\n",
    "ax3.set_title('Speed vs Quality Trade-off')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Routing entropy by strategy\n",
    "ax4 = axes[1, 1]\n",
    "for strategy in df['strategy'].unique():\n",
    "    strategy_df = df[df['strategy'] == strategy]\n",
    "    ax4.plot(\n",
    "        strategy_df['num_experts'], \n",
    "        strategy_df['avg_entropy'], \n",
    "        marker='^', \n",
    "        label=strategy,\n",
    "        linewidth=2\n",
    "    )\n",
    "ax4.set_xlabel('Number of Experts')\n",
    "ax4.set_ylabel('Average Entropy')\n",
    "ax4.set_title('Routing Entropy vs Expert Count')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('custom_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Standard vs Custom Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Standard vs Custom Routing Comparison', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Get uniform routing data from both\n",
    "std_uniform = results_df_standard[results_df_standard['strategy'] == 'uniform']\n",
    "custom_uniform = results_df_custom\n",
    "\n",
    "# 1. Perplexity comparison\n",
    "ax1 = axes[0]\n",
    "x = range(len(std_uniform))\n",
    "width = 0.35\n",
    "ax1.bar([i - width/2 for i in x], std_uniform['perplexity'], width, label='Standard', alpha=0.8)\n",
    "ax1.bar([i + width/2 for i in x], custom_uniform['perplexity'], width, label='Custom (Patched)', alpha=0.8)\n",
    "ax1.set_xlabel('Configuration')\n",
    "ax1.set_ylabel('Perplexity')\n",
    "ax1.set_title('Perplexity: Standard vs Custom')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f\"{row['num_experts']} exp\" for _, row in std_uniform.iterrows()])\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Entropy comparison\n",
    "ax2 = axes[1]\n",
    "ax2.bar([i - width/2 for i in x], std_uniform['avg_entropy'], width, label='Standard', alpha=0.8)\n",
    "ax2.bar([i + width/2 for i in x], custom_uniform['avg_entropy'], width, label='Custom (Patched)', alpha=0.8)\n",
    "ax2.set_xlabel('Configuration')\n",
    "ax2.set_ylabel('Average Entropy')\n",
    "ax2.set_title('Routing Entropy: Standard vs Custom')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f\"{row['num_experts']} exp\" for _, row in std_uniform.iterrows()])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('standard_vs_custom.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Comparison visualization created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "‚úÖ **Environment Setup**: Configured GPU/CPU for optimal performance\n",
    "‚úÖ **Installation**: Installed all required packages\n",
    "‚úÖ **Custom Routing**: Implemented custom expert selection with internal logging\n",
    "‚úÖ **Model Patching**: Added support for patching MoE forward pass\n",
    "‚úÖ **Testing**: Validated all components work correctly\n",
    "‚úÖ **Quick Experiments**: Ran both standard and custom routing experiments\n",
    "‚úÖ **Analysis**: Analyzed results with multiple methods\n",
    "‚úÖ **Visualization**: Created comprehensive visualizations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "View the results above to understand:\n",
    "1. How different expert counts affect quality (perplexity)\n",
    "2. How routing strategies compare\n",
    "3. Speed vs quality trade-offs\n",
    "4. Routing entropy patterns\n",
    "5. Differences between standard and custom routing\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Run Full Experiments**: Set `RUN_FULL_EXPERIMENTS = True` for comprehensive analysis\n",
    "2. **Custom Strategies**: Implement your own routing strategies\n",
    "3. **More Datasets**: Test on additional datasets (PIQA, etc.)\n",
    "4. **Deep Analysis**: Use the ResultAnalyzer for detailed comparisons\n",
    "5. **Save Results**: Download results to Google Drive for further analysis\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `quick_experiment_standard/` - Standard routing results\n",
    "- `quick_experiment_custom/` - Custom routing results\n",
    "- `custom_analysis.png` - Custom visualizations\n",
    "- `standard_vs_custom.png` - Comparison plot\n",
    "\n",
    "### Documentation\n",
    "\n",
    "For more details, see:\n",
    "- `QUICKSTART.md` - Quick start guide\n",
    "- `ROUTING_EXPERIMENTS_README.md` - Complete documentation\n",
    "- `ARCHITECTURE.md` - System architecture\n",
    "- `IMPLEMENTATION_SUMMARY.md` - Technical details\n",
    "\n",
    "---\n",
    "\n",
    "**Happy experimenting! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}