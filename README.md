# OLMoE Inference with More Experts

This repository contains a comprehensive guide for running OLMoE (Open Mixture-of-Experts Language Model) inference with **more experts than the default configuration**.

## Quick Start

### Open in Google Colab

1. Upload `OLMoE_Inference_More_Experts.ipynb` to Google Colab
2. Set runtime to GPU: **Runtime â†’ Change runtime type â†’ GPU (T4 or better)**
3. Run all cells sequentially

### What's Inside

The notebook covers:

- **Understanding OLMoE Architecture**: Deep dive into the MoE mechanism
- **Default Inference (8 experts)**: Baseline performance
- **Modified Inference (16, 32, 64 experts)**: Using more experts for better quality
- **Performance Analysis**: Speed vs quality trade-offs
- **Expert Selection Visualization**: See which experts are activated
- **Adaptive Routing**: Dynamic expert selection based on input complexity

## Key Features

### 1. Flexible Expert Configuration

```python
# Use 16 experts instead of default 8
model.config.num_experts_per_tok = 16
```

### 2. Performance Comparison

| Experts | Speed | Quality | Use Case |
|---------|-------|---------|----------|
| 8 (default) | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† | Real-time apps |
| 16 | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† | Balanced |
| 32 | â˜…â˜…â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | High quality |
| 64 (all) | â˜…â˜†â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | Maximum capacity |

### 3. Adaptive Expert Selection

The notebook includes a smart system that automatically selects the optimal number of experts based on:
- Input length
- Technical complexity
- Domain requirements

## OLMoE Model Overview

- **Model**: allenai/OLMoE-1B-7B-0924
- **Total Parameters**: 6.9 billion
- **Active Parameters**: 1.3 billion per token (default)
- **Experts per Layer**: 64
- **Default Top-k**: 8 experts
- **Architecture**: 16 transformer layers with MoE feedforward

## Requirements

- **GPU**: Minimum 12GB VRAM (Colab T4 is sufficient)
- **Python**: 3.8+
- **PyTorch**: 2.0+
- **Transformers**: 4.40+

All dependencies are automatically installed in the notebook.

## How Expert Selection Works

```
Input Token
    â†“
Router Network (Linear Layer)
    â†“
Softmax â†’ Top-k Selection (k = 8, 16, 32, or 64)
    â†“
Selected Experts Process Token
    â†“
Weighted Combination
    â†“
Output
```

## When to Use More Experts?

### Use More Experts (16-32) When:
- âœ… Complex reasoning tasks
- âœ… Multi-domain questions (code + math + science)
- âœ… Output quality > speed
- âœ… High uncertainty in domain

### Use Default (8) When:
- âœ… Real-time interaction needed
- âœ… Well-defined domain
- âœ… Resource constraints
- âœ… Batch processing

## Example Usage

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model
model = AutoModelForCausalLM.from_pretrained(
    "allenai/OLMoE-1B-7B-0924",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("allenai/OLMoE-1B-7B-0924")

# Increase experts to 16
model.config.num_experts_per_tok = 16

# Generate
inputs = tokenizer("Explain quantum computing:", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=150)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## Performance Benchmarks

Based on Colab T4 GPU:

| Experts | Tokens/sec | Relative Speed | Relative Time |
|---------|-----------|----------------|---------------|
| 8 | ~25-30 | 1.0x | 1.0x |
| 16 | ~15-20 | 0.6x | 1.5x |
| 32 | ~8-12 | 0.3x | 3.0x |
| 64 | ~4-6 | 0.15x | 7.0x |

*Note: Actual performance varies by prompt and GPU*

## Visualizations

The notebook generates several visualizations:

1. **Performance Comparison**: Time and throughput across expert counts
2. **Router Analysis**: Which experts are selected for each token
3. **Expert Activation Heatmap**: Patterns of expert specialization
4. **Active Parameter Growth**: Capacity vs expert count

## Research Insights

From the OLMoE paper, experts show specialization in:

- **Domain-specific content** (code, mathematics, science)
- **Vocabulary clusters** (technical vs common words)
- **Positional patterns** (sequence position sensitivity)
- **Syntactic structures** (questions, statements, lists)
- **Semantic topics** (technology, history, arts)

## References

- **Paper**: [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060)
- **GitHub**: [allenai/OLMoE](https://github.com/allenai/OLMoE)
- **Model**: [Hugging Face](https://huggingface.co/allenai/OLMoE-1B-7B-0924)
- **Docs**: [Transformers Documentation](https://huggingface.co/docs/transformers/model_doc/olmoe)

## Contributing

This is a research and educational repository. Feel free to:
- Experiment with different expert configurations
- Add new routing strategies
- Improve visualization techniques
- Share your findings

## License

This project uses the OLMoE model which is released under the Apache 2.0 license by Allen AI.

---

**Created by**: Senior AI Researcher & Software Engineer
**Last Updated**: 2025-11-15

ðŸš€ **Happy experimenting with OLMoE!**
