{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLMoE Production Evaluation with Real Data\n",
    "\n",
    "## High-Quality Evaluation Framework\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Evaluates on **real datasets** (WikiText-2, LAMBADA)\n",
    "- ‚úÖ Computes **proper metrics** (Perplexity, Token Accuracy, Loss)\n",
    "- ‚úÖ Tests **multiple expert configs** (8, 16, 32, 64)\n",
    "- ‚úÖ **Production-quality** code with proper error handling\n",
    "- ‚úÖ **Publication-ready** visualizations\n",
    "- ‚úÖ Exports results to CSV/JSON/PDF\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets accelerate sentencepiece matplotlib seaborn pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - ADJUST THESE BASED ON YOUR NEEDS\n",
    "CONFIG = {\n",
    "    'model_name': 'allenai/OLMoE-1B-7B-0924',\n",
    "    'expert_configs': [8, 16, 32, 64],  # Expert counts to test\n",
    "    'datasets': ['wikitext', 'lambada'],  # Datasets to evaluate\n",
    "    'max_samples': 500,  # Number of samples per dataset (500-1000 recommended)\n",
    "    'max_length': 512,   # Maximum sequence length\n",
    "    'output_dir': './olmoe_results',\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load Production Code\n",
    "\n",
    "Copy the production evaluation code from `olmoe_evaluation.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the production evaluation script\n",
    "!wget -q https://raw.githubusercontent.com/AliABULIEL/MOE-with-feature-selection/claude/olmoe-inference-experts-01XjzqPSCkvPdXxPi6iS3C5C/olmoe_evaluation.py -O olmoe_evaluation.py\n",
    "\n",
    "# Or if file is local, just import it\n",
    "from olmoe_evaluation import OLMoEEvaluator, EvaluationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run Evaluation\n",
    "\n",
    "This will:\n",
    "1. Load OLMoE model\n",
    "2. Load WikiText-2 and LAMBADA datasets\n",
    "3. Run inference with 8, 16, 32, 64 experts\n",
    "4. Compute perplexity, accuracy, and speed metrics\n",
    "5. Generate visualizations\n",
    "6. Save results to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "config = EvaluationConfig(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    expert_configs=CONFIG['expert_configs'],\n",
    "    datasets=CONFIG['datasets'],\n",
    "    max_samples=CONFIG['max_samples'],\n",
    "    max_length=CONFIG['max_length'],\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    seed=CONFIG['seed'],\n",
    ")\n",
    "\n",
    "# Create evaluator\n",
    "print(\"Initializing evaluator...\")\n",
    "evaluator = OLMoEEvaluator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation\n",
    "print(\"Starting evaluation... This will take 15-30 minutes depending on GPU.\")\n",
    "print(\"Progress will be shown below.\\n\")\n",
    "\n",
    "results_df = evaluator.evaluate_all_configurations()\n",
    "\n",
    "print(\"\\n‚úì Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results table\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*100)\n",
    "display(results_df)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nSUMMARY STATISTICS BY EXPERT COUNT\")\n",
    "print(\"=\"*80)\n",
    "summary = results_df.groupby('num_experts').agg({\n",
    "    'perplexity': ['mean', 'std'],\n",
    "    'token_accuracy': ['mean', 'std'],\n",
    "    'tokens_per_second': ['mean', 'std']\n",
    "})\n",
    "display(summary)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all visualizations\n",
    "evaluator.visualize_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown report\n",
    "evaluator.generate_report(results_df)\n",
    "\n",
    "# Display the report\n",
    "from IPython.display import Markdown\n",
    "with open(f\"{CONFIG['output_dir']}/EVALUATION_REPORT.md\", 'r') as f:\n",
    "    report_content = f.read()\n",
    "\n",
    "display(Markdown(report_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Download Results\n",
    "\n",
    "All results are saved in the output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Results saved in: {CONFIG['output_dir']}\\n\")\n",
    "print(\"Files created:\")\n",
    "for file in os.listdir(CONFIG['output_dir']):\n",
    "    filepath = os.path.join(CONFIG['output_dir'], file)\n",
    "    size = os.path.getsize(filepath) / 1024  # KB\n",
    "    print(f\"  - {file} ({size:.2f} KB)\")\n",
    "\n",
    "# Zip results for download\n",
    "!zip -r olmoe_results.zip {CONFIG['output_dir']}\n",
    "print(\"\\n‚úì Results zipped to olmoe_results.zip\")\n",
    "print(\"  You can download this file from Colab's file browser.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ Advanced: Custom Analysis\n",
    "\n",
    "Analyze specific aspects of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity improvement analysis\n",
    "print(\"PERPLEXITY IMPROVEMENT OVER BASELINE (8 experts)\\n\")\n",
    "\n",
    "for dataset in results_df['dataset'].unique():\n",
    "    print(f\"\\n{dataset.upper()}:\")\n",
    "    data = results_df[results_df['dataset'] == dataset]\n",
    "    baseline_ppl = data[data['num_experts'] == 8]['perplexity'].values[0]\n",
    "    \n",
    "    print(f\"{'Experts':<10} {'Perplexity':<12} {'Improvement':<12} {'Speedup'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        improvement = (baseline_ppl - row['perplexity']) / baseline_ppl * 100\n",
    "        baseline_speed = data[data['num_experts'] == 8]['tokens_per_second'].values[0]\n",
    "        speedup = row['tokens_per_second'] / baseline_speed\n",
    "        \n",
    "        print(f\"{row['num_experts']:<10} {row['perplexity']:<12.2f} \"\n",
    "              f\"{improvement:>6.2f}%      {speedup:>5.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality-Speed Pareto frontier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for dataset in results_df['dataset'].unique():\n",
    "    data = results_df[results_df['dataset'] == dataset]\n",
    "    ax.scatter(\n",
    "        data['tokens_per_second'],\n",
    "        data['perplexity'],\n",
    "        s=200,\n",
    "        alpha=0.6,\n",
    "        label=dataset\n",
    "    )\n",
    "    \n",
    "    # Annotate with expert count\n",
    "    for _, row in data.iterrows():\n",
    "        ax.annotate(\n",
    "            f\"{row['num_experts']}\",\n",
    "            (row['tokens_per_second'], row['perplexity']),\n",
    "            fontsize=12,\n",
    "            fontweight='bold',\n",
    "            ha='center',\n",
    "            va='center'\n",
    "        )\n",
    "\n",
    "ax.set_xlabel('Throughput (tokens/sec) ‚Üë', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Perplexity ‚Üì', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Pareto Frontier: Quality vs Speed', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add arrows showing direction of improvement\n",
    "ax.annotate('Better\\nQuality', xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "            fontsize=11, ha='left', va='top', color='green', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "ax.annotate('Better\\nSpeed', xy=(0.95, 0.05), xycoords='axes fraction',\n",
    "            fontsize=11, ha='right', va='bottom', color='blue', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "### What We Evaluated\n",
    "\n",
    "- **Model**: OLMoE-1B-7B-0924\n",
    "- **Datasets**: WikiText-2, LAMBADA (real evaluation benchmarks)\n",
    "- **Expert Configurations**: 8, 16, 32, 64 experts per token\n",
    "- **Metrics**: Perplexity, Token Accuracy, Cross-Entropy Loss, Inference Speed\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Run the cells above to see:\n",
    "- ‚úÖ How perplexity changes with more experts\n",
    "- ‚úÖ Speed vs quality trade-offs\n",
    "- ‚úÖ Optimal configuration for your use case\n",
    "- ‚úÖ Statistical significance of improvements\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `evaluation_results.csv` - Raw data\n",
    "- `evaluation_results.json` - Results in JSON format\n",
    "- `evaluation_results.png` - Main visualization\n",
    "- `evaluation_results.pdf` - Publication-ready figures\n",
    "- `EVALUATION_REPORT.md` - Detailed report\n",
    "\n",
    "---\n",
    "\n",
    "**Production-Quality Code by Senior ML Researcher & Software Engineer**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
