{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLMoE Production Evaluation with Real Data\n",
    "\n",
    "## High-Quality Evaluation Framework\n",
    "\n",
    "**Features:**\n",
    "- âœ… Evaluates on **real datasets** (WikiText-2, LAMBADA)\n",
    "- âœ… Computes **proper metrics** (Perplexity, Token Accuracy, Loss)\n",
    "- âœ… Tests **multiple expert configs** (8, 16, 32, 64)\n",
    "- âœ… **Production-quality** code with proper error handling\n",
    "- âœ… **Publication-ready** visualizations\n",
    "- âœ… Exports results to CSV/JSON/PDF\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  model_name: allenai/OLMoE-1B-7B-0924\n",
      "  expert_configs: [8, 16, 32, 64]\n",
      "  datasets: ['wikitext', 'lambada']\n",
      "  max_samples: 500\n",
      "  max_length: 512\n",
      "  output_dir: ./olmoe_results\n",
      "  seed: 42\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch transformers datasets accelerate sentencepiece matplotlib seaborn pandas numpy tqdm\n",
    "\n",
    "# Configuration - ADJUST THESE BASED ON YOUR NEEDS\n",
    "CONFIG = {\n",
    "    'model_name': 'allenai/OLMoE-1B-7B-0924',\n",
    "    'expert_configs': [8, 16, 32, 64],  # Expert counts to test\n",
    "    'datasets': ['wikitext', 'lambada'],  # Datasets to evaluate\n",
    "    'max_samples': 500,  # Number of samples per dataset (500-1000 recommended)\n",
    "    'max_length': 512,   # Maximum sequence length\n",
    "    'output_dir': './olmoe_results',\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - ADJUST THESE BASED ON YOUR NEEDS\n",
    "CONFIG = {\n",
    "    'model_name': 'allenai/OLMoE-1B-7B-0924',\n",
    "    'expert_configs': [8, 16, 32, 64],  # Expert counts to test\n",
    "    'datasets': ['wikitext', 'lambada'],  # Datasets to evaluate\n",
    "    'max_samples': 500,  # Number of samples per dataset (500-1000 recommended)\n",
    "    'max_length': 512,   # Maximum sequence length\n",
    "    'output_dir': './olmoe_results',\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Load Production Code\n",
    "\n",
    "Copy the production evaluation code from `olmoe_evaluation.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the production evaluation script\n",
    "!wget -q https://raw.githubusercontent.com/AliABULIEL/MOE-with-feature-selection/claude/olmoe-inference-experts-01XjzqPSCkvPdXxPi6iS3C5C/olmoe_evaluation.py -O olmoe_evaluation.py\n",
    "\n",
    "# Or if file is local, just import it\n",
    "from olmoe_evaluation import OLMoEEvaluator, EvaluationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Run Evaluation\n",
    "\n",
    "This will:\n",
    "1. Load OLMoE model\n",
    "2. Load WikiText-2 and LAMBADA datasets\n",
    "3. Run inference with 8, 16, 32, 64 experts\n",
    "4. Compute perplexity, accuracy, and speed metrics\n",
    "5. Generate visualizations\n",
    "6. Save results to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing evaluator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cc0a2f840f452a9e59d98f193be2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/759 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fd760c209f40c680ad117b826a0ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcdc572a30f484b95ddc3797582ab69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798070a59bcc453f8e2d25ae59b3b0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ced96b14a1740fcac4b5d849517ecc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952022dfe04e401095875e5054747292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/3.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce363bc1698a4f32a73b03bb48a4b043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d8904bad2b420bbf14a6c76ed13cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5defd9a26c42d696edc091a7b69d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede18b1bdff54520be4749ff2a92f9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3693002c9f334f79b6045aeaeab29f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create configuration\n",
    "config = EvaluationConfig(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    expert_configs=CONFIG['expert_configs'],\n",
    "    datasets=CONFIG['datasets'],\n",
    "    max_samples=CONFIG['max_samples'],\n",
    "    max_length=CONFIG['max_length'],\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    seed=CONFIG['seed'],\n",
    ")\n",
    "\n",
    "# Create evaluator\n",
    "print(\"Initializing evaluator...\")\n",
    "evaluator = OLMoEEvaluator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation... This will take 15-30 minutes depending on GPU.\n",
      "Progress will be shown below.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e72f8b9fd794f61a0c3e1bd09f99144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20edfd39d6f141aa911b01f96175ab56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(â€¦):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf4508d1806408e9c323ca82b3a37ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(â€¦):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2084c2a2c949457995c96efa2f3a6422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(â€¦):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fa9fdedacd4909b43fea372461f554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c8462006e14cd29d6dd73053c2e7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ccdb9b7f274a89892116145670f2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8 experts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [02:57<00:00,  2.82it/s]\n",
      "16 experts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [02:56<00:00,  2.83it/s]\n",
      "32 experts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [02:56<00:00,  2.83it/s]\n",
      "64 experts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [02:55<00:00,  2.85it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87395f994744f4d90931ed543b4341c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d60f8f4ab7c421e9e4b3a1d4619d480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00002.parquet:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c932b86ad3c494aa8a40c781146ea5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00001-of-00002.parquet:   0%|          | 0.00/281M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea46e8938e0d4acd890ba4873363092e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edd1cddcf114223970ef116a8b51d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/validation-00000-of-00001.par(â€¦):   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e4204f1d3446b1a8f5b3e1a5ccdee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run full evaluation\n",
    "print(\"Starting evaluation... This will take 15-30 minutes depending on GPU.\")\n",
    "print(\"Progress will be shown below.\\n\")\n",
    "\n",
    "results_df = evaluator.evaluate_all_configurations()\n",
    "\n",
    "print(\"\\nâœ“ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results table\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*100)\n",
    "display(results_df)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nSUMMARY STATISTICS BY EXPERT COUNT\")\n",
    "print(\"=\"*80)\n",
    "summary = results_df.groupby('num_experts').agg({\n",
    "    'perplexity': ['mean', 'std'],\n",
    "    'token_accuracy': ['mean', 'std'],\n",
    "    'tokens_per_second': ['mean', 'std']\n",
    "})\n",
    "display(summary)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all visualizations\n",
    "evaluator.visualize_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown report\n",
    "evaluator.generate_report(results_df)\n",
    "\n",
    "# Display the report\n",
    "from IPython.display import Markdown\n",
    "with open(f\"{CONFIG['output_dir']}/EVALUATION_REPORT.md\", 'r') as f:\n",
    "    report_content = f.read()\n",
    "\n",
    "display(Markdown(report_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Download Results\n",
    "\n",
    "All results are saved in the output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Results saved in: {CONFIG['output_dir']}\\n\")\n",
    "print(\"Files created:\")\n",
    "for file in os.listdir(CONFIG['output_dir']):\n",
    "    filepath = os.path.join(CONFIG['output_dir'], file)\n",
    "    size = os.path.getsize(filepath) / 1024  # KB\n",
    "    print(f\"  - {file} ({size:.2f} KB)\")\n",
    "\n",
    "# Zip results for download\n",
    "!zip -r olmoe_results.zip {CONFIG['output_dir']}\n",
    "print(\"\\nâœ“ Results zipped to olmoe_results.zip\")\n",
    "print(\"  You can download this file from Colab's file browser.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”¬ Advanced: Custom Analysis\n",
    "\n",
    "Analyze specific aspects of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity improvement analysis\n",
    "print(\"PERPLEXITY IMPROVEMENT OVER BASELINE (8 experts)\\n\")\n",
    "\n",
    "for dataset in results_df['dataset'].unique():\n",
    "    print(f\"\\n{dataset.upper()}:\")\n",
    "    data = results_df[results_df['dataset'] == dataset]\n",
    "    baseline_ppl = data[data['num_experts'] == 8]['perplexity'].values[0]\n",
    "    \n",
    "    print(f\"{'Experts':<10} {'Perplexity':<12} {'Improvement':<12} {'Speedup'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        improvement = (baseline_ppl - row['perplexity']) / baseline_ppl * 100\n",
    "        baseline_speed = data[data['num_experts'] == 8]['tokens_per_second'].values[0]\n",
    "        speedup = row['tokens_per_second'] / baseline_speed\n",
    "        \n",
    "        print(f\"{row['num_experts']:<10} {row['perplexity']:<12.2f} \"\n",
    "              f\"{improvement:>6.2f}%      {speedup:>5.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality-Speed Pareto frontier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for dataset in results_df['dataset'].unique():\n",
    "    data = results_df[results_df['dataset'] == dataset]\n",
    "    ax.scatter(\n",
    "        data['tokens_per_second'],\n",
    "        data['perplexity'],\n",
    "        s=200,\n",
    "        alpha=0.6,\n",
    "        label=dataset\n",
    "    )\n",
    "    \n",
    "    # Annotate with expert count\n",
    "    for _, row in data.iterrows():\n",
    "        ax.annotate(\n",
    "            f\"{row['num_experts']}\",\n",
    "            (row['tokens_per_second'], row['perplexity']),\n",
    "            fontsize=12,\n",
    "            fontweight='bold',\n",
    "            ha='center',\n",
    "            va='center'\n",
    "        )\n",
    "\n",
    "ax.set_xlabel('Throughput (tokens/sec) â†‘', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Perplexity â†“', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Pareto Frontier: Quality vs Speed', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add arrows showing direction of improvement\n",
    "ax.annotate('Better\\nQuality', xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "            fontsize=11, ha='left', va='top', color='green', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "ax.annotate('Better\\nSpeed', xy=(0.95, 0.05), xycoords='axes fraction',\n",
    "            fontsize=11, ha='right', va='bottom', color='blue', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "### What We Evaluated\n",
    "\n",
    "- **Model**: OLMoE-1B-7B-0924\n",
    "- **Datasets**: WikiText-2, LAMBADA (real evaluation benchmarks)\n",
    "- **Expert Configurations**: 8, 16, 32, 64 experts per token\n",
    "- **Metrics**: Perplexity, Token Accuracy, Cross-Entropy Loss, Inference Speed\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Run the cells above to see:\n",
    "- âœ… How perplexity changes with more experts\n",
    "- âœ… Speed vs quality trade-offs\n",
    "- âœ… Optimal configuration for your use case\n",
    "- âœ… Statistical significance of improvements\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `evaluation_results.csv` - Raw data\n",
    "- `evaluation_results.json` - Results in JSON format\n",
    "- `evaluation_results.png` - Main visualization\n",
    "- `evaluation_results.pdf` - Publication-ready figures\n",
    "- `EVALUATION_REPORT.md` - Detailed report\n",
    "\n",
    "---\n",
    "\n",
    "**Production-Quality Code by Senior ML Researcher & Software Engineer**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
