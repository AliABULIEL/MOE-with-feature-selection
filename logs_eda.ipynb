{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a97f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:34.047460Z",
     "start_time": "2025-12-06T17:38:33.276773Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import gaussian_kde\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabcf57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:35.010804Z",
     "start_time": "2025-12-06T17:38:33.878322Z"
    }
   },
   "outputs": [],
   "source": [
    "# open a file in \"./logs/8experts_baseline_hellaswag_internal_routing.json\" and read its content\n",
    "with open(\"/Users/aliab/Downloads/8experts_baseline_lambada_internal_routing.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce1a9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:35.012245Z",
     "start_time": "2025-12-06T17:38:34.949422Z"
    }
   },
   "outputs": [],
   "source": [
    "# find out the structure of the data\n",
    "for key in data.keys():\n",
    "    print(f\"{key}: {type(data[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4debf09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:35.013162Z",
     "start_time": "2025-12-06T17:38:34.962163Z"
    }
   },
   "outputs": [],
   "source": [
    "for key in data['samples'][0].keys():\n",
    "    print(f\"{key}: {type(data['samples'][0][key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dddea41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:35.014053Z",
     "start_time": "2025-12-06T17:38:34.967150Z"
    }
   },
   "outputs": [],
   "source": [
    "for key in data['samples'][0]['layers'][0].keys():\n",
    "    print(f\"{key}: {type(data['samples'][0]['layers'][0][key])}\")\n",
    "\n",
    "print((data['samples'][0]['layers'][0]['router_logits_sample'][0]))\n",
    "print((data['samples'][0]['layers'][0]['expert_weights'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729b994",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:35.409749Z",
     "start_time": "2025-12-06T17:38:34.975080Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The structure of the JSON file is as follows:\n",
    "{\n",
    "  \"config\": string,\n",
    "  \"strategy\": string,\n",
    "  \"num_experts\": int,\n",
    "  \"dataset\": string,\n",
    "  \"samples\": [\n",
    "    {\n",
    "      \"sample_id\": int,\n",
    "      \"num_tokens\": int,\n",
    "      \"loss\": float,\n",
    "      \"layers\": [\n",
    "        \"layer\": int,\n",
    "        \"router_logits_shape\": [int, int],\n",
    "        \"selected_experts\": list[list[int]],\n",
    "        \"expert_weights\": list[list[float]],\n",
    "        \"router_logits_sample\": list[list[float]]\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# I want an aggregate list of expert weights per layer across all samples\n",
    "num_layers = len(data['samples'][0]['layers'])\n",
    "layers_expert_weights = [[] for _ in range(num_layers)]\n",
    "layers_expert_choices = [[] for _ in range(num_layers)]\n",
    "layers_router_logits = [[] for _ in range(num_layers)]\n",
    "layers_router_logits_raw = [[] for _ in range(num_layers)]\n",
    "for sample in data['samples']:\n",
    "    for layer in sample['layers']:\n",
    "        layer_idx = layer['layer']\n",
    "        layers_expert_weights[layer_idx].extend(layer['expert_weights'])\n",
    "        layers_expert_choices[layer_idx].extend(layer['selected_experts'])\n",
    "        layer_router_logits = layer['router_logits_sample']\n",
    "        softmax_router_logits = np.exp(layer_router_logits) / np.sum(np.exp(layer_router_logits), axis=1, keepdims=True)\n",
    "        layers_router_logits[layer_idx].extend(softmax_router_logits)\n",
    "        layers_router_logits_raw[layer_idx].extend(layer_router_logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86550bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:35.880510Z",
     "start_time": "2025-12-06T17:38:35.379287Z"
    }
   },
   "outputs": [],
   "source": [
    "# The shape of layers_router_logits is (16, 5000, 64), 16 is the number of layers, 5000 is the number of tokens across all samples, 64 is the number of experts\n",
    "# I want an array of shape (16, 320000) where each row is the flattened router logits for that layer, so that I can do an histogram of the router softmax outputs per layer\n",
    "overall_router_logits = np.array([np.array(arr).flatten() for arr in layers_router_logits])\n",
    "raw_flat_router_logits = np.array([np.array(arr).flatten() for arr in layers_router_logits_raw])\n",
    "\n",
    "# I also want an array of shape (16, 64, 5000) so I can plot the distribution of the softmax outputs per expert per layer\n",
    "softmax_router_logits_per_expert = np.array([np.array(arr).T for arr in layers_router_logits])\n",
    "raw_router_logits_per_expert = np.array([np.array(arr).T for arr in layers_router_logits_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aaa5e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:35.921586Z",
     "start_time": "2025-12-06T17:38:35.871903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, layers_expert_weights is a list of length num_layers, where each element is a list of expert weights for that layer across all samples\n",
    "# Similarly, layers_expert_choices is a list of length num_layers, where each element is a list of selected experts for that layer across all samples\n",
    "# Let's convert them to numpy arrays for easier manipulation\n",
    "layers_expert_weights = np.array([np.array(weights) for weights in layers_expert_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc7914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:35.923204Z",
     "start_time": "2025-12-06T17:38:35.886500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create another list which is the sum of expert weights per token per layer\n",
    "layers_expert_weights_sum = [np.sum(weights, axis=1) for weights in layers_expert_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bcbc60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:39.556890Z",
     "start_time": "2025-12-06T17:38:35.900369Z"
    }
   },
   "outputs": [],
   "source": [
    "# using layers_expert_weights_sum, plot the distribution of the sum of expert weights per token for each layer\n",
    "for layer_idx, weights_sum in enumerate(layers_expert_weights_sum):\n",
    "    path_to_dir = \"./plots/layers_expert_weights_sum\"\n",
    "    path_to_save = os.path.join(path_to_dir, f\"layer_{layer_idx}_expert_weights_sum_distribution.png\")\n",
    "    if os.path.exists(path_to_save):\n",
    "        print(f\"Plot for layer {layer_idx} already exists, skipping...\")\n",
    "        continue\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # make sure the x axis is between 0 and 1\n",
    "    sns.histplot(weights_sum, bins=50, kde=True)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.title(f\"Distribution of Sum of Expert Weights per Token - Layer {layer_idx}\")\n",
    "    plt.xlabel(\"Sum of Expert Weights\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid()\n",
    "\n",
    "    # Save the plot as a png file in ./plots/layer_{layer_idx}_expert_weights_sum_distribution.png\n",
    "    os.makedirs(path_to_dir, exist_ok=True)\n",
    "    plt.savefig(f\"./plots/layers_expert_weights_sum/layer_{layer_idx}_expert_weights_sum_distribution.png\")\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe297591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:39.656150Z",
     "start_time": "2025-12-06T17:38:39.561828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now I want to count how much each expert was chosen per layer\n",
    "layers_expert_choice_counts = np.array([])\n",
    "for layer_choices in layers_expert_choices:\n",
    "    # layer_choices is a list of lists, where each inner list is the selected experts for a token\n",
    "    # flatten the list\n",
    "    flat_choices = [expert for token_choices in layer_choices for expert in token_choices]\n",
    "    # count the occurrences of each expert\n",
    "    unique, counts = np.unique(flat_choices, return_counts=True)\n",
    "    choice_count_dict = dict(zip(unique, counts))\n",
    "    layers_expert_choice_counts = np.append(layers_expert_choice_counts, np.array([choice_count_dict]))\n",
    "# Print the expert choice counts per layer\n",
    "print(layers_expert_choice_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4b2b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:50.878487Z",
     "start_time": "2025-12-06T17:38:39.645650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the expert choice counts per layer as a bar plot\n",
    "for layer_idx, choice_count_dict in enumerate(layers_expert_choice_counts):\n",
    "    path_to_dir = \"./plots/layer_expert_choice_counts\"\n",
    "    path_to_save = os.path.join(path_to_dir, f\"layer_{layer_idx}_expert_choice_counts.png\")\n",
    "    if os.path.exists(path_to_save):\n",
    "        print(f\"Plot for layer {layer_idx} already exists, skipping...\")\n",
    "        continue\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    experts = list(choice_count_dict.keys())\n",
    "    counts = list(choice_count_dict.values())\n",
    "    sns.barplot(x=experts, y=counts)\n",
    "    plt.title(f\"Expert Choice Counts - Layer {layer_idx}\")\n",
    "    plt.xlabel(\"Expert\")\n",
    "    plt.ylabel(\"Count\")\n",
    "\n",
    "    # Save the plot as a png file in ./plots/layer_{layer_idx}_expert_choice_counts.png\n",
    "    os.makedirs(path_to_dir, exist_ok=True)\n",
    "    plt.savefig(path_to_save)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0129fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:38:51.151532Z",
     "start_time": "2025-12-06T17:38:50.880001Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_dir = \"./plots\"\n",
    "path_to_save = os.path.join(path_to_dir, f\"combined_expert_weights_sum_distribution.png\")\n",
    "if os.path.exists(path_to_save):\n",
    "    print(f\"Combined plot already exists, skipping...\")\n",
    "else:\n",
    "  # Now I want to combine layers_expert_weights_sum and plot an histogram of the sum of expert weights per token across all layers\n",
    "  combined_weights_sum = np.concatenate(layers_expert_weights_sum)\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  # make sure the x axis is between 0 and 1\n",
    "  sns.histplot(combined_weights_sum, bins=50, kde=True)\n",
    "  plt.xlim(0, 1)\n",
    "  plt.title(\"Distribution of Sum of Expert Weights per Token - All Layers\")\n",
    "  plt.xlabel(\"Sum of Expert Weights\")\n",
    "  plt.ylabel(\"Frequency\")\n",
    "  plt.grid()\n",
    "  # Save the plot as a png file in ./plots/combined_expert_weights_sum_distribution.png\n",
    "  os.makedirs(path_to_dir, exist_ok=True)\n",
    "  plt.savefig(path_to_save)\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26982721",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:39:00.525488Z",
     "start_time": "2025-12-06T17:38:51.150437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot an histogram of the overall_router_logits per layer\n",
    "for layer_idx, router_logits in enumerate(overall_router_logits):\n",
    "    path_to_dir = \"./plots/layer_router_softmax\"\n",
    "    path_to_save = os.path.join(path_to_dir, f\"layer_{layer_idx}_router_softmax_distribution.png\")\n",
    "    if os.path.exists(path_to_save):\n",
    "        print(f\"Plot for layer {layer_idx} already exists, skipping...\")\n",
    "        continue\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(router_logits, bins=100, kde=True)\n",
    "    plt.title(f\"Distribution of Router Softmax Outputs per Layer - Layer {layer_idx}\")\n",
    "    plt.xlabel(\"Router Softmax Output\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    # Save the plot as a png file in ./plots/layer_{layer_idx}_router_softmax_distribution.png\n",
    "    os.makedirs(path_to_dir, exist_ok=True)\n",
    "    plt.savefig(path_to_save)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b55fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T17:44:26.985539Z",
     "start_time": "2025-12-06T17:39:00.520604Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot an histogram of the softmax_router_logits_per_expert per layer per expert\n",
    "num_experts = softmax_router_logits_per_expert.shape[1]\n",
    "for layer_idx in range(softmax_router_logits_per_expert.shape[0]):\n",
    "    for expert_idx in range(num_experts):\n",
    "        path_to_dir = \"./plots/layer_expert_router_softmax_distribution\"\n",
    "        path_to_save = os.path.join(path_to_dir, f\"layer_{layer_idx}_expert_{expert_idx}_router_softmax_distribution.png\")\n",
    "        if os.path.exists(path_to_save):\n",
    "            print(f\"Plot for layer {layer_idx} expert {expert_idx} already exists, skipping...\")\n",
    "            continue\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(softmax_router_logits_per_expert[layer_idx, expert_idx, :], bins=100, kde=True)\n",
    "        plt.title(f\"Distribution of Router Softmax Outputs - Layer {layer_idx} Expert {expert_idx}\")\n",
    "        plt.xlabel(\"Router Softmax Output\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        # Save the plot as a png file in ./plots/layer_expert_router_softmax_distribution/layer_{layer_idx}_expert_{expert_idx}_router_softmax_distribution.png\n",
    "        os.makedirs(path_to_dir, exist_ok=True)\n",
    "        plt.savefig(path_to_save)\n",
    "        \n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f17b91f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-06T17:44:26.990683Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "num_experts = raw_router_logits_per_expert.shape[1]\n",
    "            \n",
    "# plot an histogram of the raw_router_logits_per_expert per layer per expert\n",
    "for layer_idx in range(raw_router_logits_per_expert.shape[0]):\n",
    "    for expert_idx in range(num_experts):\n",
    "        path_to_dir = \"./plots/layer_expert_raw_router_logits_distribution\"\n",
    "        path_to_save = os.path.join(path_to_dir, f\"layer_{layer_idx}_expert_{expert_idx}_raw_router_logits_distribution.png\")\n",
    "        if os.path.exists(path_to_save):\n",
    "            print(f\"Plot for layer {layer_idx} expert {expert_idx} already exists, skipping...\")\n",
    "            continue\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(raw_router_logits_per_expert[layer_idx, expert_idx, :], bins=100, kde=True)\n",
    "        plt.title(f\"Distribution of Raw Router Logits - Layer {layer_idx} Expert {expert_idx}\")\n",
    "        plt.xlabel(\"Raw Router Logit\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        # Save the plot as a png file in ./plots/layer_expert_raw_router_logits_distribution/layer_{layer_idx}_expert_{expert_idx}_raw_router_logits_distribution.png\n",
    "        os.makedirs(path_to_dir, exist_ok=True)\n",
    "        plt.savefig(path_to_save)\n",
    "        \n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc96b90",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Turn layers_router_logits_raw into a numpy array\n",
    "layers_router_logits_raw = np.array([np.array(arr) for arr in layers_router_logits_raw])\n",
    "\n",
    "def trim_top_and_bottom_experts(arr, trim_amount=2):\n",
    "    if trim_amount == 0:\n",
    "        return arr, None\n",
    "    sorted_indices = np.argsort(arr, axis=-1)\n",
    "    bottom_indices = sorted_indices[:, :, :trim_amount]\n",
    "    top_indices = sorted_indices[:, :, -trim_amount:]\n",
    "    trimmed_indices = np.concatenate([bottom_indices, top_indices], axis=-1)\n",
    "    kept_indices = sorted_indices[:, :, trim_amount:-trim_amount]\n",
    "    trimmed_arr = np.take_along_axis(arr, kept_indices, axis=-1)\n",
    "    return np.array(trimmed_arr), np.array(trimmed_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd1dce",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def plot_trimmed_expert_logits_distribution(arr, trim_amount=2, save_plots=True, test=False):\n",
    "    trimmed_arr, _ = trim_top_and_bottom_experts(arr, trim_amount)\n",
    "    # flatten the last dimension (from (num_layers, num_tokens, num_experts) to (num_layers, num_experts * num_tokens))\n",
    "    trimmed_arr = np.array(trimmed_arr)\n",
    "    flattened_arr = trimmed_arr.reshape(trimmed_arr.shape[0], -1)\n",
    "    if test:\n",
    "        print(f\"trimmed_arr shape: {trimmed_arr.shape}\")\n",
    "        print(f\"flattened_arr shape: {flattened_arr.shape}\")\n",
    "    path_to_dir = \"./plots/trimmed_layer_router_logits\"\n",
    "    for layer_idx in range(trimmed_arr.shape[0]):\n",
    "        path_to_save = os.path.join(path_to_dir, f\"layer_{layer_idx}_trimmed_{trim_amount * 2}_experts_router_logits_distribution.png\")\n",
    "        if save_plots and os.path.exists(path_to_save):\n",
    "            print(f\"Plot for layer {layer_idx} already exists, skipping...\")\n",
    "            continue\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(flattened_arr[layer_idx], bins=100, kde=True)\n",
    "        plt.title(f\"Distribution of Trimmed Raw Router Logits - Layer {layer_idx} (trimmed top {trim_amount} and bottom {trim_amount} experts)\")\n",
    "        plt.xlabel(\"Trimmed Raw Router Logit\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        if save_plots:\n",
    "            os.makedirs(path_to_dir, exist_ok=True)\n",
    "            plt.savefig(path_to_save)\n",
    "            \n",
    "        plt.show()\n",
    "        if test:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae654f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "amount_to_trim = [1, 2, 4]\n",
    "for amt in amount_to_trim:\n",
    "    plot_trimmed_expert_logits_distribution(layers_router_logits_raw, trim_amount=amt, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a55ce4",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/aliab/Downloads/8experts_baseline_hellaswag_internal_routing.json\", \"r\") as f:\n",
    "    \n",
    "    data_hellswag = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7bc629",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# num_layers = len(data['samples'][0]['layers'])\n",
    "# layers_expert_weights = [[] for _ in range(num_layers)]\n",
    "# layers_expert_choices = [[] for _ in range(num_layers)]\n",
    "# layers_router_logits = [[] for _ in range(num_layers)]\n",
    "# layers_router_logits_raw = [[] for _ in range(num_layers)]\n",
    "# for sample in data['samples']:\n",
    "#     for layer in sample['layers']:\n",
    "#         layer_idx = layer['layer']\n",
    "#         layers_expert_weights[layer_idx].extend(layer['expert_weights'])\n",
    "#         layers_expert_choices[layer_idx].extend(layer['selected_experts'])\n",
    "#         layer_router_logits = layer['router_logits_sample']\n",
    "#         softmax_router_logits = np.exp(layer_router_logits) / np.sum(np.exp(layer_router_logits), axis=1, keepdims=True)\n",
    "#         layers_router_logits[layer_idx].extend(softmax_router_logits)\n",
    "#         layers_router_logits_raw[layer_idx].extend(layer_router_logits)\n",
    "\n",
    "num_layers_hellswag = len(data_hellswag['samples'][0]['layers'])\n",
    "layers_router_logits_raw_hellswag = [[] for _ in range(num_layers_hellswag)]\n",
    "for sample in data_hellswag['samples']:\n",
    "    for layer in sample['layers']:\n",
    "        layer_idx = layer['layer']\n",
    "        layer_router_logits = layer['router_logits_sample']\n",
    "        layers_router_logits_raw_hellswag[layer_idx].extend(layer_router_logits)\n",
    "\n",
    "layers_router_logits_raw_hellswag = np.array(layers_router_logits_raw_hellswag)\n",
    "print(f\"layers_router_logits_raw_hellswag shape: {layers_router_logits_raw_hellswag.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53c543",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def plot_trimmed_expert_logits_multiple_distribution(arr1, arr2, trim_amount=2, save_plots=True, test=False, show=True):\n",
    "    path_to_dir = \"./plots/trimmed_layer_router_logits_comparison\"\n",
    "    trimmed_arr1, _ = trim_top_and_bottom_experts(arr1, trim_amount)\n",
    "    trimmed_arr2, _ = trim_top_and_bottom_experts(arr2, trim_amount)\n",
    "    # flatten the last dimension (from (num_layers, num_tokens, num_experts) to (num_layers, num_experts * num_tokens))\n",
    "    flattened_arr1 = trimmed_arr1.reshape(trimmed_arr1.shape[0], -1)\n",
    "    flattened_arr2 = trimmed_arr2.reshape(trimmed_arr2.shape[0], -1)\n",
    "    if test:\n",
    "      print(f\"trimmed_arr1 shape: {trimmed_arr1.shape}\")\n",
    "      print(f\"flattened_arr1 shape: {flattened_arr1.shape}\")\n",
    "      print(f\"trimmed_arr2 shape: {trimmed_arr2.shape}\")\n",
    "      print(f\"flattened_arr2 shape: {flattened_arr2.shape}\")\n",
    "    for layer_idx in range(trimmed_arr1.shape[0]):\n",
    "        path_to_save = os.path.join(path_to_dir, f\"layer_{layer_idx}_trimmed_{trim_amount * 2}_experts_router_logits_distribution_comparison.png\")\n",
    "        if save_plots and os.path.exists(path_to_save):\n",
    "            print(f\"Plot for layer {layer_idx} already exists, skipping...\")\n",
    "            continue\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(flattened_arr1[layer_idx], bins=100, kde=True, color='blue', label='lambada')\n",
    "        sns.histplot(flattened_arr2[layer_idx], bins=100, kde=True, color='orange', label='hellaswag')\n",
    "        plt.title(f\"Distribution of Trimmed Raw Router Logits - Layer {layer_idx} (trimmed top {trim_amount} and bottom {trim_amount} experts)\")\n",
    "        plt.xlabel(\"Trimmed Raw Router Logit\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.legend()\n",
    "        if save_plots:\n",
    "            os.makedirs(path_to_dir, exist_ok=True)\n",
    "            plt.savefig(path_to_save)\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "        else: \n",
    "            plt.close()\n",
    "        if test:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f8510",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "amount_to_trim = [1, 2, 4]\n",
    "for amt in amount_to_trim:\n",
    "  plot_trimmed_expert_logits_multiple_distribution(layers_router_logits_raw, layers_router_logits_raw_hellswag, trim_amount=amt, test=False, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab268ea3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def plot_trimmed_expert_logits_multiple_distribution_different_trim_amount(arr1, arr2, trim_amount=[0, 1, 2, 4], save_plots=True, test=False, show=True):\n",
    "    path_to_dir = \"./plots/trimmed_layer_router_logits_comparison_multiple\"\n",
    "    # Plot multiple distributions with different trim amounts, 2 columns and len(trim_amount)/2 rows\n",
    "    for layer_idx in range(arr1.shape[0]):\n",
    "        plt.figure(figsize=(15, 5 * (len(trim_amount) // 2)))\n",
    "        path_to_save = os.path.join(path_to_dir, f\"layer_{layer_idx}_trimmed_multiple_experts_router_logits_distribution_comparison.png\")\n",
    "        if save_plots and os.path.exists(path_to_save):\n",
    "            print(f\"Plot for layer {layer_idx} already exists, skipping...\")\n",
    "            continue\n",
    "        for i, amt in enumerate(trim_amount):\n",
    "            trimmed_arr1, _ = trim_top_and_bottom_experts(arr1, amt)\n",
    "            trimmed_arr2, _ = trim_top_and_bottom_experts(arr2, amt)\n",
    "            # flatten the last dimension (from (num_layers, num_tokens, num_experts) to (num_layers, num_experts * num_tokens))\n",
    "            flattened_arr1 = trimmed_arr1.reshape(trimmed_arr1.shape[0], -1)\n",
    "            flattened_arr2 = trimmed_arr2.reshape(trimmed_arr2.shape[0], -1)\n",
    "            if test:\n",
    "                print(f\"trimmed_arr1 shape: {trimmed_arr1.shape}\")\n",
    "                print(f\"flattened_arr1 shape: {flattened_arr1.shape}\")\n",
    "                print(f\"trimmed_arr2 shape: {trimmed_arr2.shape}\")\n",
    "                print(f\"flattened_arr2 shape: {flattened_arr2.shape}\")\n",
    "            plt.subplot(len(trim_amount) // 2, 2, i + 1)\n",
    "            sns.histplot(flattened_arr1[layer_idx], bins=100, kde=True, color='blue', label='lambada')\n",
    "            sns.histplot(flattened_arr2[layer_idx], bins=100, kde=True, color='orange', label='hellaswag')\n",
    "            title_text = f\"Distribution of Trimmed Raw Router Logits - Layer {layer_idx} (trimmed top {amt} and bottom {amt} experts out of {arr1.shape[2]}, lambada has {flattened_arr1.shape[1]} samples vs. hellswag has {flattened_arr2.shape[1]} samples )\"\n",
    "            plt.title(\"\\n\".join(textwrap.wrap(title_text, width=70)))\n",
    "            # plt.title(f\"Distribution of Trimmed Raw Router Logits - Layer {layer_idx} (trimmed top {amt} and bottom {amt} experts out of {arr1.shape[2]})\")\n",
    "            plt.xlabel(\"Trimmed Raw Router Logit\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            os.makedirs(path_to_dir, exist_ok=True)\n",
    "            plt.savefig(path_to_save)\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else: \n",
    "            plt.close()\n",
    "        if test:\n",
    "            return\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef152819",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plot_trimmed_expert_logits_multiple_distribution_different_trim_amount(layers_router_logits_raw, layers_router_logits_raw_hellswag, trim_amount=[0, 1, 2, 4], test=False, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151de3d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def plot_trimmed_expert_logits_multiple_distribution_right_tail(arr1, arr2, tail_threshold=[0, 0.5, 1], save_plots=True, test=False, show=True):\n",
    "    path_to_dir = \"./plots/right_tail_layer_router_logits_comparison_multiple\"\n",
    "    rows = int(np.ceil(len(tail_threshold) / 2))\n",
    "    # Plot multiple distributions with different tail thresholds, 2 columns and len(tail_threshold)/2 rows\n",
    "    for layer_idx in range(arr1.shape[0]):\n",
    "        path_to_save = os.path.join(path_to_dir, f\"layer_{layer_idx}_right_tail_multiple_experts_router_logits_distribution_comparison.png\")\n",
    "        if save_plots and os.path.exists(path_to_save):\n",
    "            print(f\"Plot for layer {layer_idx} already exists, skipping...\")\n",
    "            continue\n",
    "        plt.figure(figsize=(15, 5 * rows))\n",
    "        for i, threshold in enumerate(tail_threshold):\n",
    "            layer_arr1 = arr1[layer_idx].copy()\n",
    "            layer_arr2 = arr2[layer_idx].copy()\n",
    "            # trim the experts whose logits are below the threshold actual value\n",
    "            trimmed_arr1 = np.where(layer_arr1 >= threshold, layer_arr1, np.nan)\n",
    "            trimmed_arr2 = np.where(layer_arr2 >= threshold, layer_arr2, np.nan)\n",
    "            # count non-nan values per \n",
    "            arr1_non_nan = np.count_nonzero(~np.isnan(trimmed_arr1))\n",
    "            arr2_non_nan = np.count_nonzero(~np.isnan(trimmed_arr2))\n",
    "            # flatten the last dimension (from (num_tokens, num_experts) to (num_experts * num_tokens))\n",
    "            flattened_arr1 = trimmed_arr1.reshape(-1)\n",
    "            flattened_arr2 = trimmed_arr2.reshape(-1)\n",
    "            if test:\n",
    "                print(f\"trimmed_arr1 shape: {trimmed_arr1.shape}\")\n",
    "                print(f\"flattened_arr1 shape: {flattened_arr1.shape}\")\n",
    "                print(f\"trimmed_arr2 shape: {trimmed_arr2.shape}\")\n",
    "                print(f\"flattened_arr2 shape: {flattened_arr2.shape}\")\n",
    "            plt.subplot(rows, 2, i + 1)\n",
    "            sns.histplot(flattened_arr1, bins=100, kde=True, color='blue', label='lambada')\n",
    "            sns.histplot(flattened_arr2, bins=100, kde=True, color='orange', label='hellaswag')\n",
    "            title_text = f\"Distribution of Right-Tail Raw Router Logits - Layer {layer_idx} (threshold: {threshold}, lambada has {arr1_non_nan} samples vs. hellswag has {arr2_non_nan} samples)\"\n",
    "            plt.title(\"\\n\".join(textwrap.wrap(title_text, width=70)))\n",
    "            plt.xlabel(\"Right-Tail Raw Router Logit\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            os.makedirs(path_to_dir, exist_ok=True)\n",
    "            plt.savefig(path_to_save)\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "        if test:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c02176",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plot_trimmed_expert_logits_multiple_distribution_right_tail(layers_router_logits_raw, layers_router_logits_raw_hellswag, tail_threshold=[0, 0.5, 1], test=False, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b6395",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "trim_amt = [0, 1, 2, 4]\n",
    "for layer_idx in range(layers_router_logits_raw.shape[0]):\n",
    "    path_to_dir = \"./kde_models/models\"\n",
    "    os.makedirs(path_to_dir, exist_ok=True)\n",
    "    kde_model = f\"distribution_model_layer_{layer_idx}.pkl\"\n",
    "    if os.path.exists(os.path.join(path_to_dir, kde_model)):\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "            x_grid = model_data['x']\n",
    "            cdf_grid = model_data['cdf']\n",
    "    else:\n",
    "        train_data = layers_router_logits_raw[layer_idx].flatten()[:, np.newaxis]\n",
    "        \n",
    "\n",
    "        kde = gaussian_kde(train_data.T)\n",
    "\n",
    "        train_data_min, train_data_max = train_data.min(), train_data.max()\n",
    "        x_grid = np.linspace(train_data_min - 0.2 * abs(train_data_min), train_data_max + 0.2 * abs(train_data_max), 10000)\n",
    "\n",
    "        pdf_grid = kde.evaluate(x_grid)\n",
    "        cdf_grid = np.cumsum(pdf_grid)\n",
    "        cdf_grid /= cdf_grid[-1]  # Normalize to [0, 1]\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"wb\") as f:\n",
    "            pickle.dump({'x': x_grid, 'cdf': cdf_grid}, f)\n",
    "\n",
    "    test_layer_data = layers_router_logits_raw_hellswag[layer_idx].flatten()\n",
    "    probabilities = np.interp(test_layer_data, x_grid, cdf_grid)\n",
    "    path_to_plot_dir = \"./kde_models/plots\"\n",
    "    os.makedirs(path_to_plot_dir, exist_ok=True)\n",
    "    path_to_save = os.path.join(path_to_plot_dir, f\"kde_cumulative_density_layer_{layer_idx}_hellswag.png\")\n",
    "    if os.path.exists(path_to_save):\n",
    "        print(f\"Plot for layer {layer_idx} already exists, skipping...\")\n",
    "        continue\n",
    "    # plot a histogram of the cumulative density between 0 and 1\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(probabilities, bins=100)\n",
    "    plt.title(f\"KDE Estimated Cumulative Density (scipy gaussian_kde) - Layer {layer_idx} on hellswag Data\")\n",
    "    plt.xlabel(\"Estimated Cumulative Density\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.grid()\n",
    "    plt.savefig(path_to_save)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1787d03",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "trim_amt = [0, 1, 2, 4]\n",
    "# For each layer, for each trim amount, fit a KDE model on the trimmed data, then evaluate on the wikitext data and plot all trim amounts in a single figure with subplots\n",
    "# We also plot another plot, which is the p-value histogram of the wikitext data under the trimmed KDE model. If the first plot is P(X <= x)\n",
    "# then the p-value plot is P(X > x) = 1 - P(X <= x)\n",
    "for layer_idx in range(layers_router_logits_raw.shape[0]):\n",
    "    path_to_plot_dir = \"./kde_models/plots\"\n",
    "    os.makedirs(path_to_plot_dir, exist_ok=True)\n",
    "    path_to_save = os.path.join(path_to_plot_dir, f\"kde_cumulative_density_layer_{layer_idx}_hellswag_multiple_trimmed.png\")\n",
    "    path_to_save_pvalue = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_multiple_trimmed.png\")\n",
    "    if os.path.exists(path_to_save) and os.path.exists(path_to_save_pvalue):\n",
    "        print(f\"Plot for layer {layer_idx} already exists, skipping...\")\n",
    "        continue\n",
    "    path_to_dir = \"./kde_models/models\"\n",
    "    os.makedirs(path_to_dir, exist_ok=True)\n",
    "    test_points_probabilities = [None] * len(trim_amt)\n",
    "    for idx, amt in enumerate(trim_amt):\n",
    "        kde_model = f\"distribution_model_layer_{layer_idx}_trimmed_{amt*2}.pkl\"\n",
    "        if os.path.exists(os.path.join(path_to_dir, kde_model)):\n",
    "            with open(os.path.join(path_to_dir, kde_model), \"rb\") as f:\n",
    "                model_data = pickle.load(f)\n",
    "                x_grid = model_data['x']\n",
    "                cdf_grid = model_data['cdf']\n",
    "        else:\n",
    "            trimmed_data, _ = trim_top_and_bottom_experts(layers_router_logits_raw[layer_idx][np.newaxis, :, :], amt)\n",
    "            train_data = trimmed_data.flatten()[:, np.newaxis]\n",
    "            \n",
    "            kde = gaussian_kde(train_data.T)\n",
    "            train_data_min, train_data_max = train_data.min(), train_data.max()\n",
    "            x_grid = np.linspace(train_data_min - 0.2 * abs(train_data_min), train_data_max + 0.2 * abs(train_data_max), 10000)\n",
    "            pdf_grid = kde.evaluate(x_grid)\n",
    "            cdf_grid = np.cumsum(pdf_grid)\n",
    "            cdf_grid /= cdf_grid[-1]  # Normalize to [0, 1]\n",
    "            with open(os.path.join(path_to_dir, kde_model), \"wb\") as f:\n",
    "                pickle.dump({'x': x_grid, 'cdf': cdf_grid}, f)\n",
    "        \n",
    "\n",
    "        test_layer_data = layers_router_logits_raw_hellswag[layer_idx].flatten()\n",
    "        test_points_probabilities[idx] = np.interp(test_layer_data, x_grid, cdf_grid)\n",
    "    \n",
    "    # For each trim amount, plot the histogram in a single figure with subplots\n",
    "    rows = int(np.ceil(len(trim_amt) / 2))\n",
    "    if not os.path.exists(path_to_save):\n",
    "        plt.figure(figsize=(15, 5 * rows))\n",
    "        for idx, amt in enumerate(trim_amt):\n",
    "            plt.subplot(rows, 2, idx + 1)\n",
    "            sns.histplot(test_points_probabilities[idx], bins=100)\n",
    "            # Make the title wrap if too long\n",
    "            title_text = f\"KDE Estimated Cumulative Density (scipy gaussian_kde) - Layer {layer_idx} on hellswag Data (trimmed top {amt} and bottom {amt} experts, overall {test_points_probabilities[idx].shape[0]} samples)\"\n",
    "            plt.title(\"\\n\".join(textwrap.wrap(title_text, width=70)))\n",
    "            plt.xlabel(\"Estimated Cumulative Density\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.xlim(0, 1)\n",
    "            plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path_to_save)\n",
    "        plt.show()\n",
    "    # Also plot the p-value histogram\n",
    "    if not os.path.exists(path_to_save_pvalue):\n",
    "        plt.figure(figsize=(15, 5 * rows))\n",
    "        for idx, amt in enumerate(trim_amt):\n",
    "            plt.subplot(rows, 2, idx + 1)\n",
    "            p_values = 1 - test_points_probabilities[idx]\n",
    "            sns.histplot(p_values, bins=100)\n",
    "            # Make the title wrap if too long\n",
    "            title_text = f\"KDE Estimated P-Value (scipy gaussian_kde) - Layer {layer_idx} on hellswag Data (trimmed top {amt} and bottom {amt} experts, overall {test_points_probabilities[idx].shape[0]} samples)\"\n",
    "            plt.title(\"\\n\".join(textwrap.wrap(title_text, width=70)))\n",
    "            plt.xlabel(\"Estimated P-Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.xlim(0, 1)\n",
    "            plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path_to_save_pvalue)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d00da",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# For each layer, load the KDE model without trimming and plot the p-value of the k-th best expert logits on the wikitext data and the histogram of each expert's p-values\n",
    "for layer_idx in range(layers_router_logits_raw.shape[0]):\n",
    "    path_to_dir = \"./kde_models/models\"\n",
    "    kde_model = f\"distribution_model_layer_{layer_idx}.pkl\"\n",
    "    if os.path.exists(os.path.join(path_to_dir, kde_model)):\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "            x_grid = model_data['x']\n",
    "            cdf_grid = model_data['cdf']\n",
    "    else:\n",
    "        train_data = layers_router_logits_raw[layer_idx].flatten()[:, np.newaxis]\n",
    "        \n",
    "\n",
    "        kde = gaussian_kde(train_data.T)\n",
    "\n",
    "        train_data_min, train_data_max = train_data.min(), train_data.max()\n",
    "        x_grid = np.linspace(train_data_min - 0.2 * abs(train_data_min), train_data_max + 0.2 * abs(train_data_max), 10000)\n",
    "\n",
    "        pdf_grid = kde.evaluate(x_grid)\n",
    "        cdf_grid = np.cumsum(pdf_grid)\n",
    "        cdf_grid /= cdf_grid[-1]  # Normalize to [0, 1]\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"wb\") as f:\n",
    "            pickle.dump({'x': x_grid, 'cdf': cdf_grid}, f)\n",
    "    test_layer_data = layers_router_logits_raw_hellswag[layer_idx]\n",
    "    # sort each row and get the k-th best expert logits, where k ranges from 0 to test_layer_data.shape[1]-1\n",
    "    for k in range(test_layer_data.shape[1]):\n",
    "        kth_best_expert_logits = np.sort(test_layer_data, axis=1)[:, -1 - k]\n",
    "        probabilities = np.interp(kth_best_expert_logits, x_grid, cdf_grid)\n",
    "        kth_expert_logits = test_layer_data[:, k]\n",
    "        probabilities_expert_k = np.interp(kth_expert_logits, x_grid, cdf_grid)\n",
    "        # p-value is 1 - cumulative density\n",
    "        p_values = 1 - probabilities\n",
    "        p_values_expert_k = 1 - probabilities_expert_k\n",
    "        path_to_plot_dir = f\"./kde_models/plots/pvalue_per_best_expert/layer_{layer_idx}\"\n",
    "        os.makedirs(path_to_plot_dir, exist_ok=True)\n",
    "        if k % 10 == 0 and k // 10 != 1:\n",
    "            path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_{k+1}st_best_expert.png\")\n",
    "        elif k % 10 == 1 and k // 10 != 1:\n",
    "            path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_{k+1}nd_best_expert.png\")\n",
    "        elif k % 10 == 2 and k // 10 != 1:\n",
    "            path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_{k+1}rd_best_expert.png\")\n",
    "        else:\n",
    "            path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_{k+1}th_best_expert.png\")\n",
    "        path_to_save_expert_k = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_expert_{k+1}_logits.png\")\n",
    "        if not os.path.exists(path_to_save):\n",
    "            # plot a histogram of the p-values between 0 and 1\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(p_values, bins=50)\n",
    "            title_text = f\"KDE Estimated P-Value - Layer {layer_idx} on hellswag (\"\n",
    "            if k % 10 == 0 and k // 10 != 1:\n",
    "                title_text += f\"{k+1}st \"\n",
    "            elif k % 10 == 1 and k // 10 != 1:\n",
    "                title_text += f\" {k+1}nd \"\n",
    "            elif k % 10 == 2 and k // 10 != 1:\n",
    "                title_text += f\"{k+1}rd \"\n",
    "            else:\n",
    "                title_text += f\"{k+1}th \"\n",
    "            title_text += f\"Best Expert out of {test_layer_data.shape[1]}, overall {test_layer_data.shape[0]} samples)\"\n",
    "            plt.title(textwrap.fill(title_text, width=70))\n",
    "            plt.xlabel(\"Estimated P-Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.xlim(0, 1)\n",
    "            plt.grid()\n",
    "            plt.savefig(path_to_save)\n",
    "            if k < 5:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "        if not os.path.exists(path_to_save_expert_k):\n",
    "            # plot a histogram of the p-values between 0 and 1 for expert k logits\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(p_values_expert_k, bins=50)\n",
    "            title_text = f\"KDE Estimated P-Value - Layer {layer_idx} on hellswag (Expert {k+1} Logits out of {test_layer_data.shape[1]}, overall {test_layer_data.shape[0]} samples)\"\n",
    "            plt.title(textwrap.fill(title_text, width=70))\n",
    "            plt.xlabel(\"Estimated P-Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.xlim(0, 1)\n",
    "            plt.grid()\n",
    "            plt.savefig(path_to_save_expert_k)\n",
    "            if k < 5:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd19cb4",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# For each layer, load the KDE model without trimming and plot the p-value of the k-th best expert logits on the wikitext data and the histogram of each expert's p-values\n",
    "for layer_idx in range(layers_router_logits_raw.shape[0]):\n",
    "    path_to_dir = \"./kde_models/models\"\n",
    "    kde_model = f\"distribution_model_layer_{layer_idx}.pkl\"\n",
    "    if os.path.exists(os.path.join(path_to_dir, kde_model)):\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "            x_grid = model_data['x']\n",
    "            cdf_grid = model_data['cdf']\n",
    "    else:\n",
    "        train_data = layers_router_logits_raw[layer_idx].flatten()[:, np.newaxis]\n",
    "        \n",
    "\n",
    "        kde = gaussian_kde(train_data.T)\n",
    "\n",
    "        train_data_min, train_data_max = train_data.min(), train_data.max()\n",
    "        x_grid = np.linspace(train_data_min - 0.2 * abs(train_data_min), train_data_max + 0.2 * abs(train_data_max), 10000)\n",
    "\n",
    "        pdf_grid = kde.evaluate(x_grid)\n",
    "        cdf_grid = np.cumsum(pdf_grid)\n",
    "        cdf_grid /= cdf_grid[-1]  # Normalize to [0, 1]\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"wb\") as f:\n",
    "            pickle.dump({'x': x_grid, 'cdf': cdf_grid}, f)\n",
    "    test_layer_data = layers_router_logits_raw_hellswag[layer_idx]\n",
    "    # sort each row and get the k-th best expert logits, where k ranges from 0 to test_layer_data.shape[1]-1\n",
    "    for j in range(0, test_layer_data.shape[1], 4):\n",
    "        path_to_plot_dir = f\"./kde_models/plots/pvalue_per_best_expert_4_experts/layer_{layer_idx}\"\n",
    "        os.makedirs(path_to_plot_dir, exist_ok=True)\n",
    "        path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_{j+1}_to_{min(j+4, test_layer_data.shape[1])}_best_experts.png\")\n",
    "        if os.path.exists(path_to_save):\n",
    "            print(f\"Plot for layer {layer_idx} experts {j} to {min(j+4, test_layer_data.shape[1])} already exists, skipping...\")\n",
    "            continue\n",
    "        # Plot 4 best experts at a time\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for k in range(j, min(j + 4, test_layer_data.shape[1])):\n",
    "            kth_best_expert_logits = np.sort(test_layer_data, axis=1)[:, -1 - k]\n",
    "            probabilities = np.interp(kth_best_expert_logits, x_grid, cdf_grid)\n",
    "            # p-value is 1 - cumulative density\n",
    "            p_values = 1 - probabilities\n",
    "            \n",
    "            # plot a histogram of the p-values between 0 and 1\n",
    "            plt.subplot(2, 2, k - j + 1)\n",
    "            sns.histplot(p_values, bins=50)\n",
    "            title_text = f\"KDE Estimated P-Value - Layer {layer_idx} on hellswag (\"\n",
    "            if k % 10 == 0 and k // 10 != 1:\n",
    "                title_text += f\"{k+1}st \"\n",
    "            elif k % 10 == 1 and k // 10 != 1:\n",
    "                title_text += f\" {k+1}nd \"\n",
    "            elif k % 10 == 2 and k // 10 != 1:\n",
    "                title_text += f\"{k+1}rd \"\n",
    "            else:\n",
    "                title_text += f\"{k+1}th \"\n",
    "            title_text += f\"Best Expert out of {test_layer_data.shape[1]}, overall {test_layer_data.shape[0]} samples)\"\n",
    "            plt.title(textwrap.fill(title_text, width=70))\n",
    "            plt.xlabel(\"Estimated P-Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.xlim(0, 1)\n",
    "            plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path_to_save)\n",
    "        plt.show()\n",
    "\n",
    "# Now plot the same but for each expert's logits\n",
    "for layer_idx in range(layers_router_logits_raw.shape[0]):\n",
    "    path_to_dir = \"./kde_models/models\"\n",
    "    kde_model = f\"distribution_model_layer_{layer_idx}.pkl\"\n",
    "    if os.path.exists(os.path.join(path_to_dir, kde_model)):\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "            x_grid = model_data['x']\n",
    "            cdf_grid = model_data['cdf']\n",
    "    else:\n",
    "        train_data = layers_router_logits_raw[layer_idx].flatten()[:, np.newaxis]\n",
    "        kde = gaussian_kde(train_data.T)\n",
    "        train_data_min, train_data_max = train_data.min(), train_data.max()\n",
    "        x_grid = np.linspace(train_data_min - 0.2 * abs(train_data_min), train_data_max + 0.2 * abs(train_data_max), 10000)\n",
    "        pdf_grid = kde.evaluate(x_grid)\n",
    "        cdf_grid = np.cumsum(pdf_grid)\n",
    "        cdf_grid /= cdf_grid[-1]  # Normalize to [0, 1]\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"wb\") as f:\n",
    "            pickle.dump({'x': x_grid, 'cdf': cdf_grid}, f)\n",
    "    test_layer_data = layers_router_logits_raw_hellswag[layer_idx]\n",
    "    for j in range(0, test_layer_data.shape[1], 4):\n",
    "        path_to_plot_dir = f\"./kde_models/plots/pvalue_per_expert_4_experts/layer_{layer_idx}\"\n",
    "        os.makedirs(path_to_plot_dir, exist_ok=True)\n",
    "        path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_experts_{j+1}_to_{min(j+4, test_layer_data.shape[1])}_logits.png\")\n",
    "        if os.path.exists(path_to_save):\n",
    "            print(f\"Plot for layer {layer_idx} experts {j} to {min(j+4, test_layer_data.shape[1])} already exists, skipping...\")\n",
    "            continue\n",
    "        # Plot 4 experts at a time\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for k in range(j, min(j + 4, test_layer_data.shape[1])):\n",
    "            kth_expert_logits = test_layer_data[:, k]\n",
    "            probabilities_expert_k = np.interp(kth_expert_logits, x_grid, cdf_grid)\n",
    "            p_values_expert_k = 1 - probabilities_expert_k\n",
    "            # plot a histogram of the p-values between 0 and 1 for expert k logits\n",
    "            plt.subplot(2, 2, k - j + 1)\n",
    "            sns.histplot(p_values_expert_k, bins=50)\n",
    "            title_text = f\"KDE Estimated P-Value - Layer {layer_idx} on hellswag (Expert {k+1} Logits out of {test_layer_data.shape[1]}, overall {test_layer_data.shape[0]} samples)\"\n",
    "            plt.title(textwrap.fill(title_text, width=70))\n",
    "            plt.xlabel(\"Estimated P-Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.xlim(0, 1)\n",
    "            plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path_to_save)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d99046",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# For each layer, load the KDE model without trimming and plot the p-value of the k-th best expert logits on the wikitext data and the histogram of each expert's p-values\n",
    "# and add a histogram of uniform distribution for comparison\n",
    "for layer_idx in range(layers_router_logits_raw.shape[0]):\n",
    "    path_to_dir = \"./kde_models/models\"\n",
    "    kde_model = f\"distribution_model_layer_{layer_idx}.pkl\"\n",
    "    if os.path.exists(os.path.join(path_to_dir, kde_model)):\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "            x_grid = model_data['x']\n",
    "            cdf_grid = model_data['cdf']\n",
    "    else:\n",
    "        train_data = layers_router_logits_raw[layer_idx].flatten()[:, np.newaxis]\n",
    "        \n",
    "\n",
    "        kde = gaussian_kde(train_data.T)\n",
    "\n",
    "        train_data_min, train_data_max = train_data.min(), train_data.max()\n",
    "        x_grid = np.linspace(train_data_min - 0.2 * abs(train_data_min), train_data_max + 0.2 * abs(train_data_max), 10000)\n",
    "\n",
    "        pdf_grid = kde.evaluate(x_grid)\n",
    "        cdf_grid = np.cumsum(pdf_grid)\n",
    "        cdf_grid /= cdf_grid[-1]  # Normalize to [0, 1]\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"wb\") as f:\n",
    "            pickle.dump({'x': x_grid, 'cdf': cdf_grid}, f)\n",
    "    test_layer_data = layers_router_logits_raw_hellswag[layer_idx]\n",
    "    # sort each row and get the k-th best expert logits, where k ranges from 0 to test_layer_data.shape[1]-1\n",
    "    for k in range(test_layer_data.shape[1]):\n",
    "        kth_best_expert_logits = np.sort(test_layer_data, axis=1)[:, -1 - k]\n",
    "        probabilities = np.interp(kth_best_expert_logits, x_grid, cdf_grid)\n",
    "        kth_expert_logits = test_layer_data[:, k]\n",
    "        probabilities_expert_k = np.interp(kth_expert_logits, x_grid, cdf_grid)\n",
    "        # p-value is 1 - cumulative density\n",
    "        p_values = 1 - probabilities\n",
    "        p_values_expert_k = 1 - probabilities_expert_k\n",
    "        path_to_plot_dir = f\"./kde_models/plots_with_uniform/pvalue_per_best_expert/layer_{layer_idx}\"\n",
    "        os.makedirs(path_to_plot_dir, exist_ok=True)\n",
    "        if k % 10 == 0 and k // 10 != 1:\n",
    "            path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_{k+1}st_best_expert.png\")\n",
    "        elif k % 10 == 1 and k // 10 != 1:\n",
    "            path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_{k+1}nd_best_expert.png\")\n",
    "        elif k % 10 == 2 and k // 10 != 1:\n",
    "            path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_{k+1}rd_best_expert.png\")\n",
    "        else:\n",
    "            path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_{k+1}th_best_expert.png\")\n",
    "        path_to_save_expert_k = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_expert_{k+1}_logits.png\")\n",
    "        if not os.path.exists(path_to_save):\n",
    "            # plot a histogram of the p-values between 0 and 1\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(p_values, bins=50, color='blue', label='KDE Estimated P-Value')\n",
    "            total_samples = p_values.shape[0]\n",
    "            num_bins = 50\n",
    "            data_range = (0, 1)\n",
    "            samples_per_bin = total_samples // num_bins\n",
    "            edges = np.linspace(data_range[0], data_range[1], num_bins + 1)\n",
    "            bin_centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "            unifrom_data = np.repeat(bin_centers, samples_per_bin)\n",
    "            sns.histplot(unifrom_data, bins=edges, color='orange', label='Uniform Distribution', alpha=0.5)\n",
    "            title_text = f\"KDE Estimated P-Value vs. Uniform Distribution - Layer {layer_idx} on hellswag (\"\n",
    "            if k % 10 == 0 and k // 10 != 1:\n",
    "                title_text += f\"{k+1}st \"\n",
    "            elif k % 10 == 1 and k // 10 != 1:\n",
    "                title_text += f\" {k+1}nd \"\n",
    "            elif k % 10 == 2 and k // 10 != 1:\n",
    "                title_text += f\"{k+1}rd \"\n",
    "            else:\n",
    "                title_text += f\"{k+1}th \"\n",
    "            title_text += f\"Best Expert out of {test_layer_data.shape[1]}, overall {test_layer_data.shape[0]} samples)\"\n",
    "            plt.title(textwrap.fill(title_text, width=70))\n",
    "            plt.xlabel(\"Estimated P-Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.xlim(0, 1)\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.savefig(path_to_save)\n",
    "            if k < 5:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f907ef46",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "# Plot the above but with 4 best experts at a time\n",
    "for layer_idx in range(layers_router_logits_raw.shape[0]):\n",
    "    path_to_dir = \"./kde_models/models\"\n",
    "    kde_model = f\"distribution_model_layer_{layer_idx}.pkl\"\n",
    "    if os.path.exists(os.path.join(path_to_dir, kde_model)):\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "            x_grid = model_data['x']\n",
    "            cdf_grid = model_data['cdf']\n",
    "    else:\n",
    "        train_data = layers_router_logits_raw[layer_idx].flatten()[:, np.newaxis]\n",
    "        \n",
    "\n",
    "        kde = gaussian_kde(train_data.T)\n",
    "\n",
    "        train_data_min, train_data_max = train_data.min(), train_data.max()\n",
    "        x_grid = np.linspace(train_data_min - 0.2 * abs(train_data_min), train_data_max + 0.2 * abs(train_data_max), 10000)\n",
    "\n",
    "        pdf_grid = kde.evaluate(x_grid)\n",
    "        cdf_grid = np.cumsum(pdf_grid)\n",
    "        cdf_grid /= cdf_grid[-1]  # Normalize to [0, 1]\n",
    "        with open(os.path.join(path_to_dir, kde_model), \"wb\") as f:\n",
    "            pickle.dump({'x': x_grid, 'cdf': cdf_grid}, f)\n",
    "    test_layer_data = layers_router_logits_raw_hellswag[layer_idx]\n",
    "    # create a sample of shape \n",
    "    uniform_samples = rng.uniform(0, 1, size=test_layer_data.shape)\n",
    "    # sort each row and get the k-th best expert logits, where k ranges from 0 to test_layer_data.shape[1]-1\n",
    "    common_bins = np.linspace(0, 1, 101)\n",
    "    for j in range(0, test_layer_data.shape[1], 4):\n",
    "        path_to_plot_dir = f\"./kde_models/plots_with_uniform/pvalue_per_best_expert_4_experts/layer_{layer_idx}\"\n",
    "        os.makedirs(path_to_plot_dir, exist_ok=True)\n",
    "        path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_hellswag_{j+1}_to_{min(j+4, test_layer_data.shape[1])}_best_experts.png\")\n",
    "        if os.path.exists(path_to_save):\n",
    "            print(f\"Plot for layer {layer_idx} experts {j} to {min(j+4, test_layer_data.shape[1])} already exists, skipping...\")\n",
    "            continue\n",
    "        # Plot 4 best experts at a time\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for k in range(j, min(j + 4, test_layer_data.shape[1])):\n",
    "            kth_best_expert_logits = np.sort(test_layer_data, axis=1)[:, -1 - k]\n",
    "            probabilities = np.interp(kth_best_expert_logits, x_grid, cdf_grid)\n",
    "            # p-value is 1 - cumulative density\n",
    "            p_values = 1 - probabilities\n",
    "            # plot a histogram of the p-values between 0 and 1\n",
    "            plt.subplot(2, 2, k - j + 1)\n",
    "            sns.histplot(p_values, bins=common_bins, color='blue', label='KDE Estimated P-Value')\n",
    "            kth_best_sample = np.sort(uniform_samples, axis=1)[:, k]\n",
    "            sns.histplot(kth_best_sample, bins=common_bins, color='orange', label='Uniform Distribution', alpha=0.5)\n",
    "            title_text = f\"KDE Estimated P-Value vs. Uniform Distribution - Layer {layer_idx} on Wikhellswagitext (\"\n",
    "            if k % 10 == 0 and k // 10 != 1:\n",
    "                title_text += f\"{k+1}st \"\n",
    "            elif k % 10 == 1 and k // 10 != 1:\n",
    "                title_text += f\" {k+1}nd \"\n",
    "            elif k % 10 == 2 and k // 10 != 1:\n",
    "                title_text += f\"{k+1}rd \"\n",
    "            else:\n",
    "                title_text += f\"{k+1}th \"\n",
    "            title_text += f\"Best Expert out of {test_layer_data.shape[1]}, overall {test_layer_data.shape[0]} samples)\"\n",
    "            plt.title(textwrap.fill(title_text, width=70))\n",
    "            plt.xlabel(\"Estimated P-Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.xlim(0, 1)\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path_to_save)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebbcde0",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Use layers_expert_choice_counts and plot similar to above, but for the logits for the most chosen expert to least chosen\n",
    "import textwrap\n",
    "num_experts = layers_router_logits_raw.shape[2]\n",
    "for layer_idx in range(layers_router_logits_raw.shape[0]):\n",
    "    path_to_dir = \"./kde_models/models\"\n",
    "    kde_model_path = os.path.join(path_to_dir, f\"distribution_model_layer_{layer_idx}.pkl\")\n",
    "    if not os.path.exists(kde_model_path):\n",
    "        print(f\"KDE model for layer {layer_idx} not found, skipping...\")\n",
    "        continue\n",
    "\n",
    "    with open(kde_model_path, \"rb\") as f:\n",
    "        model_data = pickle.load(f)\n",
    "        x_grid = model_data['x']\n",
    "        cdf_grid = model_data['cdf']\n",
    "\n",
    "    choice_count_dict = layers_expert_choice_counts[layer_idx]\n",
    "    # Create a list of all experts and sort them by choice count, descending.\n",
    "    # Experts not in choice_count_dict were chosen 0 times.\n",
    "    sorted_expert_indices = sorted(range(num_experts), key=lambda k: choice_count_dict.get(k, 0), reverse=True)\n",
    "\n",
    "    test_layer_data = layers_router_logits_raw_hellswag[layer_idx]\n",
    "\n",
    "    for j in range(0, num_experts, 4):\n",
    "        path_to_plot_dir = f\"./kde_models/plots_with_uniform/pvalue_per_chosen_expert_4_experts/layer_{layer_idx}\"\n",
    "        os.makedirs(path_to_plot_dir, exist_ok=True)\n",
    "        path_to_save = os.path.join(path_to_plot_dir, f\"kde_pvalue_layer_{layer_idx}_wikitext_{j+1}_to_{min(j+4, num_experts)}_chosen_experts.png\")\n",
    "\n",
    "        if os.path.exists(path_to_save):\n",
    "            print(f\"Plot for layer {layer_idx} experts {j+1} to {min(j+4, num_experts)} (by choice) already exists, skipping...\")\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for i, rank in enumerate(range(j, min(j + 4, num_experts))):\n",
    "            expert_idx = sorted_expert_indices[rank]\n",
    "            expert_logits = test_layer_data[:, expert_idx]\n",
    "            \n",
    "            probabilities = np.interp(expert_logits, x_grid, cdf_grid)\n",
    "            p_values = 1 - probabilities\n",
    "\n",
    "            plt.subplot(2, 2, i + 1)\n",
    "            sns.histplot(p_values, bins=50, color='blue', label='KDE Estimated P-Value')\n",
    "\n",
    "            # Add uniform distribution for comparison\n",
    "            total_samples = p_values.shape[0]\n",
    "            num_bins = 50\n",
    "            edges = np.linspace(0, 1, num_bins + 1)\n",
    "            samples_per_bin = total_samples // num_bins\n",
    "            bin_centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "            uniform_data = np.repeat(bin_centers, samples_per_bin)\n",
    "            sns.histplot(uniform_data, bins=edges, color='orange', label='Uniform Distribution', alpha=0.5)\n",
    "\n",
    "            title_text = f\"KDE P-Value vs. Uniform - Layer {layer_idx} on hellswag ({rank+1}-th Most Chosen Expert: {expert_idx})\"\n",
    "            plt.title(textwrap.fill(title_text, width=70))\n",
    "            plt.xlabel(\"Estimated P-Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.xlim(0, 1)\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path_to_save)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8075670",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# List of kernels to compare\n",
    "# Options: 'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine'\n",
    "KERNELS = ['gaussian', 'tophat', 'epanechnikov', 'linear'] \n",
    "\n",
    "trim_amt = [0, 1, 2, 4]\n",
    "\n",
    "for layer_idx in range(layers_router_logits_raw.shape[0]):\n",
    "    path_to_dir = \"./kde_models/models\"\n",
    "    os.makedirs(path_to_dir, exist_ok=True)\n",
    "    \n",
    "    # Store results for this layer to plot together later\n",
    "    layer_probabilities = {} \n",
    "\n",
    "    # Prepare training data once per layer\n",
    "    train_data = layers_router_logits_raw[layer_idx].flatten()[:, np.newaxis]\n",
    "    train_data_min, train_data_max = train_data.min(), train_data.max()\n",
    "    \n",
    "    # Define evaluation grid\n",
    "    x_grid = np.linspace(train_data_min - 0.2 * abs(train_data_min), \n",
    "                         train_data_max + 0.2 * abs(train_data_max), 10000)\n",
    "\n",
    "    # 1. Train/Load Models for each Kernel\n",
    "    for kernel_type in KERNELS:\n",
    "        kde_model_filename = f\"distribution_model_layer_{layer_idx}_{kernel_type}.pkl\"\n",
    "        model_path = os.path.join(path_to_dir, kde_model_filename)\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                model_data = pickle.load(f)\n",
    "                # Ensure the grid matches if loading old models (optional safety check)\n",
    "                cdf_grid = model_data['cdf']\n",
    "                # If loaded x_grid is different, interpolation might be slightly off, \n",
    "                # but usually we assume the grid is consistent.\n",
    "                loaded_x_grid = model_data['x'] \n",
    "        else:\n",
    "            # Bandwidth Heuristic (Scott's Rule approximation)\n",
    "            n = train_data.shape[0]\n",
    "            std_dev = np.std(train_data)\n",
    "            bandwidth = 1.06 * std_dev * (n ** (-1/5)) if std_dev > 0 else 1.0\n",
    "            \n",
    "            # Initialize and Fit\n",
    "            kde = KernelDensity(kernel=kernel_type, bandwidth=bandwidth)\n",
    "            kde.fit(train_data)\n",
    "\n",
    "            # Evaluate PDF -> CDF\n",
    "            log_pdf = kde.score_samples(x_grid[:, np.newaxis])\n",
    "            pdf_grid = np.exp(log_pdf)\n",
    "            cdf_grid = np.cumsum(pdf_grid)\n",
    "            cdf_grid /= cdf_grid[-1]  # Normalize\n",
    "\n",
    "            # Save\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                pickle.dump({'x': x_grid, 'cdf': cdf_grid}, f)\n",
    "            \n",
    "            loaded_x_grid = x_grid\n",
    "\n",
    "        # 2. Evaluate on Test Data\n",
    "        test_layer_data = layers_router_logits_raw_hellswag[layer_idx].flatten()\n",
    "        \n",
    "        # Interpolate to find where test data falls on the CDF\n",
    "        probs = np.interp(test_layer_data, loaded_x_grid, cdf_grid)\n",
    "        layer_probabilities[kernel_type] = probs\n",
    "\n",
    "    # 3. Plotting - All Kernels on One Graph\n",
    "    path_to_plot_dir = \"./kde_models/plots\"\n",
    "    os.makedirs(path_to_plot_dir, exist_ok=True)\n",
    "    path_to_save = os.path.join(path_to_plot_dir, f\"kde_comparison_layer_{layer_idx}_hellswag.png\")\n",
    "\n",
    "    if os.path.exists(path_to_save):\n",
    "        print(f\"Plot for layer {layer_idx} already exists, skipping...\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Loop through stored probabilities and plot\n",
    "    for kernel_type, probs in layer_probabilities.items():\n",
    "        # 'element=\"step\"' or \"poly\" makes overlapping histograms easier to read than bars\n",
    "        sns.histplot(probs, bins=100, element=\"step\", fill=False, \n",
    "                     label=f'{kernel_type.capitalize()}', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "    plt.title(f\"KDE Cumulative Density Comparison - Layer {layer_idx}\")\n",
    "    plt.xlabel(\"Estimated Cumulative Density\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.legend(title=\"Kernel Type\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.savefig(path_to_save)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88639ed",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Define exactly 4 kernels\n",
    "KERNELS = [\"gaussian\", \"tophat\", \"epanechnikov\", \"linear\"]\n",
    "\n",
    "for layer_idx in range(layers_router_logits_raw.shape[0]):\n",
    "    path_to_dir = \"./kde_models/models\"\n",
    "    os.makedirs(path_to_dir, exist_ok=True)\n",
    "\n",
    "    pvalue_probabilities = {}\n",
    "\n",
    "    # --- Prepare Data ---\n",
    "    train_data_raw = layers_router_logits_raw[layer_idx].flatten()\n",
    "    train_data = train_data_raw[:, np.newaxis]\n",
    "\n",
    "    # Sort for empirical calculation\n",
    "    sorted_train_data = np.sort(train_data_raw)\n",
    "    n_train = len(sorted_train_data)\n",
    "\n",
    "    train_data_min, train_data_max = train_data.min(), train_data.max()\n",
    "    x_grid = np.linspace(\n",
    "        train_data_min - 0.2 * abs(train_data_min),\n",
    "        train_data_max + 0.2 * abs(train_data_max),\n",
    "        10000,\n",
    "    )\n",
    "\n",
    "    # --- 1. Train/Load KDE Models ---\n",
    "    for kernel_type in KERNELS:\n",
    "        kde_model_filename = f\"distribution_model_layer_{layer_idx}_{kernel_type}.pkl\"\n",
    "        model_path = os.path.join(path_to_dir, kde_model_filename)\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                model_data = pickle.load(f)\n",
    "                cdf_grid = model_data[\"cdf\"]\n",
    "                loaded_x_grid = model_data[\"x\"]\n",
    "        else:\n",
    "            # Bandwidth Heuristic\n",
    "            std_dev = np.std(train_data)\n",
    "            bandwidth = 1.06 * std_dev * (n_train ** (-1 / 5)) if std_dev > 0 else 1.0\n",
    "\n",
    "            kde = KernelDensity(kernel=kernel_type, bandwidth=bandwidth)\n",
    "            kde.fit(train_data)\n",
    "\n",
    "            log_pdf = kde.score_samples(x_grid[:, np.newaxis])\n",
    "            pdf_grid = np.exp(log_pdf)\n",
    "            cdf_grid = np.cumsum(pdf_grid)\n",
    "            cdf_grid /= cdf_grid[-1]\n",
    "\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                pickle.dump({\"x\": x_grid, \"cdf\": cdf_grid}, f)\n",
    "\n",
    "            loaded_x_grid = x_grid\n",
    "\n",
    "        # Evaluate KDE on Test Data\n",
    "        test_layer_data = layers_router_logits_raw_wikitext[layer_idx].flatten()\n",
    "        probs = np.interp(test_layer_data, loaded_x_grid, cdf_grid)\n",
    "        pvalue_probabilities[kernel_type] = 1 - probs\n",
    "\n",
    "    # --- 2. Calculate Actual (Empirical) Probabilities ---\n",
    "    test_layer_data = layers_router_logits_raw_wikitext[layer_idx].flatten()\n",
    "    ranks = np.searchsorted(sorted_train_data, test_layer_data, side=\"right\")\n",
    "    empirical_probs = (ranks + 1) / n_train\n",
    "    pvalue_probabilities[\"empirical\"] = 1 - empirical_probs\n",
    "\n",
    "    # --- 3. Plotting ---\n",
    "    path_to_plot_dir = \"./kde_models/plots/kde_dists\"\n",
    "    os.makedirs(path_to_plot_dir, exist_ok=True)\n",
    "    path_to_save = os.path.join(\n",
    "        path_to_plot_dir, f\"kde_subplots_layer_{layer_idx}_wikitext_with_empirical.png\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(path_to_save):\n",
    "        print(f\"Plot for layer {layer_idx} already exists, skipping...\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(20, 22))\n",
    "\n",
    "    plot_keys = KERNELS + [\"empirical\"]\n",
    "\n",
    "    for i, key in enumerate(plot_keys):\n",
    "        plt.subplot(3, 2, i + 1)\n",
    "\n",
    "        probs = pvalue_probabilities[key]\n",
    "        color = \"salmon\" if key == \"empirical\" else \"skyblue\"\n",
    "\n",
    "        # --- Construct Title ---\n",
    "        if key == \"empirical\":\n",
    "            base_title = (\n",
    "                f\"Actual Empirical Probabilities - Layer {layer_idx} on Wikitext Data\"\n",
    "            )\n",
    "        else:\n",
    "            base_title = f\"KDE Estimated P-Value with kernel: {key.capitalize()} - Layer {layer_idx} on Wikitext Data\"\n",
    "\n",
    "        # Combine with sample count\n",
    "        title_text = f\"{base_title} (overall {probs.shape[0]} samples)\"\n",
    "\n",
    "        sns.histplot(probs, bins=100, color=color, edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "        # Apply text wrapping\n",
    "        plt.title(\"\\n\".join(textwrap.wrap(title_text, width=70)), fontsize=14)\n",
    "        plt.xlabel(\"Estimated Cumulative Density (P-Value)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.xlim(0, 1)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f\"KDE vs Empirical Density Analysis - Layer {layer_idx}\", fontsize=20)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(path_to_save)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
