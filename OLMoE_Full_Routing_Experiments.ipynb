{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLMoE Full Routing Experiments - Two-Phase Analysis\n",
    "\n",
    "**Complete framework for analyzing and modifying OLMoE expert routing**\n",
    "\n",
    "This notebook runs on:\n",
    "- ‚úÖ **Google Colab** (Recommended - GPU required)\n",
    "- ‚úÖ Local Jupyter with GPU\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start (Google Colab)\n",
    "\n",
    "1. Upload this notebook to Google Drive\n",
    "2. Open with Google Colab\n",
    "3. Enable GPU: `Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí A100`\n",
    "4. Run all cells\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Two-Phase Experimental Approach\n",
    "\n",
    "### **Phase 1: Baseline Analysis**\n",
    "Understand the model's **natural routing behavior**:\n",
    "- Which experts does it prefer?\n",
    "- How concentrated is the routing?\n",
    "- What's the expert utilization rate?\n",
    "- Save **internal router_logits** for each sample\n",
    "\n",
    "### **Phase 2: Modified Routing**\n",
    "Test **custom routing strategies**:\n",
    "- Uniform routing (equal weights)\n",
    "- Normalized routing (renormalized probabilities)\n",
    "- Save **internal router_logits** for comparison\n",
    "\n",
    "### **Phase 3: Comparative Analysis**\n",
    "Direct comparison:\n",
    "- Does custom routing improve quality?\n",
    "- What's the speed-quality trade-off?\n",
    "- Generate detailed reports and visualizations\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Output Structure\n",
    "\n",
    "```\n",
    "two_phase_experiment/\n",
    "‚îú‚îÄ‚îÄ logs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 8experts_baseline_wikitext.json                    # Summary metrics\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 8experts_baseline_wikitext_internal_routing.json  # FULL router_logits logs\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 8experts_uniform_wikitext.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 8experts_uniform_wikitext_internal_routing.json   # FULL router_logits logs\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ... (one pair per configuration)\n",
    "‚îú‚îÄ‚îÄ visualizations/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ two_phase_comparison.png                           # 6-panel comparison\n",
    "‚îú‚îÄ‚îÄ two_phase_results.csv                                  # All results\n",
    "‚îú‚îÄ‚îÄ two_phase_results.json\n",
    "‚îî‚îÄ‚îÄ two_phase_report.md                                    # Detailed analysis\n",
    "```\n",
    "\n",
    "**Each configuration gets TWO files:**\n",
    "1. `{config}_internal_routing.json` - Full router_logits for all samples/layers\n",
    "2. `{config}.json` - Summary metrics (perplexity, accuracy, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [GPU Configuration](#2-gpu-configuration)\n",
    "3. [Installation](#3-installation)\n",
    "4. [Framework Setup](#4-framework-setup)\n",
    "5. [Run Full Two-Phase Experiment](#5-run-full-two-phase-experiment)\n",
    "6. [Analyze Results](#6-analyze-results)\n",
    "7. [Visualizations](#7-visualizations)\n",
    "8. [View Internal Routing Logs](#8-view-internal-routing-logs)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Set working directory\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    print(\"\\nüìÅ Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    WORK_DIR = '/content/drive/MyDrive/olmoe_full_experiments'\n",
    "    REPO_DIR = '/content/drive/MyDrive/MOE-with-feature-selection'\n",
    "else:\n",
    "    WORK_DIR = './olmoe_full_experiments'\n",
    "    REPO_DIR = None\n",
    "\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(f\"‚úÖ Repository location: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n‚úÖ CUDA Available\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    device = 'cuda'\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"\\n‚ùå GPU not available!\")\n",
    "    print(\"\\n‚ö†Ô∏è  This notebook requires a GPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"   Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4/A100 GPU\")\n",
    "    raise Exception(\"GPU required for this experiment\")\n",
    "\n",
    "print(f\"\\n‚úÖ Device: {device}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q torch transformers datasets pandas numpy matplotlib seaborn tqdm rich\n",
    "echo \"‚úÖ All packages installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  torch: {torch.__version__}\")\n",
    "print(f\"  transformers: {transformers.__version__}\")\n",
    "print(f\"  datasets: {datasets.__version__}\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(\"\\n‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Framework Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FRAMEWORK SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Check if repo exists in Drive\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        print(f\"\\nüìÇ Repository exists in Google Drive\")\n",
    "        print(f\"   Location: {REPO_DIR}\")\n",
    "        print(f\"\\n   Pulling latest changes...\")\n",
    "        !cd {REPO_DIR} && git pull\n",
    "    else:\n",
    "        print(\"\\nüì• Cloning repository to Google Drive...\")\n",
    "        !git clone https://github.com/aliabbasjaffri/MOE-with-feature-selection.git {REPO_DIR}\n",
    "        \n",
    "    framework_dir = REPO_DIR\n",
    "else:\n",
    "    framework_dir = os.path.abspath('.')\n",
    "\n",
    "# Add to Python path\n",
    "if framework_dir not in sys.path:\n",
    "    sys.path.insert(0, framework_dir)\n",
    "    print(f\"\\n‚úÖ Added to path: {framework_dir}\")\n",
    "\n",
    "# Verify framework file\n",
    "framework_file = os.path.join(framework_dir, 'olmoe_routing_experiments.py')\n",
    "if os.path.exists(framework_file):\n",
    "    file_size = os.path.getsize(framework_file)\n",
    "    print(f\"‚úÖ Found: olmoe_routing_experiments.py ({file_size:,} bytes)\")\n",
    "else:\n",
    "    raise Exception(\"Framework file not found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ FRAMEWORK READY\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import framework\n",
    "if 'olmoe_routing_experiments' in sys.modules:\n",
    "    del sys.modules['olmoe_routing_experiments']\n",
    "\n",
    "from olmoe_routing_experiments import (\n",
    "    RoutingExperimentRunner,\n",
    "    ModelPatchingUtils\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Framework imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Run Full Two-Phase Experiment\n\n### Configuration\n\n**Expert Counts:** [4, 8, 16, 32, 64]  \n**Datasets:** ['wikitext', 'lambada', 'hellaswag']  \n**Samples:** 500 per dataset  \n**Routing Modifications:** ['uniform', 'normalized']  \n\n**Total Experiments:** 5 expert counts √ó 3 datasets √ó 3 strategies (baseline + 2 modifications) = **45 experiments**\n\n**Each experiment generates:**\n- Summary JSON with metrics\n- **Internal routing JSON with router_logits for all samples**\n\n**Estimated Time:** ~90-120 minutes on A100 GPU\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"FULL TWO-PHASE ROUTING EXPERIMENT\")\nprint(\"=\" * 70)\nprint(\"\\nConfiguration:\")\nprint(\"  Expert counts: [4, 8, 16, 32, 64]\")\nprint(\"  Datasets: [wikitext, lambada, hellaswag]\")\nprint(\"  Samples: 500 per dataset\")\nprint(\"  Modifications: [uniform, normalized]\")\nprint(\"\\nFeatures:\")\nprint(\"  ‚úÖ Phase 1: Baseline analysis with internal router_logits\")\nprint(\"  ‚úÖ Phase 2: Modified routing with internal router_logits\")\nprint(\"  ‚úÖ Phase 3: Comparative analysis and visualizations\")\nprint(\"  ‚úÖ Unique log file per configuration with FULL routing data\")\nprint(\"\\nEstimated time: ~90-120 minutes\")\nprint(\"=\" * 70)\n\n# Create runner\nrunner = RoutingExperimentRunner(\n    model_name=\"allenai/OLMoE-1B-7B-0924\",\n    device=device,\n    output_dir=\"./two_phase_full_experiment\"\n)\n\n# Run full two-phase experiment\nresults_df, routing_insights = runner.run_two_phase_experiment(\n    expert_counts=[4, 8, 16, 32, 64],\n    datasets=['wikitext', 'lambada', 'hellaswag'],\n    max_samples=500,\n    routing_modifications=['uniform', 'normalized']\n)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"‚úÖ EXPERIMENT COMPLETE!\")\nprint(\"=\" * 70)\nprint(f\"\\nResults saved to: {runner.output_dir}\")\nprint(f\"\\nGenerated files:\")\nprint(f\"  ‚Ä¢ 45 summary JSON files (logs/*.json)\")\nprint(f\"  ‚Ä¢ 45 internal routing JSON files (logs/*_internal_routing.json)\")\nprint(f\"  ‚Ä¢ Comparison visualizations (visualizations/two_phase_comparison.png)\")\nprint(f\"  ‚Ä¢ Detailed markdown report (two_phase_report.md)\")\nprint(f\"  ‚Ä¢ CSV results (two_phase_results.csv)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Baseline Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: BASELINE ROUTING INSIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display baseline insights\n",
    "insights_data = []\n",
    "for key, insight in routing_insights.items():\n",
    "    insights_data.append({\n",
    "        'Experts': insight['num_experts'],\n",
    "        'Dataset': insight['dataset'],\n",
    "        'Perplexity': f\"{insight['baseline_perplexity']:.2f}\",\n",
    "        'Accuracy': f\"{insight['baseline_accuracy']:.4f}\",\n",
    "        'Experts Used': f\"{insight['unique_experts_used']}/64\",\n",
    "        'Utilization': f\"{insight['expert_utilization']:.1%}\",\n",
    "        'Entropy': f\"{insight['avg_entropy']:.3f}\"\n",
    "    })\n",
    "\n",
    "insights_df = pd.DataFrame(insights_data)\n",
    "print(\"\\n\")\n",
    "print(insights_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nüìä Key Observations:\")\n",
    "avg_util = np.mean([i['expert_utilization'] for i in routing_insights.values()])\n",
    "avg_entropy = np.mean([i['avg_entropy'] for i in routing_insights.values()])\n",
    "\n",
    "print(f\"  ‚Ä¢ Average expert utilization: {avg_util:.1%}\")\n",
    "print(f\"  ‚Ä¢ Average routing entropy: {avg_entropy:.3f}\")\n",
    "\n",
    "if avg_util < 0.5:\n",
    "    print(f\"  ‚ö†Ô∏è  Many experts underutilized!\")\n",
    "    print(f\"      ‚Üí Custom routing may help balance usage\")\n",
    "\n",
    "if avg_entropy < 1.0:\n",
    "    print(f\"  ‚ö†Ô∏è  Routing is highly concentrated!\")\n",
    "    print(f\"      ‚Üí Model heavily favors certain experts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 & 3: Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ROUTING COMPARISON: BASELINE VS MODIFIED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For each configuration, compare baseline vs modified\n",
    "for num_experts in sorted(results_df['num_experts'].unique()):\n",
    "    for dataset in results_df['dataset'].unique():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Configuration: {num_experts} Experts | Dataset: {dataset}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        config_df = results_df[\n",
    "            (results_df['num_experts'] == num_experts) & \n",
    "            (results_df['dataset'] == dataset)\n",
    "        ]\n",
    "        \n",
    "        if config_df.empty:\n",
    "            continue\n",
    "            \n",
    "        # Create comparison table\n",
    "        comparison = []\n",
    "        baseline_row = config_df[config_df['strategy'] == 'baseline'].iloc[0]\n",
    "        \n",
    "        for _, row in config_df.iterrows():\n",
    "            delta_ppl = row['perplexity'] - baseline_row['perplexity']\n",
    "            delta_acc = row['token_accuracy'] - baseline_row['token_accuracy']\n",
    "            \n",
    "            comparison.append({\n",
    "                'Strategy': row['strategy'],\n",
    "                'Perplexity': f\"{row['perplexity']:.2f}\",\n",
    "                'Œî PPL': f\"{delta_ppl:+.2f}\",\n",
    "                'Accuracy': f\"{row['token_accuracy']:.4f}\",\n",
    "                'Œî Acc': f\"{delta_acc:+.4f}\",\n",
    "                'Speed': f\"{row['tokens_per_second']:.1f}\",\n",
    "                'Entropy': f\"{row['avg_entropy']:.3f}\"\n",
    "            })\n",
    "        \n",
    "        comp_df = pd.DataFrame(comparison)\n",
    "        print(\"\\n\" + comp_df.to_string(index=False))\n",
    "        \n",
    "        # Highlight findings\n",
    "        best_modified = config_df[config_df['strategy'] != 'baseline']['perplexity'].min()\n",
    "        if best_modified < baseline_row['perplexity']:\n",
    "            improvement = baseline_row['perplexity'] - best_modified\n",
    "            best_strategy = config_df[config_df['perplexity'] == best_modified]['strategy'].iloc[0]\n",
    "            print(f\"\\n‚úÖ Modified routing improved! Best: {best_strategy} (‚àí{improvement:.2f} PPL)\")\n",
    "        else:\n",
    "            print(f\"\\n‚ÑπÔ∏è  Baseline routing performs best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "viz_path = \"./two_phase_full_experiment/visualizations/two_phase_comparison.png\"\n",
    "print(\"üìä Two-Phase Experiment Visualizations:\\n\")\n",
    "display(Image(filename=viz_path))\n",
    "\n",
    "print(\"\\n‚úÖ Visualization includes:\")\n",
    "print(\"  1. Perplexity comparison (baseline vs modified)\")\n",
    "print(\"  2. Accuracy comparison\")\n",
    "print(\"  3. Delta perplexity (improvement/degradation)\")\n",
    "print(\"  4. Expert utilization (baseline)\")\n",
    "print(\"  5. Routing entropy comparison\")\n",
    "print(\"  6. Speed-quality trade-off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Internal Routing Logs\n",
    "\n",
    "Each configuration has a detailed internal routing log with router_logits for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example: Load internal routing logs for 8 experts baseline\n",
    "log_file = \"./two_phase_full_experiment/logs/8experts_baseline_wikitext_internal_routing.json\"\n",
    "\n",
    "with open(log_file, 'r') as f:\n",
    "    internal_logs = json.load(f)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"INTERNAL ROUTING LOGS: 8 Experts Baseline (WikiText)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nConfiguration: {internal_logs['config']}\")\n",
    "print(f\"Strategy: {internal_logs['strategy']}\")\n",
    "print(f\"Dataset: {internal_logs['dataset']}\")\n",
    "print(f\"\\nTotal samples logged: {len(internal_logs['samples'])}\")\n",
    "\n",
    "# Show summary\n",
    "if 'summary' in internal_logs:\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    for key, value in internal_logs['summary'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Show first sample details\n",
    "if internal_logs['samples']:\n",
    "    sample = internal_logs['samples'][0]\n",
    "    print(f\"\\nFirst Sample Details:\")\n",
    "    print(f\"  Sample ID: {sample['sample_id']}\")\n",
    "    print(f\"  Num tokens: {sample['num_tokens']}\")\n",
    "    print(f\"  Loss: {sample['loss']:.4f}\")\n",
    "    print(f\"  Num layers: {len(sample['layers'])}\")\n",
    "    \n",
    "    if sample['layers']:\n",
    "        layer = sample['layers'][0]\n",
    "        print(f\"\\n  Layer 0:\")\n",
    "        print(f\"    Router logits shape: {layer['router_logits_shape']}\")\n",
    "        print(f\"    Selected experts (first token): {layer['selected_experts'][0][0]}\")\n",
    "        print(f\"    Expert weights (first token): {layer['expert_weights'][0][0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Internal routing logs contain FULL router_logits data!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List All Generated Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "logs_dir = \"./two_phase_full_experiment/logs/\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ALL GENERATED LOG FILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Summary logs\n",
    "summary_logs = sorted(glob.glob(f\"{logs_dir}/*.json\"))\n",
    "summary_logs = [f for f in summary_logs if '_internal_routing' not in f]\n",
    "\n",
    "print(f\"\\nüìÑ Summary Logs ({len(summary_logs)} files):\")\n",
    "for log in summary_logs[:10]:  # Show first 10\n",
    "    print(f\"  ‚Ä¢ {os.path.basename(log)}\")\n",
    "if len(summary_logs) > 10:\n",
    "    print(f\"  ... and {len(summary_logs) - 10} more\")\n",
    "\n",
    "# Internal routing logs\n",
    "internal_logs = sorted(glob.glob(f\"{logs_dir}/*_internal_routing.json\"))\n",
    "\n",
    "print(f\"\\nüîç Internal Routing Logs ({len(internal_logs)} files):\")\n",
    "for log in internal_logs[:10]:  # Show first 10\n",
    "    size_mb = os.path.getsize(log) / (1024 * 1024)\n",
    "    print(f\"  ‚Ä¢ {os.path.basename(log)} ({size_mb:.2f} MB)\")\n",
    "if len(internal_logs) > 10:\n",
    "    print(f\"  ... and {len(internal_logs) - 10} more\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total: {len(summary_logs)} summary + {len(internal_logs)} internal routing logs\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### What We Accomplished\n\n‚úÖ **Full Two-Phase Experiment**: Analyzed baseline + tested 2 routing modifications  \n‚úÖ **Internal Routing Logs**: Saved router_logits for every sample in unique files  \n‚úÖ **Comprehensive Analysis**: 45 experiments across 5 expert counts √ó 3 datasets √ó 3 strategies  \n‚úÖ **Detailed Reports**: Markdown report with tables and recommendations  \n‚úÖ **Rich Visualizations**: 6-panel comparison plots  \n\n### Datasets Evaluated\n\n- **WikiText**: Language modeling benchmark\n- **LAMBADA**: Word prediction in context\n- **HellaSwag**: Commonsense reasoning and sentence completion\n\n### Files Generated\n\n- **90 log files** (45 summary + 45 internal routing)\n- **CSV results** with all metrics\n- **Markdown report** with detailed analysis\n- **PNG/PDF visualizations**\n\n### Next Steps\n\n1. **Analyze internal routing logs** to understand expert selection patterns\n2. **Compare router_logits** between baseline and modified routing\n3. **Identify patterns** in expert utilization\n4. **Design better routing strategies** based on insights\n\n---\n\n**All results persisted in Google Drive!** üéâ"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}