{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2a889",
   "metadata": {},
   "source": [
    "OLMoE_random_routing_WorkFlow.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1LsxZ2chq-bEISwiGQndJGKYGJ7C_nRPI\n",
    "\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8296e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Set working directory\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    print(\"\\nðŸ“ Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    WORK_DIR = '/content/drive/MyDrive/olmoe_random_experiments'\n",
    "    REPO_DIR = '/content/drive/MyDrive/MOE-with-feature-selection'\n",
    "else:\n",
    "    WORK_DIR = './olmoe_random_experiments'\n",
    "    REPO_DIR = None\n",
    "\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"\\nâœ… Working directory: {os.getcwd()}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(f\"âœ… Repository location: {REPO_DIR}\")\n",
    "from pathlib import Path\n",
    "\n",
    "# Output directory for results\n",
    "OUTPUT_DIR = Path(WORK_DIR) / 'results'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"âœ… Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a61ae2",
   "metadata": {},
   "source": [
    "## 2. GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nâœ… CUDA Available\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "    device = 'cuda'\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"\\nâŒ GPU not available!\")\n",
    "    print(\"\\nâš ï¸  This notebook requires a GPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"   Enable GPU: Runtime â†’ Change runtime type â†’ T4/A100 GPU\")\n",
    "    raise Exception(\"GPU required for this experiment\")\n",
    "\n",
    "print(f\"\\nâœ… Device: {device}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b81fb",
   "metadata": {},
   "source": [
    "## 3. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3628518",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# pip install -q torch transformers datasets pandas numpy matplotlib seaborn tqdm scipy\n",
    "# echo \"âœ… All packages installed!\"\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  torch: {torch.__version__}\")\n",
    "print(f\"  transformers: {transformers.__version__}\")\n",
    "print(f\"  datasets: {datasets.__version__}\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(\"\\nâœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c995a8",
   "metadata": {},
   "source": [
    "## 4. Random Routing Module Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RANDOM ROUTING MODULE SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ==========================================================================\n",
    "# CONFIGURATION\n",
    "# ==========================================================================\n",
    "BRANCH = \"random_ben\"  # Change this to switch branches (e.g., \"dev\", \"experiment\", \"feature-x\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Check if repo exists in Drive\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        print(f\"\\nðŸ“‚ Repository exists in Google Drive\")\n",
    "        print(f\"   Location: {REPO_DIR}\")\n",
    "        print(f\"   Branch: {BRANCH}\")\n",
    "        print(f\"\\n   Fetching and checking out branch '{BRANCH}'...\")\n",
    "        !cd {REPO_DIR} && git fetch origin && git checkout {BRANCH} && git pull origin {BRANCH}\n",
    "    else:\n",
    "        print(f\"\\nðŸ“¥ Cloning repository to Google Drive (branch: {BRANCH})...\")\n",
    "        !git clone --branch {BRANCH} https://github.com/AliABULIEL/MOE-with-feature-selection.git {REPO_DIR}\n",
    "\n",
    "    framework_dir = REPO_DIR\n",
    "else:\n",
    "    framework_dir = os.path.abspath('..')\n",
    "\n",
    "# Add to Python path\n",
    "if framework_dir not in sys.path:\n",
    "    sys.path.insert(0, framework_dir)\n",
    "    print(f\"\\nâœ… Added to path: {framework_dir}\")\n",
    "\n",
    "# Verify random_routing.py exists\n",
    "random_routing_file = os.path.join(framework_dir, 'random_routing.py')\n",
    "if os.path.exists(random_routing_file):\n",
    "    file_size = os.path.getsize(random_routing_file)\n",
    "    print(f\"âœ… Found: random_routing.py ({file_size:,} bytes)\")\n",
    "else:\n",
    "    raise Exception(\"random_routing.py not found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… MODULE READY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Import Random routing functions\n",
    "if 'random_routing' in sys.modules:\n",
    "    del sys.modules['random_routing']\n",
    "\n",
    "from random_routing import random_routing\n",
    "\n",
    "print(\"âœ… Random routing module imported successfully!\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RANDOM ROUTING API\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Key Parameters:\n",
    "- experts_amount: Number of experts to randomly select.\n",
    "- sum_of_weights: Target sum for normalized weights.\n",
    "                  If None, uses sum of original top-8 weights.\n",
    "\n",
    "Usage:\n",
    "  random_routing(logits, experts_amount=4, sum_of_weights=0.5)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ade4ce",
   "metadata": {},
   "source": [
    "## 4.5 Import Comprehensive Framework Modules\n",
    "\n",
    "Import the full evaluation framework with metrics, datasets, and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531177fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"IMPORTING COMPREHENSIVE FRAMEWORK MODULES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import importlib\n",
    "\n",
    "# âœ… Import Random metrics\n",
    "try:\n",
    "    if 'random_routing_metrics' in sys.modules:\n",
    "        importlib.reload(sys.modules['random_routing_metrics'])\n",
    "    from random_routing_metrics import RandomMetricsComputer, UnifiedEvaluationMetrics, save_metrics\n",
    "    print(\"âœ… Imported RandomMetricsComputer\")\n",
    "    print(\"âœ… Imported UnifiedEvaluationMetrics, save_metrics\")\n",
    "    metrics_computer = RandomMetricsComputer()\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Could not import metrics: {e}\")\n",
    "    metrics_computer = None\n",
    "\n",
    "# âœ… Import Random logger\n",
    "try:\n",
    "    if 'random_routing_logging' in sys.modules:\n",
    "        importlib.reload(sys.modules['random_routing_logging'])\n",
    "    from random_routing_logging import RandomRoutingLogger\n",
    "    print(\"âœ… Imported RandomRoutingLogger\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Could not import RandomRoutingLogger: {e}\")\n",
    "    RandomRoutingLogger = None\n",
    "\n",
    "# âœ… Import dataset evaluation functions\n",
    "try:\n",
    "    if 'random_routing_evaluation' in sys.modules:\n",
    "        importlib.reload(sys.modules['random_routing_evaluation'])\n",
    "    from random_routing_evaluation import (\n",
    "        load_wikitext, load_lambada, load_hellaswag,\n",
    "        evaluate_perplexity, evaluate_lambada, evaluate_hellaswag\n",
    "    )\n",
    "    print(\"âœ… Imported dataset evaluation functions\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Could not import evaluation functions: {e}\")\n",
    "\n",
    "# âœ… Import visualization functions\n",
    "try:\n",
    "    if 'random_routing_visualization' in sys.modules:\n",
    "        importlib.reload(sys.modules['random_routing_visualization'])\n",
    "    from random_routing_visualization import create_comprehensive_visualization\n",
    "    print(\"âœ… Imported visualization functions\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Could not import visualization functions: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… FRAMEWORK MODULES READY\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8180586",
   "metadata": {},
   "source": [
    "## 4.6 DEBUG_MODE Configuration\n",
    "\n",
    "Configure fast testing vs full evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DEBUG MODE CONFIGURATION\")\n",
    "print(\"=\" * 70)# Toggle for fast testing vs full evaluation\n",
    "DEBUG_MODE = True  # Set to True for quick testing\n",
    "if DEBUG_MODE:\n",
    "  # Fast testing configuration\n",
    "  MAX_SAMPLES = 10\n",
    "  # Very small sample for speed\n",
    "  LOG_EVERY_N = 5   # Log every 5 tokens\n",
    "  SAVE_PLOTS = True\n",
    "  print(\"\\nâš¡ DEBUG MODE: ENABLED\")\n",
    "  print(\"   â€¢ Max samples: 10 (fast testing)\")\n",
    "  print(\"   â€¢ Logging: Every 5 tokens\")\n",
    "  print(\"   â€¢ Plots: Generated for all experiments\")\n",
    "else:\n",
    "  # Full evaluation configuration\n",
    "  MAX_SAMPLES = 200  # Full benchmark evaluation\n",
    "  LOG_EVERY_N = 100\n",
    "  # Log every 100 tokens for efficiency\n",
    "  SAVE_PLOTS = False  # Only save summaries, not per-token logs\n",
    "  print(\"\\nðŸŽ¯ PRODUCTION MODE: ENABLED\")\n",
    "  print(\"   â€¢ Max samples: 200 (full evaluation)\")\n",
    "  print(\"   â€¢ Logging: Every 100 tokens\")\n",
    "  print(\"   â€¢ Plots: Summary only\")\n",
    "  print(\"\\n\" + \"=\" * 70)\n",
    "  from pathlib import Path\n",
    "  if IN_COLAB:\n",
    "     OUTPUT_DIR = Path(WORK_DIR) / 'random_comprehensive_results'\n",
    "  else:\n",
    "    OUTPUT_DIR = Path('./random_comprehensive_results')\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    (OUTPUT_DIR / 'logs').mkdir(exist_ok=True)\n",
    "    (OUTPUT_DIR / 'plots').mkdir(exist_ok=True)\n",
    "    (OUTPUT_DIR / 'visualizations').mkdir(exist_ok=True)\n",
    "print(f\"\\nðŸ“ Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Routing method identifier\n",
    "ROUTING_METHOD = 'random'  # Used for logging and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf1486",
   "metadata": {},
   "source": [
    "## 5. Load OLMoE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0728f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OlmoeForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING OLMoE MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "MODEL_NAME = \"allenai/OLMoE-1B-7B-0924\"\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(\"Loading...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"âœ… Tokenizer loaded\")\n",
    "\n",
    "# Load model\n",
    "model = OlmoeForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… Model loaded in {load_time:.1f}s\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  â€¢ Architecture: {model.config.model_type}\")\n",
    "print(f\"  â€¢ Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  â€¢ Num layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  â€¢ Num experts: {model.config.num_experts}\")\n",
    "print(f\"  â€¢ Experts per token (Top-K): {model.config.num_experts_per_tok}\")\n",
    "print(f\"  â€¢ Vocab size: {model.config.vocab_size}\")\n",
    "\n",
    "NUM_LAYERS = model.config.num_hidden_layers\n",
    "NUM_EXPERTS = model.config.num_experts\n",
    "DEFAULT_K = model.config.num_experts_per_tok\n",
    "\n",
    "print(f\"\\nðŸ“Š OLMoE Routing Info:\")\n",
    "print(f\"  â€¢ Total experts per layer: {NUM_EXPERTS}\")\n",
    "print(f\"  â€¢ Default Top-K: {DEFAULT_K}\")\n",
    "print(f\"  â€¢ Routing happens in {NUM_LAYERS} layers\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… MODEL READY\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4dc21",
   "metadata": {},
   "source": [
    "## 6. OLMoE Router Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from olmoe_random_integration import RandomRoutingIntegration\n",
    "\n",
    "# Create patcher instance\n",
    "patcher = RandomRoutingIntegration(model, device=device)\n",
    "\n",
    "print(\"âœ… MoE block patcher initialized for Random routing\")\n",
    "print(f\"   Ready to patch {len(patcher.moe_blocks)} OlmoeSparseMoeBlock modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afabf325",
   "metadata": {},
   "source": [
    "## 6.5 VERIFICATION: Random Routing Actually Changed\n",
    "\n",
    "**CRITICAL TEST:** Prove that Random routing is actually working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b51c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"VERIFICATION TEST: RANDOM ROUTING IS ACTUALLY WORKING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "verification_prompt = \"The capital of France is\"\n",
    "inputs = tokenizer(verification_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "print(f\"\\nTest prompt: '{verification_prompt}'\")\n",
    "print(f\"\\nRunning 2 tests:\\n\")\n",
    "\n",
    "# TEST 1: Baseline (no patching) - should always use 8 experts\n",
    "print(\"TEST 1: Baseline (Native Top-K=8)\")\n",
    "print(\"-\" * 70)\n",
    "patcher.unpatch_model()  # Ensure no patching\n",
    "patcher.clear_stats()\n",
    "print(\"  âœ… Model is not patched. Using native Top-K=8.\")\n",
    "\n",
    "# TEST 2: Random routing with experts_amount=4\n",
    "print(\"TEST 2: Random Routing (experts_amount=4)\")\n",
    "print(\"-\" * 70)\n",
    "patcher.patch_model(experts_amount=4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_random = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "\n",
    "stats_random = patcher.get_stats_summary()\n",
    "generated_random = tokenizer.decode(outputs_random[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"  Generated: '{generated_random}'\")\n",
    "\n",
    "avg_experts = np.mean([stats_random[key]['mean'] for key in stats_random if 'counts' in key])\n",
    "\n",
    "print(f\"  Avg experts from stats: {avg_experts:.2f}\")\n",
    "\n",
    "if avg_experts == 4.0:\n",
    "    print(\"  âœ… SUCCESS: Expert count is fixed at 4, as expected.\")\n",
    "    test2_pass = True\n",
    "else:\n",
    "    print(f\"  âŒ FAILURE: Expert count is {avg_experts:.2f}, expected 4.\")\n",
    "    test2_pass = False\n",
    "\n",
    "patcher.unpatch_model()\n",
    "\n",
    "# FINAL VERDICT\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if test2_pass:\n",
    "    print(\"\\nðŸŽ‰ ALL CRITICAL TESTS PASSED!\")\n",
    "    print(\"\\nâœ… Expert counts are fixed by `experts_amount`.\")\n",
    "    print(\"âœ… Output quality maintained (text is coherent).\")\n",
    "    print(\"\\nðŸŽ¯ RANDOM ROUTING IS ACTUALLY WORKING!\")\n",
    "    print(\"   The model NOW uses random expert selection instead of fixed Top-K!\")\n",
    "else:\n",
    "    print(\"\\nâŒ VERIFICATION FAILED\")\n",
    "    print(\"   Expert counts are not varying as expected.\")\n",
    "    print(\"   The patching may not be working correctly.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f35e3",
   "metadata": {},
   "source": [
    "## 7. Test Prompts Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c7b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts by complexity level\n",
    "TEST_PROMPTS = {\n",
    "    'simple': [\n",
    "        \"The cat sat on the\",\n",
    "        \"Hello, my name is\",\n",
    "        \"The capital of France is\"\n",
    "    ],\n",
    "    'medium': [\n",
    "        \"In machine learning, a neural network\",\n",
    "        \"The process of photosynthesis involves\",\n",
    "        \"Climate change refers to long-term shifts in\"\n",
    "    ],\n",
    "    'complex': [\n",
    "        \"Explain the relationship between quantum entanglement and\",\n",
    "        \"Compare and contrast the economic policies of\",\n",
    "        \"The philosophical implications of consciousness suggest that\"\n",
    "    ],\n",
    "    'technical': [\n",
    "        \"In Python, a decorator is a function that\",\n",
    "        \"The time complexity of quicksort is\",\n",
    "        \"Transformer attention mechanism computes\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten all prompts\n",
    "ALL_PROMPTS = []\n",
    "PROMPT_COMPLEXITY = []\n",
    "\n",
    "for complexity, prompts in TEST_PROMPTS.items():\n",
    "    ALL_PROMPTS.extend(prompts)\n",
    "    PROMPT_COMPLEXITY.extend([complexity] * len(prompts))\n",
    "\n",
    "print(f\"Total test prompts: {len(ALL_PROMPTS)}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "for complexity, prompts in TEST_PROMPTS.items():\n",
    "    print(f\"  â€¢ {complexity.capitalize()}: {len(prompts)} prompts\")\n",
    "\n",
    "print(f\"\\nExample prompts:\")\n",
    "for complexity, prompts in list(TEST_PROMPTS.items())[:2]:\n",
    "    print(f\"\\n  {complexity.upper()}:\")\n",
    "    print(f\"    '{prompts[0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a6f8a",
   "metadata": {},
   "source": [
    "## 7.5 Load Benchmark Datasets\n",
    "\n",
    "Load WikiText-2, LAMBADA, and HellaSwag for comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING BENCHMARK DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configure sample count\n",
    "MAX_SAMPLES = 100  # Samples per dataset for comprehensive evaluation\n",
    "if 'MAX_SAMPLES' not in globals():\n",
    "    MAX_SAMPLES = 100  # Default if DEBUG_MODE section was not run\n",
    "\n",
    "print(f\"ðŸ“Š Using MAX_SAMPLES = {MAX_SAMPLES}\")\n",
    "EVAL_DATASETS = {}\n",
    "\n",
    "# Load WikiText-2\n",
    "try:\n",
    "    print(\"\\nðŸ“š Loading WikiText-2...\")\n",
    "    wikitext_data = load_wikitext(max_samples=MAX_SAMPLES)\n",
    "    EVAL_DATASETS['wikitext'] = wikitext_data\n",
    "    print(f\"   âœ… Loaded {len(wikitext_data)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ Failed to load WikiText: {e}\")\n",
    "\n",
    "# Load LAMBADA\n",
    "try:\n",
    "    print(\"\\nðŸ“š Loading LAMBADA...\")\n",
    "    lambada_data = load_lambada(max_samples=MAX_SAMPLES)\n",
    "    EVAL_DATASETS['lambada'] = lambada_data\n",
    "    print(f\"   âœ… Loaded {len(lambada_data)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ Failed to load LAMBADA: {e}\")\n",
    "\n",
    "# Load HellaSwag\n",
    "try:\n",
    "    print(\"\\nðŸ“š Loading HellaSwag...\")\n",
    "    hellaswag_data = load_hellaswag(max_samples=MAX_SAMPLES)\n",
    "    EVAL_DATASETS['hellaswag'] = hellaswag_data\n",
    "    print(f\"   âœ… Loaded {len(hellaswag_data)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ Failed to load HellaSwag: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… BENCHMARK DATASETS READY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset Summary:\")\n",
    "for name, data in EVAL_DATASETS.items():\n",
    "    count = len(data) if hasattr(data, '__len__') else 0\n",
    "    print(f\"  â€¢ {name}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76162c6c",
   "metadata": {},
   "source": [
    "## 8. Experiment Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e835674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "\n",
    "@dataclass\n",
    "class RoutingConfig:\n",
    "    \"\"\"Configuration for a routing experiment.\"\"\"\n",
    "    name: str\n",
    "    routing_type: str  # 'baseline' or 'random'\n",
    "    experts_amount: int\n",
    "    sum_of_weights: Optional[float] = None\n",
    "    temperature: float = 1.0\n",
    "    k: Optional[int] = None # For baseline top-k\n",
    "    layers_to_patch: Optional[List[int]] = None\n",
    "\n",
    "# =========================================================================\n",
    "# RANDOM ROUTING EXPERIMENT CONFIGURATIONS\n",
    "# =========================================================================\n",
    "configs = []\n",
    "\n",
    "# BASELINE CONFIGURATIONS\n",
    "baseline_k_values =  [4, 8, 16]\n",
    "\n",
    "for k in baseline_k_values:\n",
    "    configs.append(RoutingConfig(\n",
    "        name=f'{k}experts_topk_baseline',\n",
    "        routing_type='baseline',\n",
    "        k=k,\n",
    "        experts_amount=k\n",
    "    ))\n",
    "\n",
    "# RANDOM ROUTING CONFIGURATIONS\n",
    "experts_amounts = [4, 8, 16]\n",
    "sum_of_weights_options = [None, 0.5, \"original\"]\n",
    "layers_to_patch_options = [None, [0, 15], [4, 8, 12]]\n",
    "\n",
    "for exp_amount in experts_amounts:\n",
    "    for sw in sum_of_weights_options:\n",
    "        for layers in layers_to_patch_options:\n",
    "            if sw == \"original\":\n",
    "                sw_name = \"swOriginal\"\n",
    "            else:\n",
    "                sw_name = f\"sw{int(sw*100)}\" if sw is not None else \"swDyn\"\n",
    "\n",
    "            layers_name = f\"_layers_{'_'.join(map(str, layers))}\" if layers is not None else \"\"\n",
    "\n",
    "            configs.append(RoutingConfig(\n",
    "                name=f'{exp_amount}experts_random_{sw_name}{layers_name}',\n",
    "                routing_type='random',\n",
    "                experts_amount=exp_amount,\n",
    "                sum_of_weights=sw,\n",
    "                layers_to_patch=layers\n",
    "            ))\n",
    "\n",
    "print(f\"Total configurations: {len(configs)}\")\n",
    "print(f\"  â€¢ Baselines (TopK): {len(baseline_k_values)} configs (K={baseline_k_values})\")\n",
    "print(f\"  â€¢ Random routing: {len([c for c in configs if c.routing_type == 'random'])} configs\")\n",
    "\n",
    "print(f\"\\nFirst 12 configurations:\")\n",
    "for i, cfg in enumerate(configs[:12]):\n",
    "    if cfg.routing_type == 'baseline':\n",
    "        print(f\"  {i+1}. {cfg.name} (TopK={cfg.k})\")\n",
    "    elif cfg.routing_type == 'random':\n",
    "        print(f\"  {i+1}. {cfg.name} (experts={cfg.experts_amount}, sum_weights={cfg.sum_of_weights})\")\n",
    "\n",
    "if len(configs) > 12:\n",
    "    print(f\"  ... and {len(configs) - 12} more\")\n",
    "\n",
    "print(f\"\\nâœ… Configuration setup complete!\")\n",
    "print(f\"   Ready to run {len(configs)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eada47",
   "metadata": {},
   "source": [
    "## 9. Run Experiments\n",
    "\n",
    "This section runs all 21 configurations on all test prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List # Added for type hints\n",
    "\n",
    "def run_inference(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 20,\n",
    "    collect_routing: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run inference on a single prompt.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "            - generated_text: str\n",
    "            - num_tokens: int\n",
    "            - inference_time: float\n",
    "            - routing_stats: dict (if collect_routing=True)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    num_tokens = outputs.shape[1]\n",
    "\n",
    "    result = {\n",
    "        'generated_text': generated_text,\n",
    "        'num_tokens': num_tokens,\n",
    "        'inference_time': inference_time\n",
    "    }\n",
    "\n",
    "    if collect_routing:\n",
    "        # Support different patcher APIs: prefer get_stats() if present,\n",
    "        # otherwise fall back to a `stats` attribute (used elsewhere in the code).\n",
    "        if hasattr(patcher, 'get_stats'):\n",
    "            routing_stats = patcher.get_stats_summary()\n",
    "        else:\n",
    "            routing_stats = getattr(patcher, 'stats', None)\n",
    "        result['routing_stats'] = routing_stats\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_configuration(\n",
    "    config: RoutingConfig,\n",
    "    prompts: List[str],\n",
    "    prompt_complexities: List[str],\n",
    "    max_new_tokens: int = 20\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run a single configuration on all prompts.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with aggregated results.\n",
    "    \"\"\"\n",
    "    # Setup routing\n",
    "    if config.routing_type == 'baseline':\n",
    "        # Use OLMoE's native routing (no patching)\n",
    "        patcher.unpatch_model()\n",
    "        print(f\"  Running with OLMoE native Top-{config.k} routing\")\n",
    "    elif config.routing_type == 'random':\n",
    "        patcher.patch_model(\n",
    "            experts_amount=config.experts_amount,\n",
    "            sum_of_weights=config.sum_of_weights,\n",
    "            layers_to_patch=config.layers_to_patch\n",
    "        )\n",
    "        print(f\"  Running with Random routing (experts_amount={config.experts_amount}, sum_of_weights={config.sum_of_weights}, layers_to_patch={config.layers_to_patch})\")\n",
    "\n",
    "    # Run inference on all prompts\n",
    "    all_results = []\n",
    "    expert_counts_all = []\n",
    "\n",
    "    for prompt, complexity in tqdm(\n",
    "        zip(prompts, prompt_complexities),\n",
    "        total=len(prompts),\n",
    "        desc=f\"  {config.name}\",\n",
    "        leave=False\n",
    "    ):\n",
    "        patcher.stats.clear()  # Clear stats for this prompt\n",
    "\n",
    "        result = run_inference(prompt, max_new_tokens=max_new_tokens)\n",
    "        result['prompt'] = prompt\n",
    "        result['complexity'] = complexity\n",
    "\n",
    "        all_results.append(result)\n",
    "\n",
    "        if config.routing_type == 'random' and 'routing_stats' in result:\n",
    "            stats = result['routing_stats']\n",
    "            if 'avg_experts' in stats:\n",
    "                expert_counts_all.extend(\n",
    "                    [stats['avg_experts']] * result['num_tokens']\n",
    "                )\n",
    "\n",
    "    # Aggregate results\n",
    "    total_tokens = sum(r['num_tokens'] for r in all_results)\n",
    "    total_time = sum(r['inference_time'] for r in all_results)\n",
    "\n",
    "    aggregated = {\n",
    "        'config_name': config.name,\n",
    "        'routing_type': config.routing_type,\n",
    "        'num_prompts': len(prompts),\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_time': total_time,\n",
    "        'avg_time_per_prompt': total_time / len(prompts),\n",
    "        'tokens_per_second': total_tokens / total_time if total_time > 0 else 0,\n",
    "        'detailed_results': all_results\n",
    "    }\n",
    "\n",
    "    # Add Random-specific metrics\n",
    "    if config.routing_type == 'random':\n",
    "        # Aggregate routing stats across all prompts\n",
    "        all_expert_counts = []\n",
    "        for r in all_results:\n",
    "            if 'routing_stats' in r and r['routing_stats']:\n",
    "                if 'distribution' in r['routing_stats']:\n",
    "                    dist = r['routing_stats']['distribution']\n",
    "                    for k, count in enumerate(dist):\n",
    "                        all_expert_counts.extend([k] * count)\n",
    "\n",
    "        if all_expert_counts:\n",
    "            all_expert_counts = np.array(all_expert_counts)\n",
    "            aggregated['avg_experts'] = float(np.mean(all_expert_counts))\n",
    "            aggregated['std_experts'] = float(np.std(all_expert_counts))\n",
    "            aggregated['min_experts'] = int(np.min(all_expert_counts))\n",
    "            aggregated['max_experts'] = int(np.max(all_expert_counts))\n",
    "            aggregated['median_experts'] = float(np.median(all_expert_counts))\n",
    "\n",
    "            # Reduction vs baseline\n",
    "            baseline_experts = 8  # OLMoE default\n",
    "            aggregated['reduction_vs_baseline'] = float(\n",
    "                (baseline_experts - aggregated['avg_experts']) / baseline_experts * 100\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        aggregated['k'] = config.k\n",
    "        aggregated['avg_experts'] = config.k\n",
    "        aggregated['std_experts'] = 0.0\n",
    "        aggregated['min_experts'] = config.k\n",
    "        aggregated['max_experts'] = config.k\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "print(\"âœ… Inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f7944",
   "metadata": {},
   "source": [
    "## 9.5 Comprehensive Benchmark Evaluation\n",
    "\n",
    "Run evaluation on WikiText (perplexity), LAMBADA (accuracy), and HellaSwag (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define topk_routing function (standard baseline) - FIXED VERSION\n",
    "def topk_routing(router_logits, k=8, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Standard Top-K routing (baseline).\n",
    "\n",
    "    Args:\n",
    "        router_logits: [num_tokens, num_experts]\n",
    "        k: Number of experts to select\n",
    "        temperature: Softmax temperature\n",
    "\n",
    "    Returns:\n",
    "        routing_weights: [num_tokens, num_experts]\n",
    "        selected_experts: [num_tokens, k]\n",
    "        expert_counts: [num_tokens] (always k)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    num_tokens, num_experts = router_logits.shape\n",
    "    device = router_logits.device\n",
    "\n",
    "    # Apply temperature and softmax\n",
    "    scaled_logits = router_logits / temperature\n",
    "    weights = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "    # Select top-k experts\n",
    "    topk_weights, topk_indices = torch.topk(weights, k, dim=-1)\n",
    "\n",
    "    # Create full routing weights (zeros except for top-k)\n",
    "    routing_weights = torch.zeros_like(weights)\n",
    "    routing_weights.scatter_(1, topk_indices, topk_weights)\n",
    "\n",
    "    # FIXED: No renormalization! OLMoE expects sparse weights (sum < 1.0)\n",
    "    # when norm_topk_prob=False (the default).\n",
    "    # The old buggy line was:\n",
    "    # routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # Expert counts (always k for top-k)\n",
    "    expert_counts = torch.full((num_tokens,), k, device=device, dtype=torch.long)\n",
    "\n",
    "    return routing_weights, topk_indices, expert_counts\n",
    "\n",
    "print(\"âœ… topk_routing function defined (FIXED - no renormalization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e3326",
   "metadata": {},
   "source": [
    "SECTION 9.5: COMPREHENSIVE BENCHMARK EVALUATION WITH LOGGING\n",
    "=============================================================\n",
    "Evaluates ALL configurations on ALL datasets (WikiText, LAMBADA, HellaSwag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c608d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPREHENSIVE BENCHMARK EVALUATION WITH LOGGING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import json # Ensure json is imported\n",
    "\n",
    "if 'EVAL_DATASETS' not in globals() or not EVAL_DATASETS:\n",
    "    print(\"âš ï¸ No datasets loaded. Skipping benchmark evaluation.\")\n",
    "    print(\"   Run Section 7.5 to load datasets first.\")\n",
    "    comprehensive_results = []\n",
    "else:\n",
    "    print(f\"\\nExperiment Scope:\")\n",
    "    print(f\"  â€¢ Configurations: {len(configs)}\")\n",
    "    print(f\"  â€¢ Datasets: {list(EVAL_DATASETS.keys())}\")\n",
    "    print(f\"  â€¢ Samples per dataset: {MAX_SAMPLES}\")\n",
    "    print(f\"  â€¢ Total experiments: {len(configs) * len(EVAL_DATASETS)}\")\n",
    "\n",
    "    # Configure logging based on DEBUG_MODE\n",
    "    if 'LOG_EVERY_N' not in globals():\n",
    "        LOG_EVERY_N = 100  # Default\n",
    "\n",
    "    comprehensive_results = []\n",
    "    benchmark_start = time.time()\n",
    "    \n",
    "    # Ensure logs directory exists (for checking)\n",
    "    logs_dir = OUTPUT_DIR / 'logs'\n",
    "\n",
    "    for dataset_name, dataset_data in EVAL_DATASETS.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EVALUATING ON: {dataset_name.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        for i, config in enumerate(configs):\n",
    "            print(f\"\\n[{i+1}/{len(configs)}] {config.name} on {dataset_name}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # =============================================================\n",
    "            # CHECK FOR EXISTING RESULTS (CACHE)\n",
    "            # =============================================================\n",
    "            experiment_name = f\"{config.name}_{dataset_name}\"\n",
    "            log_file_path = logs_dir / f\"{experiment_name}.json\"\n",
    "            \n",
    "            if log_file_path.exists():\n",
    "                try:\n",
    "                    print(f\"  ðŸ” Found existing log: {log_file_path.name}\")\n",
    "                    with open(log_file_path, 'r') as f:\n",
    "                        cached_result = json.load(f)\n",
    "                    \n",
    "                    # Append cached result and skip execution\n",
    "                    comprehensive_results.append(cached_result)\n",
    "                    print(f\"  âœ… Loaded results from cache. Skipping re-evaluation.\")\n",
    "                    continue \n",
    "                except Exception as e:\n",
    "                    print(f\"  âš ï¸ Error loading cache ({e}). Re-running experiment...\")\n",
    "\n",
    "            # =============================================================\n",
    "            # RUN NEW EVALUATION\n",
    "            # =============================================================\n",
    "            config_start = time.time()\n",
    "\n",
    "            # Setup routing\n",
    "            patcher.unpatch_model() # Fixed: Changed from unpatch() to unpatch_model()\n",
    "            patcher.stats.clear()\n",
    "\n",
    "            # Initialize logger for Random configurations only\n",
    "            logger = None\n",
    "            enable_logging = (config.routing_type == 'random')\n",
    "            # experiment_name is already defined above\n",
    "            \n",
    "            if enable_logging:\n",
    "                print(f\"  ðŸ“Š Logging enabled: {experiment_name}\")\n",
    "\n",
    "            if config.routing_type == 'baseline':\n",
    "                # For baseline, we ensure the model uses its native routing.\n",
    "                # RandomRoutingIntegration's 'unpatch_model' restores the original model gates,\n",
    "                # which means the model's default K (typically 8 for OLMoE) will be used.\n",
    "                # The patcher does NOT provide functionality to set a custom Top-K (e.g., K=4 or K=16).\n",
    "                # Therefore, all 'baseline' configs will effectively run with the model's default K=8.\n",
    "                patcher.unpatch_model()\n",
    "                print(f\"  Using OLMoE native Top-8 routing (requested K={config.k} not supported by this patcher for baseline)\")\n",
    "                # Temporarily override config.k to reflect actual K for results\n",
    "                # config.k = 8\n",
    "\n",
    "            elif config.routing_type == 'random':\n",
    "                patcher.patch_model(\n",
    "                    experts_amount=config.experts_amount,\n",
    "                    sum_of_weights=config.sum_of_weights,\n",
    "                    layers_to_patch=config.layers_to_patch\n",
    "                )\n",
    "                print(f\"  Using Random routing (experts_amount={config.experts_amount}, sum_of_weights={config.sum_of_weights}, layers_to_patch={config.layers_to_patch})\")\n",
    "\n",
    "            # Initialize result with common fields\n",
    "            result = {\n",
    "                'config_name': config.name,\n",
    "                'dataset': dataset_name,\n",
    "            }\n",
    "\n",
    "            if config.routing_type == 'baseline':\n",
    "                result['routing_type'] = 'topk'\n",
    "                result['k_or_max_k'] = config.k\n",
    "            elif config.routing_type == 'random':\n",
    "                result['routing_type'] = 'random'\n",
    "                result['k_or_max_k'] = config.experts_amount\n",
    "\n",
    "\n",
    "            # â­ CRITICAL: Call start_sample() BEFORE evaluation for proper logging\n",
    "            patcher.start_sample()\n",
    "\n",
    "            try:\n",
    "                # =============================================================\n",
    "                # EVALUATE BASED ON DATASET TYPE\n",
    "                # =============================================================\n",
    "\n",
    "                if dataset_name == 'wikitext':\n",
    "                    eval_result = evaluate_perplexity(\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        dataset=dataset_data,\n",
    "                        patcher=patcher,  # âœ… NEW: Pass patcher for logging\n",
    "                        device=device,\n",
    "                        max_length=512,\n",
    "                        # âœ… NEW: Logging parameters\n",
    "                        log_routing=enable_logging,\n",
    "                        output_dir=str(OUTPUT_DIR),\n",
    "                        experiment_name=experiment_name,\n",
    "                        log_every_n=LOG_EVERY_N\n",
    "                    )\n",
    "                    # Use eval_result['perplexity'] as the primary perplexity value\n",
    "                    result['perplexity'] = eval_result['perplexity']\n",
    "                    # No longer attempting to access missing keys like perplexity_token_weighted\n",
    "                    print(f\"  âœ… Perplexity (token-weighted): {eval_result['perplexity']:.2f}\")\n",
    "\n",
    "                elif dataset_name == 'lambada':\n",
    "                    eval_result = evaluate_lambada(\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        dataset=dataset_data,\n",
    "                        patcher=patcher,  # âœ… NEW: Pass patcher\n",
    "                        device=device,\n",
    "                        # âœ… NEW: Logging parameters\n",
    "                        log_routing=enable_logging,\n",
    "                        output_dir=str(OUTPUT_DIR),\n",
    "                        experiment_name=experiment_name,\n",
    "                        log_every_n=LOG_EVERY_N\n",
    "                    )\n",
    "                    result['lambada_accuracy'] = eval_result['accuracy']\n",
    "                    print(f\"  âœ… LAMBADA Accuracy: {eval_result['accuracy']:.4f}\")\n",
    "\n",
    "                # elif dataset_name == 'hellaswag':\n",
    "                #     eval_result = evaluate_hellaswag(\n",
    "                #         model=model,\n",
    "                #         tokenizer=tokenizer,\n",
    "                #         dataset=dataset_data,\n",
    "                #         patcher=patcher,  # âœ… NEW: Pass patcher\n",
    "                #         device=device,\n",
    "                #         # âœ… NEW: Logging parameters\n",
    "                #         log_routing=enable_logging,\n",
    "                #         output_dir=str(OUTPUT_DIR),\n",
    "                #         experiment_name=experiment_name,\n",
    "                #         log_every_n=LOG_EVERY_N\n",
    "                #     )\n",
    "                #     # âœ… NEW: Capture BOTH raw and normalized accuracy\n",
    "                #     result['hellaswag_accuracy'] = eval_result['accuracy']  # Raw (backward compat)\n",
    "                #     result['hellaswag_accuracy_raw'] = eval_result['accuracy_raw']\n",
    "                #     result['hellaswag_accuracy_normalized'] = eval_result['accuracy_normalized']\n",
    "                #     print(f\"  âœ… HellaSwag Accuracy (raw): {eval_result['accuracy_raw']:.4f}\")\n",
    "                #     print(f\"  âœ… HellaSwag Accuracy (normalized): {eval_result['accuracy_normalized']:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Evaluation failed: {e}\")\n",
    "                import traceback\n",
    "                print(traceback.format_exc())\n",
    "                result['error'] = str(e)\n",
    "\n",
    "            # =============================================================\n",
    "            # GET ROUTING STATISTICS\n",
    "            # =============================================================\n",
    "\n",
    "            # Changed from get_stats() to get_stats_summary()\n",
    "            stats = patcher.get_stats_summary()\n",
    "\n",
    "            # Determine k_val for statistics based on config type\n",
    "            if config.routing_type == 'baseline':\n",
    "                k_val = config.k # this is the overridden value (8)\n",
    "            elif config.routing_type == 'random':\n",
    "                k_val = config.experts_amount # for random, experts_amount is the effective K\n",
    "            else:\n",
    "                k_val = 8 # Fallback to default OLMoE K\n",
    "\n",
    "            if stats:\n",
    "                result['avg_experts'] = stats.get('avg_experts', k_val)\n",
    "                result['std_experts'] = stats.get('std_experts', 0)\n",
    "                result['min_experts'] = stats.get('min_experts', k_val)\n",
    "                result['max_experts'] = stats.get('max_experts', k_val)\n",
    "\n",
    "                # Compute additional metrics if metrics_computer exists\n",
    "                expert_counts = np.array(patcher.stats.get('expert_counts', []))\n",
    "                if len(expert_counts) > 0:\n",
    "                    # Compute metrics inline (no external metrics_computer needed)\n",
    "\n",
    "                    # Adaptive range: max - min experts\n",
    "                    result['adaptive_range'] = int(np.max(expert_counts) - np.min(expert_counts))\n",
    "\n",
    "                    # Ceiling hit rate: % of tokens hitting max_k\n",
    "                    result['ceiling_hit_rate'] = float((expert_counts >= k_val).sum() / len(expert_counts) * 100)\n",
    "\n",
    "                    # Floor hit rate: % of tokens at min_k\n",
    "                    # For random routing, min_k is effectively k_val unless explicitly varied.\n",
    "                    result['floor_hit_rate'] = float((expert_counts <= k_val).sum() / len(expert_counts) * 100)\n",
    "\n",
    "                    # Mid range rate\n",
    "                    result['mid_range_rate'] = 100.0 - result['ceiling_hit_rate'] - result['floor_hit_rate']\n",
    "\n",
    "                    # Selection entropy\n",
    "                    try:\n",
    "                        from scipy.stats import entropy as scipy_entropy\n",
    "                        counts_dist = np.bincount(expert_counts.astype(int), minlength=65)\n",
    "                        counts_dist = counts_dist / (counts_dist.sum() + 1e-10)\n",
    "                        result['selection_entropy'] = float(scipy_entropy(counts_dist + 1e-10))\n",
    "                        max_entropy = np.log(k_val) if k_val > 1 else 1.0\n",
    "                        result['normalized_entropy'] = result['selection_entropy'] / max_entropy\n",
    "                    except:\n",
    "                        result['selection_entropy'] = 0.0\n",
    "                        result['normalized_entropy'] = 0.0\n",
    "\n",
    "                    # Expert activation ratio: avg_experts / total_experts (64)\n",
    "                    result['expert_activation_ratio'] = result['avg_experts'] / 64.0\n",
    "\n",
    "                    # FLOPs reduction vs baseline (k=8)\n",
    "                    result['flops_reduction_pct'] = (8.0 - result['avg_experts']) / 8.0 * 100\n",
    "\n",
    "                    # Reduction vs baseline (positive = fewer experts = more efficient)\n",
    "                    result['reduction_vs_baseline'] = (8 - result['avg_experts']) / 8 * 100\n",
    "\n",
    "                    print(f\"  ðŸ“Š Avg Experts: {result.get('avg_experts', 'N/A'):.2f} Â± {result.get('std_experts', 0):.2f}\")\n",
    "                    print(f\"  ðŸ“Š Range: [{result.get('min_experts', 'N/A')}, {result.get('max_experts', 'N/A')}]\")\n",
    "                    print(f\"  ðŸ“Š Reduction vs Baseline: {result.get('reduction_vs_baseline', 0):.1f}%\")\n",
    "            else:\n",
    "                # For configurations where stats were not collected (e.g., initial baseline run)\n",
    "                result['avg_experts'] = k_val\n",
    "                result['std_experts'] = 0\n",
    "                result['min_experts'] = k_val\n",
    "                result['max_experts'] = k_val\n",
    "                result['adaptive_range'] = 0\n",
    "                result['ceiling_hit_rate'] = 100.0\n",
    "                result['floor_hit_rate'] = 0.0\n",
    "                result['mid_range_rate'] = 0.0\n",
    "                result['reduction_vs_baseline'] = (8 - k_val) / 8 * 100\n",
    "\n",
    "            # =============================================================\n",
    "            # SAVE LOGS AND GENERATE PLOTS\n",
    "            # =============================================================\n",
    "\n",
    "            # if logger is not None:\n",
    "            #     try:\n",
    "            #         logger.save_logs()\n",
    "\n",
    "            #         if 'SAVE_PLOTS' in globals() and SAVE_PLOTS:\n",
    "            #             logger.generate_plots()\n",
    "            #             print(f\"  ðŸ“Š Generated plots\")\n",
    "\n",
    "            #         summary = logger.get_summary()\n",
    "            #         if summary:\n",
    "            #             result['logger_summary'] = summary\n",
    "\n",
    "            #         logger.clear()\n",
    "            #     except Exception as e:\n",
    "            #         print(f\"  âš ï¸ Logging/plotting failed: {e}\")\n",
    "\n",
    "            config_time = time.time() - config_start\n",
    "            result['elapsed_time'] = config_time\n",
    "            print(f\"  â±ï¸ Completed in {config_time:.1f}s\")\n",
    "\n",
    "            comprehensive_results.append(result)\n",
    "            \n",
    "            # --- Auto-save individual result immediately (Double safety) ---\n",
    "            try:\n",
    "                # Save immediately to avoid losing progress if kernel crashes\n",
    "                summary_file = logs_dir / f\"{experiment_name}.json\"\n",
    "                with open(summary_file, 'w') as f:\n",
    "                    json.dump(result, f, indent=2, default=str)\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ Failed to save intermediate log: {e}\")\n",
    "\n",
    "            # Clear GPU cache\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Ensure unpatched\n",
    "    patcher.unpatch_model() # Fixed: Changed from unpatch() to unpatch_model()\n",
    "\n",
    "    benchmark_time = time.time() - benchmark_start\n",
    "\n",
    "    # =========================================================================\n",
    "    # SUMMARY\n",
    "    # =========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPREHENSIVE BENCHMARK EVALUATION COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nâ±ï¸ Total time: {benchmark_time / 60:.1f} minutes\")\n",
    "    print(f\"ðŸ“Š Experiments completed: {len(comprehensive_results)}\")\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    results_df = pd.DataFrame(comprehensive_results)\n",
    "\n",
    "    # =========================================================================\n",
    "    # DISPLAY RESULTS BY DATASET\n",
    "    # =========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"RESULTS BY DATASET\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for dataset_name in EVAL_DATASETS.keys():\n",
    "        dataset_results = results_df[results_df['dataset'] == dataset_name]\n",
    "\n",
    "        print(f\"\\nðŸ“Š {dataset_name.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if dataset_name == 'wikitext':\n",
    "            cols = ['config_name', 'avg_experts', 'perplexity', 'reduction_vs_baseline']\n",
    "            if 'perplexity' in dataset_results.columns:\n",
    "                display_df = dataset_results[cols].copy()\n",
    "                display_df['perplexity'] = display_df['perplexity'].round(2)\n",
    "                display_df['avg_experts'] = display_df['avg_experts'].round(2)\n",
    "                display_df['reduction_vs_baseline'] = display_df['reduction_vs_baseline'].round(1)\n",
    "                print(display_df.to_string(index=False))\n",
    "\n",
    "        elif dataset_name == 'lambada':\n",
    "            cols = ['config_name', 'avg_experts', 'lambada_accuracy', 'reduction_vs_baseline']\n",
    "            if 'lambada_accuracy' in dataset_results.columns:\n",
    "                display_df = dataset_results[cols].copy()\n",
    "                display_df['lambada_accuracy'] = display_df['lambada_accuracy'].round(4)\n",
    "                display_df['avg_experts'] = display_df['avg_experts'].round(2)\n",
    "                display_df['reduction_vs_baseline'] = display_df['reduction_vs_baseline'].round(1)\n",
    "                print(display_df.to_string(index=False))\n",
    "\n",
    "        # elif dataset_name == 'hellaswag':\n",
    "        #     cols = ['config_name', 'avg_experts', 'hellaswag_accuracy', 'reduction_vs_baseline']\n",
    "        #     if 'hellaswag_accuracy' in dataset_results.columns:\n",
    "        #         display_df = dataset_results[cols].copy()\n",
    "        #         display_df['hellaswag_accuracy'] = display_df['hellaswag_accuracy'].round(4)\n",
    "        #         display_df['avg_experts'] = display_df['avg_experts'].round(2)\n",
    "        #         display_df['reduction_vs_baseline'] = display_df['reduction_vs_baseline'].round(1)\n",
    "        #         print(display_df.to_string(index=False))\n",
    "\n",
    "    # =========================================================================\n",
    "    # SAVE RESULTS\n",
    "    # =========================================================================\n",
    "\n",
    "    results_path = OUTPUT_DIR / 'comprehensive_results_all_datasets.csv'\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"\\nâœ… Results saved to: {results_path}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # KEY FINDINGS SUMMARY\n",
    "    # =========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"KEY FINDINGS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Compare baseline vs best random config\n",
    "    baseline_results = results_df[results_df['routing_type'] == 'topk']\n",
    "    random_results = results_df[results_df['routing_type'] == 'random']\n",
    "\n",
    "    if len(baseline_results) > 0 and len(random_results) > 0:\n",
    "        print(\"\\nðŸ“ˆ Baseline vs Random Routing Comparison:\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        for dataset_name in EVAL_DATASETS.keys():\n",
    "            baseline_ds = baseline_results[baseline_results['dataset'] == dataset_name]\n",
    "            random_ds = random_results[random_results['dataset'] == dataset_name]\n",
    "\n",
    "            if len(baseline_ds) > 0 and len(random_ds) > 0:\n",
    "                baseline_row = baseline_ds.iloc[0]\n",
    "\n",
    "                print(f\"\\n  {dataset_name.upper()}:\")\n",
    "                print(f\"    Baseline (TopK=8): avg_experts=8.00\")\n",
    "\n",
    "                if dataset_name == 'wikitext' and 'perplexity' in baseline_row:\n",
    "                    baseline_ppl = baseline_row['perplexity']\n",
    "                    print(f\"      Perplexity: {baseline_ppl:.2f}\")\n",
    "\n",
    "                    for _, random_row in random_ds.iterrows():\n",
    "                        random_ppl = random_row.get('perplexity', float('inf'))\n",
    "                        ppl_diff = ((random_ppl - baseline_ppl) / baseline_ppl) * 100\n",
    "                        print(f\"    {random_row['config_name']}: avg={random_row['avg_experts']:.2f}, PPL={random_ppl:.2f} ({ppl_diff:+.1f}%)\")\n",
    "\n",
    "                elif dataset_name == 'lambada' and 'lambada_accuracy' in baseline_row:\n",
    "                    baseline_acc = baseline_row['lambada_accuracy']\n",
    "                    print(f\"      Accuracy: {baseline_acc:.4f}\")\n",
    "\n",
    "                    for _, random_row in random_ds.iterrows():\n",
    "                        random_acc = random_row.get('lambada_accuracy', 0)\n",
    "                        acc_diff = ((random_acc - baseline_acc) / baseline_acc) * 100 if baseline_acc > 0 else 0\n",
    "                        print(f\"    {random_row['config_name']}: avg={random_row['avg_experts']:.2f}, Acc={random_acc:.4f} ({acc_diff:+.1f}%)\")\n",
    "\n",
    "                # elif dataset_name == 'hellaswag' and 'hellaswag_accuracy' in baseline_row:\n",
    "                #     baseline_acc = baseline_row['hellaswag_accuracy']\n",
    "                #     print(f\"      Accuracy: {baseline_acc:.4f}\")\n",
    "\n",
    "                #     for _, random_row in random_ds.iterrows():\n",
    "                #         random_acc = random_row.get('hellaswag_accuracy', 0)\n",
    "                #         acc_diff = ((random_acc - baseline_acc) / baseline_acc) * 100 if baseline_acc > 0 else 0\n",
    "                #         print(f\"    {random_row['config_name']}: avg={random_row['avg_experts']:.2f}, Acc={random_acc:.4f} ({acc_diff:+.1f}%)\")\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b5893",
   "metadata": {},
   "source": [
    "## 10. Save Comprehensive Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42572b03",
   "metadata": {},
   "source": [
    "SECTION: SAVING COMPREHENSIVE RESULTS\n",
    "=====================================\n",
    "Copy this entire cell to replace the broken one in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4afc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SAVING COMPREHENSIVE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create output directories\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "if 'OUTPUT_DIR' not in globals():\n",
    "    if IN_COLAB:\n",
    "        OUTPUT_DIR = Path(WORK_DIR) / 'random_comprehensive_results'\n",
    "    else:\n",
    "        OUTPUT_DIR = Path('./random_comprehensive_results')\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / 'logs').mkdir(exist_ok=True)\n",
    "(OUTPUT_DIR / 'visualizations').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Prefer comprehensive benchmark results, fallback to test prompt results\n",
    "if 'comprehensive_results' in globals() and comprehensive_results:\n",
    "    results_to_save = comprehensive_results\n",
    "    print(\"âœ… Using comprehensive benchmark results\")\n",
    "elif 'all_experiment_results' in globals() and all_experiment_results:\n",
    "    # Add missing columns to test prompt results for compatibility\n",
    "    for result in all_experiment_results:\n",
    "        if 'routing_type' in result and result['routing_type'] == 'baseline':\n",
    "            result['routing_type'] = 'topk'  # Match viz expectations\n",
    "        if 'k' in result and 'k_or_max_k' not in result:\n",
    "            result['k_or_max_k'] = result['k']\n",
    "        elif 'max_k' in result and 'k_or_max_k' not in result:\n",
    "            result['k_or_max_k'] = result['max_k']\n",
    "        if 'dataset' not in result:\n",
    "            result['dataset'] = 'test_prompts'\n",
    "        if 'mid_range_rate' not in result and 'ceiling_hit_rate' in result:\n",
    "            result['mid_range_rate'] = 100 - result.get('ceiling_hit_rate', 0) - result.get('floor_hit_rate', 0)\n",
    "    results_to_save = all_experiment_results\n",
    "    print(\"âš ï¸ Using test prompt results (no benchmark data)\")\n",
    "else:\n",
    "    print(\"âš ï¸ No results to save!\")\n",
    "    results_to_save = []\n",
    "\n",
    "if results_to_save:\n",
    "    # Create comprehensive DataFrame\n",
    "    results_df = pd.DataFrame(results_to_save)\n",
    "\n",
    "    # Save CSV\n",
    "    csv_path = OUTPUT_DIR / 'random_comprehensive_results.csv'\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ… Saved CSV: {csv_path}\")\n",
    "\n",
    "    # Save JSON\n",
    "    json_path = OUTPUT_DIR / 'random_comprehensive_results.json'\n",
    "    results_df.to_json(json_path, orient='records', indent=2)\n",
    "    print(f\"âœ… Saved JSON: {json_path}\")\n",
    "\n",
    "    # Save per-config summary JSONs (dual file logging)\n",
    "    logs_dir = OUTPUT_DIR / 'logs'\n",
    "    for result in results_to_save:\n",
    "        config_name = result.get('config_name', 'unknown')\n",
    "        dataset = result.get('dataset', 'mixed')\n",
    "\n",
    "        summary_file = logs_dir / f\"{config_name}_{dataset}.json\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(result, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"âœ… Saved {len(results_to_save)} individual log files to {logs_dir}\")\n",
    "\n",
    "    # Display summary tables\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Show top results\n",
    "    if 'avg_experts' in results_df.columns:\n",
    "        print(\"\\nTop 10 configurations by average experts:\")\n",
    "        display_cols = ['config_name', 'avg_experts']\n",
    "        for col in ['reduction_vs_baseline', 'dataset']:\n",
    "            if col in results_df.columns:\n",
    "                display_cols.append(col)\n",
    "        top_df = results_df.nsmallest(10, 'avg_experts')[display_cols]\n",
    "        print(top_df.to_string(index=False))\n",
    "\n",
    "    # Quality metrics by dataset\n",
    "    if 'dataset' in results_df.columns:\n",
    "        for dataset in results_df['dataset'].unique():\n",
    "            subset = results_df[results_df['dataset'] == dataset]\n",
    "            print(f\"\\nðŸ“Š {dataset.upper()} Results:\")\n",
    "\n",
    "            display_cols = ['config_name']\n",
    "            if 'perplexity' in subset.columns:\n",
    "                display_cols.append('perplexity')\n",
    "            if 'lambada_accuracy' in subset.columns:\n",
    "                display_cols.append('lambada_accuracy')\n",
    "            # if 'hellaswag_accuracy' in subset.columns:\n",
    "            #     display_cols.append('hellaswag_accuracy')\n",
    "            if 'avg_experts' in subset.columns:\n",
    "                display_cols.append('avg_experts')\n",
    "\n",
    "            if len(display_cols) > 1:\n",
    "                print(subset[display_cols].head(10).to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No results to save!\")\n",
    "    results_df = pd.DataFrame()  # Create empty DataFrame\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Routing method identifier\n",
    "ROUTING_METHOD = 'random'  # Used for logging and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91124655",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20841d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GENERATING COMPREHENSIVE VISUALIZATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'results_df' not in globals() or results_df is None or len(results_df) == 0:\n",
    "    print(\"\\nâš ï¸ No results DataFrame available\")\n",
    "    print(\"   Run Sections 9.5 and 10 first.\")\n",
    "elif 'create_comprehensive_visualization' not in globals():\n",
    "    print(\"\\nâš ï¸ Visualization function not loaded\")\n",
    "    print(\"   Run Section 4.5 to import framework modules.\")\n",
    "else:\n",
    "    # Validate required columns\n",
    "    required_cols = ['routing_type', 'k_or_max_k', 'dataset', 'avg_experts']\n",
    "    missing_cols = [c for c in required_cols if c not in results_df.columns]\n",
    "\n",
    "    if missing_cols:\n",
    "        print(f\"\\nâš ï¸ Missing columns for full visualization: {missing_cols}\")\n",
    "        print(\"Falling back to basic visualization...\")\n",
    "\n",
    "        # Fallback to basic visualization\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "\n",
    "        sns.set_style('whitegrid')\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle('OLMoE Random Routing Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Filter Random results\n",
    "        random_df = results_df[results_df['routing_type'] == 'random'] if 'routing_type' in results_df.columns else results_df\n",
    "\n",
    "        # Plot 1: Average experts\n",
    "        if 'avg_experts' in results_df.columns:\n",
    "            ax1 = axes[0, 0]\n",
    "            top_10 = results_df.nsmallest(10, 'avg_experts')\n",
    "            ax1.barh(range(len(top_10)), top_10['avg_experts'])\n",
    "            ax1.set_yticks(range(len(top_10)))\n",
    "            ax1.set_yticklabels(top_10['config_name'], fontsize=8)\n",
    "            ax1.set_title('Top 10: Fewest Experts')\n",
    "            ax1.set_xlabel('Average Experts')\n",
    "\n",
    "        # Plot 2: Throughput\n",
    "        if 'tokens_per_second' in results_df.columns:\n",
    "            ax2 = axes[0, 1]\n",
    "            top_10 = results_df.nlargest(10, 'tokens_per_second')\n",
    "            ax2.barh(range(len(top_10)), top_10['tokens_per_second'])\n",
    "            ax2.set_yticks(range(len(top_10)))\n",
    "            ax2.set_yticklabels(top_10['config_name'], fontsize=8)\n",
    "            ax2.set_title('Top 10: Throughput')\n",
    "            ax2.set_xlabel('Tokens/Second')\n",
    "\n",
    "        # Plot 3: Reduction vs baseline\n",
    "        if 'reduction_vs_baseline' in random_df.columns and len(random_df) > 0:\n",
    "            ax3 = axes[1, 0]\n",
    "            top_10 = random_df.nlargest(10, 'reduction_vs_baseline')\n",
    "            ax3.barh(range(len(top_10)), top_10['reduction_vs_baseline'])\n",
    "            ax3.set_yticks(range(len(top_10)))\n",
    "            ax3.set_yticklabels(top_10['config_name'], fontsize=8)\n",
    "            ax3.set_title('Top 10: Expert Reduction')\n",
    "            ax3.set_xlabel('Reduction (%)')\n",
    "\n",
    "        # Plot 4: Summary stats\n",
    "        ax4 = axes[1, 1]\n",
    "        ax4.axis('off')\n",
    "        summary_text = f\"\"\"\n",
    "        Total Configs: {len(results_df)}\n",
    "        Random Configs: {len(random_df)}\n",
    "        \"\"\"\n",
    "        if 'avg_experts' in results_df.columns:\n",
    "            summary_text += f\"\\nMin Avg Experts: {results_df['avg_experts'].min():.2f}\"\n",
    "            summary_text += f\"\\nMax Avg Experts: {results_df['avg_experts'].max():.2f}\"\n",
    "        ax4.text(0.1, 0.5, summary_text, fontsize=12, va='center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        viz_path = OUTPUT_DIR / 'visualizations' / 'basic_analysis.png'\n",
    "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ… Saved basic visualization: {viz_path}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Use comprehensive visualization\n",
    "        try:\n",
    "            viz_path = create_comprehensive_visualization(\n",
    "                results_df=results_df,\n",
    "                output_path=str(OUTPUT_DIR / 'visualizations' / 'random_comprehensive_comparison.png')\n",
    "            )\n",
    "\n",
    "            if viz_path:\n",
    "                print(f\"âœ… Saved comprehensive visualization: {viz_path}\")\n",
    "\n",
    "                # Display the visualization\n",
    "                from IPython.display import Image, display\n",
    "                if Path(viz_path).exists():\n",
    "                    display(Image(filename=str(viz_path)))\n",
    "            else:\n",
    "                print(\"âš ï¸ Visualization not created\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Comprehensive visualization failed: {e}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            print(\"\\nTry running the basic visualization fallback above.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… VISUALIZATIONS COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f1770",
   "metadata": {},
   "source": [
    "## 12. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d24578",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define random_df and baseline_df from results\n",
    "if 'results_df' not in globals() or results_df is None or len(results_df) == 0:\n",
    "    print(\"\\nâš ï¸ No results DataFrame available for analysis\")\n",
    "    print(\"   Run Sections 9.5 and 10 first to generate results.\")\n",
    "else:\n",
    "    # Filter Random and baseline results\n",
    "    random_df = results_df[results_df['routing_type'] == 'random'].copy() if 'routing_type' in results_df.columns else pd.DataFrame()\n",
    "    baseline_df = results_df[results_df['routing_type'] == 'topk'].copy() if 'routing_type' in results_df.columns else pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nResults breakdown:\")\n",
    "    print(f\"  â€¢ Total configurations: {len(results_df)}\")\n",
    "    print(f\"  â€¢ Baseline (TopK): {len(baseline_df)}\")\n",
    "    print(f\"  â€¢ random routing: {len(random_df)}\")\n",
    "\n",
    "    if len(random_df) == 0:\n",
    "        print(\"\\nâš ï¸ No Random results found. Skipping Random-specific analysis.\")\n",
    "    else:\n",
    "        # =====================================================================\n",
    "        # 1. BASELINE COMPARISON\n",
    "        # =====================================================================\n",
    "        print(\"\\n1. BASELINE COMPARISON\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if len(baseline_df) > 0 and 'avg_experts' in baseline_df.columns:\n",
    "            print(\"\\nBaseline Configurations:\")\n",
    "            baseline_cols = ['config_name', 'avg_experts']\n",
    "            if 'dataset' in baseline_df.columns:\n",
    "                baseline_cols.append('dataset')\n",
    "            if 'perplexity' in baseline_df.columns:\n",
    "                baseline_cols.append('perplexity')\n",
    "            display_cols = [c for c in baseline_cols if c in baseline_df.columns]\n",
    "            print(baseline_df[display_cols].to_string(index=False))\n",
    "\n",
    "        # =====================================================================\n",
    "        # 2. RANDOM ROUTING ANALYSIS\n",
    "        # =====================================================================\n",
    "        print(\"\\n\\n2. RANDOM ROUTING ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Best by reduction (most efficient)\n",
    "        if 'reduction_vs_baseline' in random_df.columns:\n",
    "            print(\"\\nTop 5 by Expert Reduction:\")\n",
    "            best_reduction = random_df.nlargest(5, 'reduction_vs_baseline')\n",
    "            display_cols = ['config_name', 'avg_experts', 'reduction_vs_baseline']\n",
    "            if 'max_k' in best_reduction.columns:\n",
    "                display_cols.append('max_k')\n",
    "            display_cols = [c for c in display_cols if c in best_reduction.columns]\n",
    "            print(best_reduction[display_cols].to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ Column 'reduction_vs_baseline' not found\")\n",
    "\n",
    "        # Best by low ceiling hit rate (not constrained)\n",
    "        if 'ceiling_hit_rate' in random_df.columns:\n",
    "            print(\"\\n\\nTop 5 by Low Ceiling Hit Rate (unconstrained):\")\n",
    "            best_unconstrained = random_df.nsmallest(5, 'ceiling_hit_rate')\n",
    "            display_cols = ['config_name', 'avg_experts', 'ceiling_hit_rate']\n",
    "            if 'max_k' in best_unconstrained.columns:\n",
    "                display_cols.append('max_k')\n",
    "            display_cols = [c for c in display_cols if c in best_unconstrained.columns]\n",
    "            print(best_unconstrained[display_cols].to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ Column 'ceiling_hit_rate' not found\")\n",
    "\n",
    "        # Best by adaptive range (most dynamic)\n",
    "        if 'adaptive_range' in random_df.columns:\n",
    "            print(\"\\n\\nTop 5 by Adaptive Range (most dynamic):\")\n",
    "            best_adaptive = random_df.nlargest(5, 'adaptive_range')\n",
    "            display_cols = ['config_name', 'adaptive_range', 'avg_experts']\n",
    "            if 'dataset' in best_adaptive.columns:\n",
    "                display_cols.append('dataset')\n",
    "            display_cols = [c for c in display_cols if c in best_adaptive.columns]\n",
    "            print(best_adaptive[display_cols].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a5ad8",
   "metadata": {},
   "source": [
    "## 13. Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' not in globals() or results_df is None or len(results_df) == 0:\n",
    "    print(\"âš ï¸ No results to generate report from\")\n",
    "    print(\"   Run Sections 9.5 and 10 first.\")\n",
    "else:\n",
    "    report_path = OUTPUT_DIR / 'random_routing_comprehensive_report.md'\n",
    "\n",
    "    with open(report_path, 'w') as f:\n",
    "        from datetime import datetime\n",
    "        f.write(\"# OLMoE Random Routing Comprehensive Evaluation Report\\n\\n\")\n",
    "        f.write(f\"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        f.write(f\"**Model:** {MODEL_NAME}\\n\\n\")\n",
    "\n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "        f.write(\"## Executive Summary\\n\\n\")\n",
    "        f.write(f\"- **Configurations tested:** {len(results_df)}\\n\")\n",
    "        if 'routing_type' in results_df.columns:\n",
    "            baseline_count = len(results_df[results_df['routing_type'] == 'topk'])\n",
    "            random_count = len(results_df[results_df['routing_type'] == 'random'])\n",
    "            f.write(f\"  - Baselines: {baseline_count}\\n\")\n",
    "            f.write(f\"  - Random variants: {random_count}\\n\")\n",
    "        if 'dataset' in results_df.columns:\n",
    "            datasets = results_df['dataset'].unique().tolist()\n",
    "            f.write(f\"- **Datasets evaluated:** {datasets}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "        f.write(\"## Key Findings\\n\\n\")\n",
    "\n",
    "        # Best configurations\n",
    "        random_df = results_df[results_df['routing_type'] == 'random'] if 'routing_type' in results_df.columns else results_df\n",
    "\n",
    "        if len(random_df) > 0 and 'avg_experts' in random_df.columns:\n",
    "            best = random_df.nsmallest(1, 'avg_experts').iloc[0]\n",
    "            f.write(\"### Best Efficiency (Fewest Experts)\\n\\n\")\n",
    "            f.write(f\"- **Configuration:** {best['config_name']}\\n\")\n",
    "            f.write(f\"- **Avg Experts:** {best['avg_experts']:.2f}\\n\\n\")\n",
    "\n",
    "            if len(random_df) > 0 and 'reduction_vs_baseline' in random_df.columns:\n",
    "\n",
    "                best_red = random_df.nlargest(1, 'reduction_vs_baseline').iloc[0]\n",
    "\n",
    "                f.write(\"### Best Expert Reduction\\n\\n\")\n",
    "\n",
    "                f.write(f\"- **Configuration:** {best_red['config_name']}\\n\")\n",
    "\n",
    "                f.write(f\"- **Reduction:** {best_red['reduction_vs_baseline']:.1f}%\\n\")\n",
    "\n",
    "                f.write(f\"- **Avg Experts:** {best_red['avg_experts']:.2f}\\n\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "                f.write(\"---\\n\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "                f.write(\"## Full Results\\n\\n\")\n",
    "\n",
    "                f.write(results_df.to_markdown(index=False))\n",
    "\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "                f.write(\"---\\n\\n\")\n",
    "\n",
    "                f.write(f\"Generated by Random Routing Framework\\n\")\n",
    "\n",
    "                f.write(f\"Output directory: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "            print(f\"âœ… Generated comprehensive report: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
